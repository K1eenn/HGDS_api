from fastapi import FastAPI, File, UploadFile, Form, Depends, HTTPException, Request, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
import uvicorn
from pydantic import BaseModel
from typing import Optional, List, Dict, Any, Union, Tuple
import json
import base64
from io import BytesIO
import asyncio
import os
import dotenv
import datetime
import random
import hashlib
import requests
import time
import logging
from PIL import Image
import uuid
# Updated OpenAI import style
from openai import OpenAI
from openai.types.chat import ChatCompletionMessage, ChatCompletionMessageToolCall
import shutil
import tempfile
from gtts import gTTS
import re
from html import unescape # For cleaning HTML before TTS

# T·∫£i bi·∫øn m√¥i tr∆∞·ªùng
dotenv.load_dotenv()

# Thi·∫øt l·∫≠p log
logging.basicConfig(level=logging.INFO,
                   format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
                   handlers=[logging.StreamHandler()])
# Use specific loggers per module/area if desired, otherwise a root logger is fine
logger = logging.getLogger('family_assistant_api')

# Kh·ªüi t·∫°o API
app = FastAPI(title="Tr·ª£ l√Ω Gia ƒë√¨nh API (Tool Calling)",
              description="API cho Tr·ª£ l√Ω Gia ƒë√¨nh th√¥ng minh v·ªõi kh·∫£ nƒÉng x·ª≠ l√Ω text, h√¨nh ·∫£nh, √¢m thanh v√† s·ª≠ d·ª•ng Tool Calling",
              version="1.1.0") # Version bump

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Trong production n√™n gi·ªõi h·∫°n origins
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

DATA_DIR = os.environ.get("DATA_DIR", "data")
os.makedirs(DATA_DIR, exist_ok=True)

# ƒê∆∞·ªùng d·∫´n file l∆∞u tr·ªØ d·ªØ li·ªáu
FAMILY_DATA_FILE = os.path.join(DATA_DIR, "family_data.json")
EVENTS_DATA_FILE = os.path.join(DATA_DIR, "events_data.json")
NOTES_DATA_FILE = os.path.join(DATA_DIR, "notes_data.json")
CHAT_HISTORY_FILE = os.path.join(DATA_DIR, "chat_history.json")
SESSIONS_DATA_FILE = os.path.join(DATA_DIR, "sessions_data.json")

# Th∆∞ m·ª•c l∆∞u tr·ªØ t·∫°m th·ªùi
TEMP_DIR = os.path.join(DATA_DIR, "temp_files")
os.makedirs(TEMP_DIR, exist_ok=True)

# Danh s√°ch domain tin t·ª©c Vi·ªát Nam
VIETNAMESE_NEWS_DOMAINS = [
    "vnexpress.net", "tuoitre.vn", "thanhnien.vn", "vietnamnet.vn", "vtv.vn",
    "nhandan.vn", "baochinhphu.vn", "laodong.vn", "tienphong.vn", "zingnews.vn",
    "cand.com.vn", "kenh14.vn", "baophapluat.vn",
]

openai_model = "gpt-4o-mini" # Or your preferred model supporting Tool Calling

# ------- Classes & Models -------------

class SessionManager:
    """Qu·∫£n l√Ω session v√† tr·∫°ng th√°i cho m·ªói client v·ªõi kh·∫£ nƒÉng l∆∞u tr·∫°ng th√°i"""
    def __init__(self, sessions_file=SESSIONS_DATA_FILE): # Use constant
        self.sessions = {}
        self.sessions_file = sessions_file
        self._load_sessions()

    def _load_sessions(self):
        """T·∫£i d·ªØ li·ªáu session t·ª´ file"""
        try:
            if os.path.exists(self.sessions_file):
                with open(self.sessions_file, "r", encoding="utf-8") as f:
                    loaded_sessions = json.load(f)
                    # Basic validation: ensure it's a dictionary
                    if isinstance(loaded_sessions, dict):
                         # Optional: Further validation of individual session structure
                        self.sessions = loaded_sessions
                        logger.info(f"ƒê√£ t·∫£i {len(self.sessions)} session t·ª´ {self.sessions_file}")
                    else:
                        logger.warning(f"D·ªØ li·ªáu session trong {self.sessions_file} kh√¥ng h·ª£p l·ªá (kh√¥ng ph·∫£i dict), kh·ªüi t·∫°o l·∫°i.")
                        self.sessions = {} # Reset if invalid structure
        except json.JSONDecodeError as e:
            logger.error(f"L·ªói JSON khi t·∫£i session t·ª´ {self.sessions_file}: {e}. Kh·ªüi t·∫°o l·∫°i.")
            self.sessions = {} # Reset on JSON error
        except Exception as e:
            logger.error(f"L·ªói kh√¥ng x√°c ƒë·ªãnh khi t·∫£i session: {e}", exc_info=True)
            self.sessions = {} # Reset on other errors


    def _save_sessions(self):
        """L∆∞u d·ªØ li·ªáu session v√†o file"""
        try:
            # Ensure directory exists
            os.makedirs(os.path.dirname(self.sessions_file) or '.', exist_ok=True)
            with open(self.sessions_file, "w", encoding="utf-8") as f:
                json.dump(self.sessions, f, ensure_ascii=False, indent=2)
            logger.info(f"ƒê√£ l∆∞u {len(self.sessions)} session v√†o {self.sessions_file}")
            return True
        except Exception as e:
            logger.error(f"L·ªói khi l∆∞u session: {e}", exc_info=True)
            return False

    def get_session(self, session_id):
        """L·∫•y session ho·∫∑c t·∫°o m·ªõi n·∫øu ch∆∞a t·ªìn t·∫°i"""
        if session_id not in self.sessions:
            logger.info(f"T·∫°o session m·ªõi: {session_id}")
            self.sessions[session_id] = {
                "messages": [],
                "current_member": None,
                "suggested_question": None,
                "process_suggested": False,
                "question_cache": {},
                "created_at": datetime.datetime.now().isoformat(), # Use ISO format
                "last_updated": datetime.datetime.now().isoformat() # Use ISO format
            }
            self._save_sessions() # Save immediately after creation
        # Always update last access time? Maybe not needed if last_updated is on modification.
        # self.sessions[session_id]["last_accessed"] = datetime.datetime.now().isoformat()
        return self.sessions[session_id]

    def update_session(self, session_id, data):
        """C·∫≠p nh·∫≠t d·ªØ li·ªáu session"""
        if session_id in self.sessions:
            try:
                self.sessions[session_id].update(data)
                self.sessions[session_id]["last_updated"] = datetime.datetime.now().isoformat() # Use ISO format
                # Save frequently? Or batch saves? Saving on every update can be slow.
                # Consider saving less often if performance is an issue.
                if not self._save_sessions():
                     logger.error(f"C·∫≠p nh·∫≠t session {session_id} th√†nh c√¥ng trong b·ªô nh·ªõ nh∆∞ng L∆ØU TH·∫§T B·∫†I.")
                     # Should we revert the update in memory? Depends on desired behavior.
                return True
            except Exception as e:
                logger.error(f"L·ªói khi c·∫≠p nh·∫≠t session {session_id} trong b·ªô nh·ªõ: {e}", exc_info=True)
                return False
        else:
             logger.warning(f"C·ªë g·∫Øng c·∫≠p nh·∫≠t session kh√¥ng t·ªìn t·∫°i: {session_id}")
             return False

    def delete_session(self, session_id):
        """X√≥a session"""
        if session_id in self.sessions:
            del self.sessions[session_id]
            self._save_sessions()
            logger.info(f"ƒê√£ x√≥a session: {session_id}")
            return True
        return False

    def cleanup_old_sessions(self, days_threshold=30):
        """X√≥a c√°c session c≈© kh√¥ng ho·∫°t ƒë·ªông sau s·ªë ng√†y nh·∫•t ƒë·ªãnh"""
        now = datetime.datetime.now(datetime.timezone.utc) # Use timezone-aware datetime
        sessions_to_remove = []
        removed_count = 0

        for session_id, session_data in list(self.sessions.items()): # Iterate over a copy
            last_updated_str = session_data.get("last_updated")
            if last_updated_str:
                try:
                    # Parse ISO format string
                    last_updated_date = datetime.datetime.fromisoformat(last_updated_str)
                    # Make it timezone-aware if it's naive (assume local timezone if naive, or UTC if stored as UTC)
                    if last_updated_date.tzinfo is None:
                        # Assuming stored time is UTC, make it aware
                        last_updated_date = last_updated_date.replace(tzinfo=datetime.timezone.utc)
                        # Or if assuming local time:
                        # last_updated_date = last_updated_date.astimezone(datetime.timezone.utc)

                    time_inactive = now - last_updated_date
                    if time_inactive.days > days_threshold:
                        sessions_to_remove.append(session_id)
                except ValueError:
                    logger.error(f"ƒê·ªãnh d·∫°ng last_updated kh√¥ng h·ª£p l·ªá ('{last_updated_str}') cho session {session_id}. Xem x√©t x√≥a.")
                    # Optionally remove sessions with invalid dates too
                    # sessions_to_remove.append(session_id)
                except Exception as e:
                    logger.error(f"L·ªói khi x·ª≠ l√Ω th·ªùi gian cho session {session_id}: {e}", exc_info=True)

        # X√≥a c√°c session c≈©
        if sessions_to_remove:
             for session_id in sessions_to_remove:
                 if session_id in self.sessions: # Check again in case of concurrent modification
                     del self.sessions[session_id]
                     removed_count += 1
             if removed_count > 0:
                 self._save_sessions()
                 logger.info(f"ƒê√£ x√≥a {removed_count} session c≈© (qu√° {days_threshold} ng√†y kh√¥ng ho·∫°t ƒë·ªông).")
             else:
                  logger.info("Kh√¥ng c√≥ session c≈© n√†o c·∫ßn x√≥a.")
        else:
            logger.info("Kh√¥ng c√≥ session c≈© n√†o c·∫ßn x√≥a.")


# --- Weather Service ---
class WeatherService:
    VIETNAMESE_WEEKDAY_MAP = {
        "th·ª© 2": 0, "th·ª© hai": 0, "t2": 0,
        "th·ª© 3": 1, "th·ª© ba": 1, "t3": 1,
        "th·ª© 4": 2, "th·ª© t∆∞": 2, "t4": 2,
        "th·ª© 5": 3, "th·ª© nƒÉm": 3, "t5": 3,
        "th·ª© 6": 4, "th·ª© s√°u": 4, "t6": 4,
        "th·ª© 7": 5, "th·ª© b·∫£y": 5, "t7": 5,
        "ch·ªß nh·∫≠t": 6, "cn": 6,
    }

    def __init__(self, openweather_api_key: str = None):
        self.openweather_api_key = openweather_api_key or os.getenv("OPENWEATHER_API_KEY", "")
        self.cache = {}
        self.cache_duration = 30 * 60 # 30 minutes

        if not self.openweather_api_key or len(self.openweather_api_key) < 10:
             logger.warning(f"OpenWeatherMap API key kh√¥ng h·ª£p l·ªá ho·∫∑c ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh. D·ªãch v·ª• th·ªùi ti·∫øt s·∫Ω kh√¥ng ho·∫°t ƒë·ªông.")


    def _get_cache_key(self, location: str, forecast_days: int = 1) -> str:
        return f"{location.lower()}_{forecast_days}_{datetime.datetime.now().strftime('%Y-%m-%d_%H')}" # Cache per hour

    def _is_cache_valid(self, timestamp: float) -> bool:
        return (time.time() - timestamp) < self.cache_duration

    async def _make_api_request(self, url: str, params: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Helper function to make requests and handle common errors."""
        if not self.openweather_api_key or len(self.openweather_api_key) < 10:
             logger.error("B·ªè qua g·ªçi API th·ªùi ti·∫øt do thi·∫øu API key h·ª£p l·ªá.")
             return None
        try:
            # Use asyncio compatible requests or httpx
            async with requests.Session() as session: # Use session for potential reuse
                 # Need an async request library like httpx or aiohttp
                 # Using requests synchronously here will block the event loop
                 # Let's use requests for now, but ideally replace with httpx
                 response = await asyncio.to_thread(session.get, url, params=params, timeout=10) # Run sync requests in thread
                 response.raise_for_status()
                 return response.json()
        except requests.exceptions.Timeout:
            logger.error(f"Y√™u c·∫ßu API th·ªùi ti·∫øt t·ªõi {url} b·ªã timeout.")
            return None
        except requests.exceptions.HTTPError as e:
            status_code = e.response.status_code
            if status_code == 401:
                logger.error("L·ªói API th·ªùi ti·∫øt: API key kh√¥ng h·ª£p l·ªá ho·∫∑c ƒë√£ h·∫øt h·∫°n.")
            elif status_code == 404:
                logger.error(f"L·ªói API th·ªùi ti·∫øt: Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu cho v·ªã tr√≠ (Params: {params}).")
            elif status_code == 429:
                logger.error("L·ªói API th·ªùi ti·∫øt: V∆∞·ª£t qu√° gi·ªõi h·∫°n g·ªçi API.")
            else:
                logger.error(f"L·ªói HTTP API th·ªùi ti·∫øt ({status_code}): {e}. URL: {url}")
            return None
        except requests.exceptions.RequestException as e:
            logger.error(f"L·ªói k·∫øt n·ªëi API th·ªùi ti·∫øt: {e}. URL: {url}")
            return None
        except Exception as e:
            logger.error(f"L·ªói kh√¥ng x√°c ƒë·ªãnh khi g·ªçi API th·ªùi ti·∫øt: {e}", exc_info=True)
            return None

    async def get_weather(self, location: str, forecast_days: int = 1, language: str = "vi", target_date: str = None) -> Dict[str, Any]:
        """L·∫•y th√¥ng tin th·ªùi ti·∫øt, ∆∞u ti√™n OneCall API, fallback v·ªÅ basic API."""
        if not self.openweather_api_key or len(self.openweather_api_key) < 10:
            return {
                "error": True, "message": "API key OpenWeatherMap kh√¥ng h·ª£p l·ªá ho·∫∑c ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh.",
                "recommendation": "Vui l√≤ng b·ªï sung OPENWEATHER_API_KEY h·ª£p l·ªá v√†o file .env"
            }

        calculated_days = forecast_days
        if target_date:
            try:
                target_date_obj = datetime.datetime.strptime(target_date, "%Y-%m-%d").date()
                today = datetime.datetime.now().date()
                days_difference = (target_date_obj - today).days
                if days_difference >= 0:
                    calculated_days = max(forecast_days, days_difference + 1)
                    logger.info(f"ƒêi·ªÅu ch·ªânh forecast_days th√†nh {calculated_days} ƒë·ªÉ bao g·ªìm ng√†y {target_date}")
                else:
                    logger.warning(f"Ng√†y y√™u c·∫ßu {target_date} l√† trong qu√° kh·ª©, kh√¥ng th·ªÉ l·∫•y d·ª± b√°o.")
                    # Return an error or just current weather? Let's return error for past dates.
                    return {"error": True, "message": f"Kh√¥ng th·ªÉ l·∫•y d·ª± b√°o cho ng√†y qu√° kh·ª© ({target_date})."}
            except ValueError:
                 logger.error(f"ƒê·ªãnh d·∫°ng target_date kh√¥ng h·ª£p l·ªá: {target_date}. S·ª≠ d·ª•ng forecast_days m·∫∑c ƒë·ªãnh.")
                 target_date = None # Reset target_date if invalid
            except Exception as e:
                 logger.error(f"L·ªói khi ph√¢n t√≠ch target_date {target_date}: {e}")
                 target_date = None # Reset on error

        calculated_days = min(calculated_days, 7) # Max 7 days forecast typically

        cache_key = self._get_cache_key(location, calculated_days)
        if cache_key in self.cache and self._is_cache_valid(self.cache[cache_key].get("timestamp", 0)):
            logger.info(f"S·ª≠ d·ª•ng d·ªØ li·ªáu th·ªùi ti·∫øt t·ª´ cache cho {location}")
            return self.cache[cache_key].get("data", {})

        # --- Try OneCall API First ---
        geo_data = await self._get_coordinates(location)
        if geo_data:
            lat, lon = geo_data['lat'], geo_data['lon']
            onecall_data = await self._get_weather_from_onecall_api(lat, lon, calculated_days, language)
            if onecall_data:
                standardized_data = self._parse_onecall_data(onecall_data, geo_data, calculated_days)
                if standardized_data:
                    self._update_cache(cache_key, standardized_data)
                    logger.info(f"L·∫•y d·ªØ li·ªáu th·ªùi ti·∫øt th√†nh c√¥ng t·ª´ OneCall API cho {location}")
                    return standardized_data
                else:
                    logger.error("Kh√¥ng th·ªÉ ph√¢n t√≠ch d·ªØ li·ªáu t·ª´ OneCall API.")
            else:
                 logger.warning("Th·∫•t b·∫°i khi g·ªçi OneCall API. Th·ª≠ API c∆° b·∫£n.")
        else:
             logger.warning(f"Kh√¥ng t√¨m th·∫•y t·ªça ƒë·ªô cho {location}. Th·ª≠ API c∆° b·∫£n.")


        # --- Fallback to Basic APIs ---
        logger.info(f"Th·ª≠ l·∫•y th·ªùi ti·∫øt b·∫±ng API c∆° b·∫£n cho {location}")
        current_data = await self._get_current_weather_basic(location, language)
        forecast_data = await self._get_forecast_basic(location, calculated_days, language)

        if current_data and forecast_data:
            combined_data = self._combine_basic_weather_data(current_data, forecast_data, calculated_days)
            if combined_data:
                 self._update_cache(cache_key, combined_data)
                 logger.info(f"L·∫•y d·ªØ li·ªáu th·ªùi ti·∫øt th√†nh c√¥ng t·ª´ API c∆° b·∫£n cho {location}")
                 return combined_data
            else:
                 logger.error("Kh√¥ng th·ªÉ k·∫øt h·ª£p d·ªØ li·ªáu t·ª´ API th·ªùi ti·∫øt c∆° b·∫£n.")
        else:
            logger.error("Th·∫•t b·∫°i khi l·∫•y d·ªØ li·ªáu t·ª´ c·∫£ current v√† forecast API c∆° b·∫£n.")


        # --- Final Error ---
        error_msg = f"Kh√¥ng th·ªÉ l·∫•y th√¥ng tin th·ªùi ti·∫øt cho {location}"
        if target_date: error_msg += f" v√†o ng√†y {target_date}"
        return {
            "error": True, "message": f"{error_msg}. C√≥ l·ªói khi k·∫øt n·ªëi ho·∫∑c x·ª≠ l√Ω d·ªØ li·ªáu t·ª´ OpenWeatherMap.",
            "recommendation": "Ki·ªÉm tra k·∫øt n·ªëi m·∫°ng, API key v√† t√™n v·ªã tr√≠."
        }

    def _update_cache(self, key: str, data: Dict[str, Any]) -> None:
        """C·∫≠p nh·∫≠t cache v·ªõi d·ªØ li·ªáu m·ªõi v√† timestamp"""
        self.cache[key] = {"data": data, "timestamp": time.time()}
        # Optional: Limit cache size if necessary
        # MAX_CACHE_SIZE = 100
        # if len(self.cache) > MAX_CACHE_SIZE:
        #     oldest_key = min(self.cache, key=lambda k: self.cache[k]['timestamp'])
        #     del self.cache[oldest_key]

    async def _get_coordinates(self, location: str) -> Optional[Dict[str, Any]]:
        """L·∫•y t·ªça ƒë·ªô (lat, lon) t·ª´ t√™n v·ªã tr√≠."""
        url = "https://api.openweathermap.org/geo/1.0/direct"
        params = {"q": location, "limit": 1, "appid": self.openweather_api_key}
        logger.info(f"G·ªçi Geocoding API cho {location}")
        geo_response = await self._make_api_request(url, params)
        if geo_response and len(geo_response) > 0:
            logger.info(f"T√¨m th·∫•y t·ªça ƒë·ªô cho {location}: {geo_response[0]['lat']}, {geo_response[0]['lon']}")
            return geo_response[0]
        else:
            logger.error(f"Kh√¥ng t√¨m th·∫•y t·ªça ƒë·ªô cho {location}.")
            return None

    async def _get_weather_from_onecall_api(self, lat: float, lon: float, days: int, language: str) -> Optional[Dict[str, Any]]:
        """G·ªçi OpenWeatherMap OneCall API v3.0."""
        url = "https://api.openweathermap.org/data/3.0/onecall"
        params = {
            "lat": lat, "lon": lon,
            "exclude": "minutely,alerts", # Exclude less used parts
            "units": "metric", "lang": language,
            "appid": self.openweather_api_key
        }
        logger.info(f"G·ªçi OneCall API cho t·ªça ƒë·ªô {lat}, {lon}")
        return await self._make_api_request(url, params)

    def _parse_onecall_data(self, data: Dict[str, Any], geo_data: Dict[str, Any], forecast_days: int) -> Optional[Dict[str, Any]]:
        """Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu OneCall API sang ƒë·ªãnh d·∫°ng chu·∫©n."""
        try:
            current_dt = data.get("current", {}).get("dt")
            if not current_dt:
                 logger.error("D·ªØ li·ªáu OneCall thi·∫øu th√¥ng tin 'current'.")
                 return None

            standardized = {
                "location": {
                    "name": geo_data.get("local_names", {}).get("vi", geo_data.get("name", "Kh√¥ng r√µ")),
                    "country": geo_data.get("country", ""),
                    "lat": geo_data.get("lat"), "lon": geo_data.get("lon"),
                    "localtime": datetime.datetime.fromtimestamp(current_dt, tz=datetime.timezone.utc).astimezone().strftime("%Y-%m-%d %H:%M:%S %Z") # Show local time with zone
                },
                "current": {
                    "temp_c": data["current"]["temp"],
                    "temp_f": data["current"]["temp"] * 9/5 + 32,
                    "is_day": 1 if data["current"].get("sunrise", 0) < current_dt < data["current"].get("sunset", float('inf')) else 0,
                    "condition": {
                        "text": data["current"]["weather"][0]["description"],
                        "icon": f"https://openweathermap.org/img/wn/{data['current']['weather'][0]['icon']}@2x.png"
                    },
                    "wind_kph": data["current"]["wind_speed"] * 3.6,
                    "wind_dir": self._get_wind_direction(data["current"].get("wind_deg", 0)),
                    "humidity": data["current"]["humidity"],
                    "feelslike_c": data["current"]["feels_like"],
                    "uv": data["current"].get("uvi", 0),
                    "pressure_mb": data["current"].get("pressure"),
                    "visibility_km": data["current"].get("visibility", 10000) / 1000, # Meters to km
                },
                "forecast": []
            }

            # Parse daily forecast
            if "daily" in data:
                for day_data in data["daily"][:forecast_days]:
                    day_dt = day_data.get("dt")
                    if not day_dt: continue

                    day_forecast = {
                        "date": datetime.datetime.fromtimestamp(day_dt, tz=datetime.timezone.utc).strftime("%Y-%m-%d"),
                        "max_temp_c": day_data["temp"]["max"],
                        "min_temp_c": day_data["temp"]["min"],
                        "condition": {
                            "text": day_data["weather"][0]["description"],
                            "icon": f"https://openweathermap.org/img/wn/{day_data['weather'][0]['icon']}@2x.png"
                        },
                        "chance_of_rain": day_data.get("pop", 0) * 100,
                        "sunrise": datetime.datetime.fromtimestamp(day_data["sunrise"], tz=datetime.timezone.utc).astimezone().strftime("%H:%M"),
                        "sunset": datetime.datetime.fromtimestamp(day_data["sunset"], tz=datetime.timezone.utc).astimezone().strftime("%H:%M"),
                        "humidity": day_data.get("humidity"),
                        "uv": day_data.get("uvi"),
                        "wind_kph": day_data.get("wind_speed", 0) * 3.6,
                        "summary": day_data.get("summary", "") # Daily summary if available
                    }
                    standardized["forecast"].append(day_forecast)

            # Parse hourly forecast for the first day (or relevant period)
            if "hourly" in data:
                 # Find the first relevant forecast day to attach hourly data
                if standardized["forecast"]:
                     first_forecast_date = standardized["forecast"][0]["date"]
                     hourly_list = []
                     added_hours = 0
                     max_hours_to_show = 24 # Show up to 24 hours
                     now_local = datetime.datetime.now().astimezone()

                     for hour_data in data["hourly"]:
                         hour_dt_obj = datetime.datetime.fromtimestamp(hour_data["dt"], tz=datetime.timezone.utc).astimezone()
                         # Add only future hours or very recent past for context
                         if hour_dt_obj >= now_local - datetime.timedelta(hours=1) and added_hours < max_hours_to_show:
                             hour_forecast = {
                                 "time": hour_dt_obj.strftime("%H:%M"), # Local time
                                 "temp_c": hour_data["temp"],
                                 "condition": {
                                     "text": hour_data["weather"][0]["description"],
                                     "icon": f"https://openweathermap.org/img/wn/{hour_data['weather'][0]['icon']}@2x.png"
                                 },
                                 "chance_of_rain": hour_data.get("pop", 0) * 100,
                                 "humidity": hour_data.get("humidity"),
                                 "wind_kph": hour_data.get("wind_speed", 0) * 3.6
                             }
                             hourly_list.append(hour_forecast)
                             added_hours += 1

                     if hourly_list:
                          # Find which day to attach it to (usually the first one)
                          # This assumes hourly data starts from near current time
                          if standardized["forecast"][0]["date"] == now_local.strftime("%Y-%m-%d"):
                              standardized["forecast"][0]["hourly"] = hourly_list
                          elif len(standardized["forecast"]) > 1 and standardized["forecast"][1]["date"] == now_local.strftime("%Y-%m-%d"):
                               # If forecast starts tomorrow, but we have today's hourly
                               pass # Or decide where to put it
                          else: # Fallback: attach to first day anyway
                               standardized["forecast"][0]["hourly"] = hourly_list

            return standardized

        except (KeyError, IndexError, TypeError, ValueError) as e:
            logger.error(f"L·ªói khi ph√¢n t√≠ch d·ªØ li·ªáu OneCall: {e}", exc_info=True)
            logger.error(f"D·ªØ li·ªáu l·ªói: {json.dumps(data, indent=2)}")
            return None

    async def _get_current_weather_basic(self, location: str, language: str) -> Optional[Dict[str, Any]]:
        """L·∫•y th·ªùi ti·∫øt hi·ªán t·∫°i b·∫±ng API c∆° b·∫£n."""
        url = "https://api.openweathermap.org/data/2.5/weather"
        params = {"q": location, "units": "metric", "lang": language, "appid": self.openweather_api_key}
        logger.info(f"G·ªçi API th·ªùi ti·∫øt c∆° b·∫£n (current) cho {location}")
        return await self._make_api_request(url, params)

    async def _get_forecast_basic(self, location: str, days: int, language: str) -> Optional[Dict[str, Any]]:
        """L·∫•y d·ª± b√°o b·∫±ng API c∆° b·∫£n (5 day / 3 hour)."""
        url = "https://api.openweathermap.org/data/2.5/forecast"
        # cnt = days * 8 # Approximate number of 3-hour intervals, max is 40 for free tier
        params = {"q": location, "units": "metric", "lang": language, "appid": self.openweather_api_key} # Let API decide count
        logger.info(f"G·ªçi API th·ªùi ti·∫øt c∆° b·∫£n (forecast) cho {location}")
        return await self._make_api_request(url, params)

    def _combine_basic_weather_data(self, current_data: Dict[str, Any], forecast_data: Dict[str, Any], days: int) -> Optional[Dict[str, Any]]:
        """K·∫øt h·ª£p d·ªØ li·ªáu t·ª´ API c∆° b·∫£n th√†nh ƒë·ªãnh d·∫°ng chu·∫©n."""
        try:
            current_dt = current_data.get("dt")
            if not current_dt: return None

            standardized = {
                "location": {
                    "name": current_data.get("name", ""),
                    "country": current_data.get("sys", {}).get("country", ""),
                    "lat": current_data.get("coord", {}).get("lat"),
                    "lon": current_data.get("coord", {}).get("lon"),
                    "localtime": datetime.datetime.fromtimestamp(current_dt, tz=datetime.timezone.utc).astimezone().strftime("%Y-%m-%d %H:%M:%S %Z")
                },
                "current": {
                    "temp_c": current_data["main"]["temp"],
                    "temp_f": current_data["main"]["temp"] * 9/5 + 32,
                    "is_day": 1 if current_data.get("sys", {}).get("sunrise", 0) < current_dt < current_data.get("sys", {}).get("sunset", float('inf')) else 0,
                    "condition": {
                        "text": current_data["weather"][0]["description"],
                        "icon": f"https://openweathermap.org/img/wn/{current_data['weather'][0]['icon']}@2x.png"
                    },
                    "wind_kph": current_data.get("wind", {}).get("speed", 0) * 3.6,
                    "wind_dir": self._get_wind_direction(current_data.get("wind", {}).get("deg", 0)),
                    "humidity": current_data["main"]["humidity"],
                    "feelslike_c": current_data["main"]["feels_like"],
                    # Basic API doesn't provide UV reliably
                    "uv": None,
                    "pressure_mb": current_data["main"].get("pressure"),
                    "visibility_km": current_data.get("visibility", 10000) / 1000,
                },
                "forecast": []
            }

            # Process forecast data (group by day)
            daily_forecasts = {} # { "YYYY-MM-DD": {temps:[], conditions:[], rain_chances:[], hourly:[]} }
            if "list" in forecast_data:
                for item in forecast_data["list"]:
                    item_dt_obj = datetime.datetime.fromtimestamp(item["dt"], tz=datetime.timezone.utc)
                    date_str = item_dt_obj.strftime("%Y-%m-%d")

                    if date_str not in daily_forecasts:
                         # Limit number of forecast days collected
                         if len(daily_forecasts) >= days: break
                         daily_forecasts[date_str] = {"temps": [], "conditions": [], "icons": [], "rain_chances": [], "hourly": []}

                    daily_forecasts[date_str]["temps"].append(item["main"]["temp"])
                    daily_forecasts[date_str]["conditions"].append(item["weather"][0]["description"])
                    daily_forecasts[date_str]["icons"].append(item["weather"][0]["icon"])
                    daily_forecasts[date_str]["rain_chances"].append(item.get("pop", 0) * 100)

                    # Add hourly detail
                    hour_data = {
                        "time": item_dt_obj.astimezone().strftime("%H:%M"),
                        "temp_c": item["main"]["temp"],
                        "condition": {
                            "text": item["weather"][0]["description"],
                            "icon": f"https://openweathermap.org/img/wn/{item['weather'][0]['icon']}@2x.png"
                        },
                        "chance_of_rain": item.get("pop", 0) * 100
                    }
                    daily_forecasts[date_str]["hourly"].append(hour_data)

            # Aggregate daily forecasts
            for date_str, data in daily_forecasts.items():
                if not data["temps"]: continue # Skip empty days

                # Determine dominant condition/icon for the day (simple mode)
                main_condition = max(set(data["conditions"]), key=data["conditions"].count) if data["conditions"] else ""
                main_icon = max(set(data["icons"]), key=data["icons"].count) if data["icons"] else "01d" # Default icon

                day_forecast = {
                    "date": date_str,
                    "max_temp_c": max(data["temps"]),
                    "min_temp_c": min(data["temps"]),
                    "condition": {
                        "text": main_condition,
                        "icon": f"https://openweathermap.org/img/wn/{main_icon}@2x.png"
                    },
                    "chance_of_rain": max(data["rain_chances"]) if data["rain_chances"] else 0,
                    "hourly": data["hourly"] # Add all collected hourly data for the day
                    # Sunrise/sunset not available in basic forecast API
                }
                standardized["forecast"].append(day_forecast)

            return standardized

        except (KeyError, IndexError, TypeError, ValueError) as e:
            logger.error(f"L·ªói khi k·∫øt h·ª£p d·ªØ li·ªáu th·ªùi ti·∫øt c∆° b·∫£n: {e}", exc_info=True)
            return None

    def _get_wind_direction(self, degrees: float) -> str:
        """Chuy·ªÉn ƒë·ªïi g√≥c gi√≥ (ƒë·ªô) sang h∆∞·ªõng gi√≥."""
        if degrees is None: return "Kh√¥ng r√µ"
        directions = ["B·∫Øc", "ƒê√¥ng B·∫Øc", "ƒê√¥ng", "ƒê√¥ng Nam", "Nam", "T√¢y Nam", "T√¢y", "T√¢y B·∫Øc"]
        index = round(degrees / 45) % 8
        return directions[index]

    @staticmethod
    def format_weather_message(weather_data: Dict[str, Any], location: str, days: int = 1, target_date: str = None, advice_only: bool = False) -> str:
        """ƒê·ªãnh d·∫°ng d·ªØ li·ªáu th·ªùi ti·∫øt th√†nh th√¥ng ƒëi·ªáp HTML (c·∫≠p nh·∫≠t ƒë·ªÉ s·ª≠ d·ª•ng d·ªØ li·ªáu m·ªõi)."""
        if weather_data.get("error"):
            # ... (error handling remains the same) ...
             title = f"Th√¥ng tin th·ªùi ti·∫øt cho {location}"
             if target_date:
                try:
                    date_obj = datetime.datetime.strptime(target_date, "%Y-%m-%d")
                    formatted_date = date_obj.strftime("%d/%m/%Y")
                    weekday_names = ["Th·ª© Hai", "Th·ª© Ba", "Th·ª© T∆∞", "Th·ª© NƒÉm", "Th·ª© S√°u", "Th·ª© B·∫£y", "Ch·ªß Nh·∫≠t"]
                    weekday = weekday_names[date_obj.weekday()]
                    title = f"Th√¥ng tin th·ªùi ti·∫øt cho {location} v√†o {weekday} ({formatted_date})"
                except:
                    title = f"Th√¥ng tin th·ªùi ti·∫øt cho {location} v√†o {target_date}"

             return f"""
             <h3>{title}</h3>
             <p>{weather_data.get('message', 'ƒêang g·∫∑p s·ª± c·ªë khi l·∫•y d·ªØ li·ªáu th·ªùi ti·∫øt.')}</p>
             <p><i>{weather_data.get('recommendation', '')}</i></p>
             """

        current = weather_data.get("current", {})
        location_info = weather_data.get("location", {})
        actual_location = location_info.get("name", location)
        forecast = weather_data.get("forecast", [])

        target_forecast = None
        if target_date and forecast:
            for day_forecast in forecast:
                if day_forecast.get("date") == target_date:
                    target_forecast = day_forecast
                    break

        # Advice Only Mode
        if advice_only:
            temp_desc = ""
            weather_cond = ""
            rain_info = ""
            date_str = ""

            if target_forecast:
                min_temp = target_forecast.get("min_temp_c", 0)
                max_temp = target_forecast.get("max_temp_c", 0)
                temp_desc = f"nhi·ªát ƒë·ªô t·ª´ {min_temp:.1f}¬∞C ƒë·∫øn {max_temp:.1f}¬∞C"
                weather_cond = target_forecast.get("condition", {}).get("text", "")
                rain_chance = target_forecast.get("chance_of_rain", 0)
                if rain_chance > 30:
                    rain_info = f", {rain_chance:.0f}% kh·∫£ nƒÉng m∆∞a"
                date_str = f" v√†o ng√†y {target_date}"
            elif current: # Use current if no target forecast
                temp_c = current.get("temp_c", 0)
                feels_like = current.get("feelslike_c", temp_c) # Use feels_like if available
                temp_desc = f"nhi·ªát ƒë·ªô hi·ªán t·∫°i {temp_c:.1f}¬∞C, c·∫£m gi√°c nh∆∞ {feels_like:.1f}¬∞C"
                weather_cond = current.get("condition", {}).get("text", "")
                humidity = current.get("humidity", 0)
                if humidity > 70:
                    rain_info = f", ƒë·ªô ·∫©m kh√° cao ({humidity}%)"
                date_str = " hi·ªán t·∫°i"
            else: # No data available
                 return f"Kh√¥ng c√≥ ƒë·ªß d·ªØ li·ªáu th·ªùi ti·∫øt t·∫°i {actual_location} ƒë·ªÉ ƒë∆∞a ra t∆∞ v·∫•n."


            return f"Th·ªùi ti·∫øt t·∫°i {actual_location}{date_str}: {temp_desc}, tr·ªùi {weather_cond}{rain_info}."

        # Normal Formatting Mode
        weather_emoji = ""
        title = ""

        if target_forecast:
            weather_emoji = WeatherService._get_weather_emoji(target_forecast.get("condition", {}).get("text", "").lower())
            try:
                date_obj = datetime.datetime.strptime(target_date, "%Y-%m-%d")
                formatted_date = date_obj.strftime("%d/%m/%Y")
                weekday_names = ["Th·ª© Hai", "Th·ª© Ba", "Th·ª© T∆∞", "Th·ª© NƒÉm", "Th·ª© S√°u", "Th·ª© B·∫£y", "Ch·ªß Nh·∫≠t"]
                weekday = weekday_names[date_obj.weekday()]
                title = f"Th·ªùi ti·∫øt {actual_location} - {weekday}, {formatted_date} {weather_emoji}"
            except:
                title = f"Th·ªùi ti·∫øt {actual_location} - {target_date} {weather_emoji}"
        elif current:
            weather_emoji = WeatherService._get_weather_emoji(current.get("condition", {}).get("text", "").lower())
            title = f"Th·ªùi ti·∫øt hi·ªán t·∫°i ·ªü {actual_location} {weather_emoji}"
        else:
             return f"<h3>Kh√¥ng c√≥ d·ªØ li·ªáu th·ªùi ti·∫øt cho {location}</h3>"


        result = f"<h3>{title}</h3>\n"

        # Display main info (target day or current)
        if target_forecast:
            result += f"<p><b>Nhi·ªát ƒë·ªô:</b> {target_forecast.get('min_temp_c', 'N/A'):.1f}¬∞C - {target_forecast.get('max_temp_c', 'N/A'):.1f}¬∞C</p>\n"
            result += f"<p><b>Th·ªùi ti·∫øt:</b> {target_forecast.get('condition', {}).get('text', 'N/A').capitalize()}</p>\n"
            result += f"<p><b>Kh·∫£ nƒÉng m∆∞a:</b> {target_forecast.get('chance_of_rain', 'N/A'):.0f}%</p>\n"
            if target_forecast.get('humidity') is not None:
                result += f"<p><b>ƒê·ªô ·∫©m:</b> {target_forecast.get('humidity'):.0f}%</p>\n"
            if target_forecast.get('wind_kph') is not None:
                 wind_dir = WeatherService._get_wind_direction(target_forecast.get('wind_deg', 0)) # Assuming wind_deg is available
                 result += f"<p><b>Gi√≥:</b> {target_forecast.get('wind_kph'):.1f} km/h (h∆∞·ªõng {wind_dir})</p>\n" # Need wind direction too
            if "sunrise" in target_forecast and "sunset" in target_forecast:
                result += f"<p><b>M·∫∑t tr·ªùi:</b> m·ªçc {target_forecast.get('sunrise', 'N/A')} - l·∫∑n {target_forecast.get('sunset', 'N/A')}</p>\n"
            if target_forecast.get("summary"):
                 result += f"<p><b>T√≥m t·∫Øt:</b> {target_forecast['summary']}</p>\n"

        elif current:
            result += f"<p><b>Hi·ªán t·∫°i:</b> {current.get('temp_c', 'N/A'):.1f}¬∞C (c·∫£m gi√°c nh∆∞ {current.get('feelslike_c', 'N/A'):.1f}¬∞C)</p>\n"
            result += f"<p><b>Th·ªùi ti·∫øt:</b> {current.get('condition', {}).get('text', 'N/A').capitalize()}</p>\n"
            result += f"<p><b>ƒê·ªô ·∫©m:</b> {current.get('humidity', 'N/A')}%</p>\n"
            result += f"<p><b>Gi√≥:</b> {current.get('wind_kph', 'N/A'):.1f} km/h (h∆∞·ªõng {current.get('wind_dir', 'N/A')})</p>\n"
            if current.get('uv') is not None:
                result += f"<p><b>Ch·ªâ s·ªë UV:</b> {current.get('uv'):.1f}</p>\n"


        # Display Hourly Forecast (if available for the displayed day)
        hourly_data = None
        if target_forecast and target_forecast.get("hourly"):
            hourly_data = target_forecast["hourly"]
            result += f"<h4>D·ª± b√°o theo gi·ªù ({target_forecast['date']}):</h4>\n"
        elif not target_date and forecast and forecast[0].get("date") == datetime.datetime.now().strftime("%Y-%m-%d") and forecast[0].get("hourly"):
             hourly_data = forecast[0]["hourly"]
             result += f"<h4>D·ª± b√°o theo gi·ªù (H√¥m nay):</h4>\n"

        if hourly_data:
            result += "<ul>\n"
            max_hourly_entries = 8 # Limit displayed hours
            count = 0
            for hour in hourly_data:
                if count >= max_hourly_entries: break
                hour_emoji = WeatherService._get_weather_emoji(hour.get("condition", {}).get("text", "").lower())
                rain_text = f"{hour.get('chance_of_rain', 0):.0f}% m∆∞a"
                result += f"<li><b>{hour.get('time', '')}:</b> {hour_emoji} {hour.get('temp_c', 'N/A'):.1f}¬∞C, {hour.get('condition', {}).get('text', '').capitalize()}, {rain_text}</li>\n"
                count += 1
            result += "</ul>\n"


        # Display Forecast for subsequent days
        # Start index depends on whether target_date was shown
        start_forecast_index = 0
        if target_date:
            # Find the index after the target date
            for i, day in enumerate(forecast):
                if day.get("date") == target_date:
                    start_forecast_index = i + 1
                    break
        elif forecast: # If showing current, forecast starts from index 0 (today) or 1 (if today is not in forecast)
             if forecast[0].get("date") != datetime.datetime.now().strftime("%Y-%m-%d"):
                  start_forecast_index = 0 # Forecast data starts tomorrow
             else:
                  start_forecast_index = 1 # Skip today if already shown hourly/current

        if forecast and len(forecast) > start_forecast_index and days > 1 :
            result += "<h4>D·ª± b√°o c√°c ng√†y t·ªõi:</h4>\n<ul>\n"
            days_shown = 0
            max_forecast_days_to_show = days -1 # Show remaining days requested

            for i in range(start_forecast_index, len(forecast)):
                if days_shown >= max_forecast_days_to_show: break
                day = forecast[i]
                try:
                    day_date_obj = datetime.datetime.strptime(day.get("date", ""), "%Y-%m-%d")
                    day_date = day_date_obj.strftime("%d/%m")
                    weekday_short = ["T2","T3","T4","T5","T6","T7","CN"][day_date_obj.weekday()]
                    day_display = f"{weekday_short} {day_date}"
                except:
                    day_display = day.get("date", "")

                day_emoji = WeatherService._get_weather_emoji(day.get("condition", {}).get("text", "").lower())
                rain_text = f"{day.get('chance_of_rain', 0):.0f}% m∆∞a"
                temp_text = f"{day.get('min_temp_c', '?'):.0f}-{day.get('max_temp_c', '?'):.0f}¬∞C"

                result += f"<li><b>{day_display}:</b> {day_emoji} {day.get('condition', {}).get('text', '').capitalize()}, {temp_text}, {rain_text}</li>\n"
                days_shown += 1

            result += "</ul>\n"

        # Add update time
        result += f"<p><i>C·∫≠p nh·∫≠t l√∫c: {location_info.get('localtime', 'Kh√¥ng r√µ')}</i></p>"

        return result

    @staticmethod
    def _get_weather_emoji(condition: str) -> str:
        """Tr·∫£ v·ªÅ emoji ph√π h·ª£p v·ªõi ƒëi·ªÅu ki·ªán th·ªùi ti·∫øt (lowercase input)."""
        condition = condition.lower() # Ensure lowercase comparison
        if any(word in condition for word in ["m∆∞a", "rain", "shower", "drizzle", "d√¥ng"]):
            return "üåßÔ∏è"
        if any(word in condition for word in ["gi√¥ng", "b√£o", "thunder", "storm"]):
            return "‚õàÔ∏è"
        if any(word in condition for word in ["tuy·∫øt", "snow"]):
            return "‚ùÑÔ∏è"
        if any(word in condition for word in ["n·∫Øng", "sunny", "clear", "quang"]):
            return "‚òÄÔ∏è"
        if any(word in condition for word in ["m√¢y", "cloud", "cloudy", "overcast", "√¢m u"]):
            return "‚òÅÔ∏è"
        if any(word in condition for word in ["s∆∞∆°ng m√π", "fog", "mist"]):
            return "üå´Ô∏è"
        return "üå§Ô∏è" # Default: Partly cloudy


    def get_date_from_relative_term(self, term: str) -> Optional[str]:
        """
        Chuy·ªÉn ƒë·ªïi t·ª´ m√¥ t·∫£ t∆∞∆°ng ƒë·ªëi v·ªÅ ng√†y th√†nh ng√†y th·ª±c t·∫ø (YYYY-MM-DD).
        H·ªó tr·ª£: h√¥m nay, ng√†y mai, ng√†y kia, h√¥m qua, th·ª© X tu·∫ßn sau, th·ª© X.
        """
        if not term: return None

        term = term.lower().strip()
        today = datetime.date.today()
        logger.debug(f"Calculating date for term: '{term}', today is: {today.strftime('%Y-%m-%d %A')}")

        # Basic relative terms
        if term in ["h√¥m nay", "today"]: return today.strftime("%Y-%m-%d")
        if term in ["ng√†y mai", "mai", "tomorrow"]: return (today + datetime.timedelta(days=1)).strftime("%Y-%m-%d")
        if term in ["ng√†y kia", "day after tomorrow"]: return (today + datetime.timedelta(days=2)).strftime("%Y-%m-%d")
        if term in ["h√¥m qua", "yesterday"]: return (today - datetime.timedelta(days=1)).strftime("%Y-%m-%d")

        # Specific weekdays
        target_weekday = -1
        is_next_week = False
        term_for_weekday_search = term

        for kw in ["tu·∫ßn sau", "tu·∫ßn t·ªõi", "next week"]:
            if kw in term:
                is_next_week = True
                term_for_weekday_search = term.replace(kw, "").strip()
                break

        for day_str, day_num in self.VIETNAMESE_WEEKDAY_MAP.items():
            if re.search(r'\b' + re.escape(day_str) + r'\b', term_for_weekday_search):
                target_weekday = day_num
                logger.debug(f"Found target weekday: {day_str} ({target_weekday}) in '{term_for_weekday_search}'")
                break

        if target_weekday != -1:
            today_weekday = today.weekday() # Monday is 0, Sunday is 6
            if is_next_week:
                days_to_next_monday = (7 - today_weekday) % 7 # Days until next Monday (0 if today is Monday)
                if days_to_next_monday == 0: days_to_next_monday = 7 # If today is Monday, jump to next week's Monday
                next_monday_date = today + datetime.timedelta(days=days_to_next_monday)
                final_date = next_monday_date + datetime.timedelta(days=target_weekday)
            else: # Upcoming weekday
                days_ahead = (target_weekday - today_weekday + 7) % 7
                if days_ahead == 0: days_ahead = 7 # If asking for today's weekday, mean next week's
                final_date = today + datetime.timedelta(days=days_ahead)

            logger.info(f"Calculated date for '{term}': {final_date.strftime('%Y-%m-%d %A')}")
            return final_date.strftime("%Y-%m-%d")

        # Fallback general terms
        if any(kw in term for kw in ["tu·∫ßn sau", "tu·∫ßn t·ªõi", "next week"]):
            days_to_next_monday = (7 - today.weekday()) % 7
            if days_to_next_monday == 0: days_to_next_monday = 7
            calculated_date = today + datetime.timedelta(days=days_to_next_monday) # Next Monday
            logger.info(f"Calculated date for general 'next week': {calculated_date.strftime('%Y-%m-%d')} (Next Monday)")
            return calculated_date.strftime("%Y-%m-%d")
        if "th√°ng t·ªõi" in term or "th√°ng sau" in term or "next month" in term:
             # Calculate first day of next month
             next_month_date = (today.replace(day=1) + datetime.timedelta(days=32)).replace(day=1)
             logger.info(f"Calculated date for 'next month': {next_month_date.strftime('%Y-%m-%d')}")
             return next_month_date.strftime("%Y-%m-%d")


        # Check explicit date formats
        try:
            if re.match(r'\d{4}-\d{2}-\d{2}', term):
                 parsed_date = datetime.datetime.strptime(term, "%Y-%m-%d").date()
                 logger.info(f"Term '{term}' is YYYY-MM-DD format.")
                 return parsed_date.strftime("%Y-%m-%d")
            if re.match(r'\d{1,2}/\d{1,2}/\d{4}', term):
                 parsed_date = datetime.datetime.strptime(term, "%d/%m/%Y").date()
                 logger.info(f"Term '{term}' is DD/MM/YYYY format, normalized.")
                 return parsed_date.strftime("%Y-%m-%d")
            if re.match(r'\d{1,2}/\d{1,2}', term): # e.g., 20/7
                 day, month = map(int, term.split('/'))
                 current_year = today.year
                 parsed_date = datetime.date(current_year, month, day)
                 # If date is in the past, assume next year
                 if parsed_date < today:
                      parsed_date = datetime.date(current_year + 1, month, day)
                 logger.info(f"Term '{term}' is DD/MM format, assumed year {parsed_date.year}.")
                 return parsed_date.strftime("%Y-%m-%d")

        except ValueError:
            logger.warning(f"Term '{term}' looks like a date but is invalid.")
            pass # Not a valid date format or value

        logger.warning(f"Could not interpret relative date term: '{term}'. Returning None.")
        return None

    @staticmethod
    def detect_weather_query(text: str) -> Tuple[bool, Optional[str], Optional[int], Optional[str]]:
        """Ph√°t hi·ªán c√¢u h·ªèi th·ªùi ti·∫øt, tr√≠ch xu·∫•t v·ªã tr√≠, s·ªë ng√†y, c·ª•m t·ª´ th·ªùi gian."""
        text_lower = text.lower()
        weather_keywords = [
            "th·ªùi ti·∫øt", "d·ª± b√°o", "nhi·ªát ƒë·ªô", "n·∫Øng", "m∆∞a", "gi√≥", "b√£o",
            "gi√¥ng", "n√≥ng", "l·∫°nh", "ƒë·ªô ·∫©m", "m·∫•y ƒë·ªô", "bao nhi√™u ƒë·ªô"
        ]
        # More specific time keywords
        time_keywords = {
            # Relative days
            "h√¥m nay": 1, "nay": 1,
            "ng√†y mai": 2, "mai": 2,
            "ng√†y kia": 3, "m·ªët": 3, "ng√†y m·ªët": 3,
            "h√¥m qua": 0, # Indicate past, maybe return error or current
            # Week references
            "cu·ªëi tu·∫ßn": 3, # Approx days needed to cover weekend
            "tu·∫ßn n√†y": 7,
            "tu·∫ßn t·ªõi": 7, "tu·∫ßn sau": 7,
            # Specific weekdays (handled by get_date_from_relative_term later)
            # Number of days
            "3 ng√†y t·ªõi": 3, "ba ng√†y t·ªõi": 3,
            "5 ng√†y t·ªõi": 5, "nƒÉm ng√†y t·ªõi": 5,
            "7 ng√†y t·ªõi": 7, "b·∫£y ng√†y t·ªõi": 7, "m·ªôt tu·∫ßn t·ªõi": 7,
        }

        is_weather_query = any(keyword in text_lower for keyword in weather_keywords)
        if not is_weather_query:
            return False, None, None, None

        days = 1
        time_term = "h√¥m nay" # Default

        # Check for specific day counts first
        day_count_match = re.search(r'(\d+|m·ªôt|hai|ba|b·ªën|nƒÉm|s√°u|b·∫£y)\s+ng√†y\s+t·ªõi', text_lower)
        num_map = {"m·ªôt": 1, "hai": 2, "ba": 3, "b·ªën": 4, "nƒÉm": 5, "s√°u": 6, "b·∫£y": 7}
        if day_count_match:
             count_str = day_count_match.group(1)
             try:
                  days = int(count_str)
             except ValueError:
                  days = num_map.get(count_str, 1)
             time_term = f"{days} ng√†y t·ªõi"

        # Check other time keywords if no day count found
        if time_term == "h√¥m nay": # Only override default if found
             # Sort keywords by length descending to match longer phrases first
            sorted_time_keywords = sorted(time_keywords.keys(), key=len, reverse=True)
            for term, days_value in time_keywords.items(): # Use original map for days_value
                if term in text_lower:
                    time_term = term
                    # Use days_value only if it's larger than current 'days' (from potential day count)
                    days = max(days, days_value if days_value > 0 else 1) # Ensure days >= 1
                    break

        # Check for specific weekdays like "th·ª© 6 tu·∫ßn sau"
        weekday_match = re.search(r'(th·ª©\s+[2-7]|th·ª©\s+(hai|ba|t∆∞|nƒÉm|s√°u|b·∫£y)|ch·ªß\s+nh·∫≠t|cn)(\s+tu·∫ßn\s+(n√†y|sau|t·ªõi))?', text_lower)
        if weekday_match:
             # Extract the full term like "th·ª© 6 tu·∫ßn sau"
             time_term = weekday_match.group(0).strip()
             # Estimate days needed (max 7 for a week)
             days = max(days, 7)


        # Extract location (improved)
        location = None
        # Look for patterns like "th·ªùi ti·∫øt ·ªü/t·∫°i [Location]"
        loc_pattern1 = r'(th·ªùi\s+ti·∫øt|d·ª±\s+b√°o)\s+(·ªü|t·∫°i)\s+([^?.,!\n]+)'
        match1 = re.search(loc_pattern1, text_lower)
        if match1:
            location = match1.group(3).strip().title()
        else:
            # Look for location at the end, possibly after time phrase
            loc_pattern2 = r'\b(·ªü|t·∫°i)\s+([^?.,!\n]+)$' # Location at the very end
            match2 = re.search(loc_pattern2, text_lower)
            if match2:
                 location = match2.group(2).strip().title()
            else:
                 # Try common locations
                 popular_locations = [
                    "h√† n·ªôi", "h·ªì ch√≠ minh", "tp hcm", "s√†i g√≤n", "ƒë√† n·∫µng", "h·∫£i ph√≤ng",
                    "c·∫ßn th∆°", "hu·∫ø", "nha trang", "ƒë√† l·∫°t", "v≈©ng t√†u", "quy nh∆°n",
                    "ph√∫ qu·ªëc", "h·ªôi an", "nam ƒë·ªãnh", "h√† giang", "l√†o cai", "sapa",
                    "b·∫Øc ninh", "th√°i nguy√™n", "vinh", "thanh h√≥a", "bu√¥n ma thu·ªôt", "c√† mau"
                 ]
                 # Check from longest to shortest to avoid partial matches e.g. "h√†" in "h√† n·ªôi"
                 sorted_locations = sorted(popular_locations, key=len, reverse=True)
                 for loc in sorted_locations:
                      # Use word boundary to avoid matching parts of words
                     if re.search(r'\b' + re.escape(loc) + r'\b', text_lower):
                         location = loc.title()
                         # Specific mapping
                         if location == "Tp Hcm": location = "H·ªì Ch√≠ Minh"
                         if location == "S√†i G√≤n": location = "H·ªì Ch√≠ Minh"
                         break

        # Default location if none found
        if not location:
            location = "H√† N·ªôi"
            logger.info("Kh√¥ng t√¨m th·∫•y v·ªã tr√≠ th·ªùi ti·∫øt, m·∫∑c ƒë·ªãnh l√† H√† N·ªôi.")

        logger.info(f"Ph√°t hi·ªán truy v·∫•n th·ªùi ti·∫øt: V·ªã tr√≠='{location}', C·ª•m t·ª´ th·ªùi gian='{time_term}', S·ªë ng√†y ∆∞·ªõc t√≠nh={days}")
        return True, location, days, time_term


# --- Weather Advisor ---
class WeatherAdvisor:
    CLOTHING_TEMP_RANGES = { # Use tuples for keys for ordering/comparison if needed
        (-float('inf'), 15): "r·∫•t l·∫°nh",
        (15, 20): "l·∫°nh",
        (20, 25): "m√°t m·∫ª",
        (25, 29): "·∫•m √°p",
        (29, 35): "n√≥ng",
        (35, float('inf')): "r·∫•t n√≥ng"
    }
    WEATHER_CONDITIONS_KEYWORDS = {
        "m∆∞a": ["m∆∞a", "rain", "shower", "drizzle", "d√¥ng"],
        "gi√¥ng b√£o": ["gi√¥ng", "b√£o", "thunder", "storm"],
        "tuy·∫øt": ["tuy·∫øt", "snow"],
        "n·∫Øng": ["n·∫Øng", "sunny", "clear", "quang"],
        "m√¢y": ["m√¢y", "cloud", "cloudy", "overcast", "u √°m"],
        "gi√≥": ["gi√≥", "wind", "windy"],
        "s∆∞∆°ng m√π": ["s∆∞∆°ng m√π", "fog", "mist"],
    }

    def __init__(self):
        pass

    @staticmethod
    def detect_advice_query(text: str) -> Tuple[bool, str, Optional[str]]:
        """Ph√°t hi·ªán c√¢u h·ªèi xin t∆∞ v·∫•n d·ª±a tr√™n th·ªùi ti·∫øt."""
        text_lower = text.lower()

        # Keywords MUST indicate asking for advice *in relation to weather*
        advice_trigger = ["th·ªùi ti·∫øt", "ng√†y mai", "h√¥m nay", "cu·ªëi tu·∫ßn", "ƒëi ch∆°i", "ra ngo√†i"] # Context words
        if not any(trigger in text_lower for trigger in advice_trigger):
             # If none of the context words are present, it's less likely weather advice
             # Check if temperature/rain mentioned explicitly
             if not any(cond in text_lower for cond in ["n√≥ng", "l·∫°nh", "m∆∞a", "n·∫Øng", "ƒë·ªô"]):
                  return False, "", None # Not clearly weather related advice

        # Advice types
        clothing_keywords = ["m·∫∑c g√¨", "trang ph·ª•c", "qu·∫ßn √°o", "ƒÉn m·∫∑c", "ƒë·ªì g√¨"]
        activity_keywords = ["l√†m g√¨", "ƒëi ƒë√¢u", "ch∆°i g√¨", "ho·∫°t ƒë·ªông", "n√™n ƒëi"]
        item_keywords = ["mang g√¨", "mang theo", "chu·∫©n b·ªã g√¨", "ƒëem theo", "c·∫ßn g√¨"]

        advice_type = ""
        if any(keyword in text_lower for keyword in clothing_keywords): advice_type = "clothing"
        elif any(keyword in text_lower for keyword in activity_keywords): advice_type = "activity"
        elif any(keyword in text_lower for keyword in item_keywords): advice_type = "items"

        if not advice_type:
            return False, "", None # No specific advice type keyword found

        # Find time term (similar to weather detection, maybe reuse?)
        time_term = None
        # Prioritize specific weekdays first
        weekday_match = re.search(r'(th·ª©\s+[2-7]|th·ª©\s+(hai|ba|t∆∞|nƒÉm|s√°u|b·∫£y)|ch·ªß\s+nh·∫≠t|cn)(\s+tu·∫ßn\s+(n√†y|sau|t·ªõi))?', text_lower)
        if weekday_match:
             time_term = weekday_match.group(0).strip()
        else:
             # Check relative terms (longest first)
            time_keywords = [
                "ng√†y kia", "ng√†y m·ªët", "cu·ªëi tu·∫ßn n√†y", "cu·ªëi tu·∫ßn sau", "tu·∫ßn t·ªõi", "tu·∫ßn sau",
                "ng√†y mai", "mai", "t·ªëi nay", "s√°ng mai", "chi·ªÅu mai", "t·ªëi mai", "h√¥m nay", "nay"
            ]
            for keyword in time_keywords:
                if keyword in text_lower:
                    time_term = keyword
                    break

        # Default to "h√¥m nay" if asking generally or no time found
        if not time_term:
            time_term = "h√¥m nay"

        logger.info(f"Ph√°t hi·ªán truy v·∫•n t∆∞ v·∫•n: Lo·∫°i={advice_type}, Th·ªùi gian='{time_term}'")
        return True, advice_type, time_term

    def _get_relevant_weather(self, weather_data: Dict[str, Any], target_date: Optional[str]) -> Optional[Dict[str, Any]]:
        """Extracts weather info for the target date or current conditions."""
        if target_date and weather_data.get("forecast"):
            for day in weather_data["forecast"]:
                if day.get("date") == target_date:
                    # Return a combined structure for easier access
                    return {
                        "temp_min": day.get("min_temp_c"),
                        "temp_max": day.get("max_temp_c"),
                        "condition_text": day.get("condition", {}).get("text", "").lower(),
                        "rain_chance": day.get("chance_of_rain"),
                        "humidity": day.get("humidity"),
                        "uv": day.get("uv")
                    }
        elif weather_data.get("current"): # Use current if no target date or forecast not found
            current = weather_data["current"]
            return {
                "temp_c": current.get("temp_c"),
                "feels_like": current.get("feelslike_c"),
                "condition_text": current.get("condition", {}).get("text", "").lower(),
                "rain_chance": 0, # Assume 0 chance for current 'pop' usually not in basic current weather
                "humidity": current.get("humidity"),
                "uv": current.get("uv")
            }
        return None # No relevant data found

    def _get_temp_category(self, temp_info: Dict[str, Any]) -> str:
        """Determines the temperature category (r·∫•t l·∫°nh, l·∫°nh, etc.)."""
        temp_to_check = None
        if temp_info.get("temp_min") is not None and temp_info.get("temp_max") is not None:
            temp_to_check = (temp_info["temp_min"] + temp_info["temp_max"]) / 2 # Average for forecast
        elif temp_info.get("feels_like") is not None:
             temp_to_check = temp_info["feels_like"] # Use feels_like for current
        elif temp_info.get("temp_c") is not None:
             temp_to_check = temp_info["temp_c"] # Use actual temp if feels_like unavailable

        if temp_to_check is None: return "√¥n h√≤a" # Default if no temp data

        for (min_t, max_t), category in self.CLOTHING_TEMP_RANGES.items():
            if min_t <= temp_to_check < max_t:
                return category
        return "√¥n h√≤a" # Fallback

    def _get_dominant_condition(self, condition_text: str, rain_chance: float) -> str:
        """Determines the most significant weather condition."""
        condition_text = condition_text.lower()
        if rain_chance > 60: return "m∆∞a" # High chance of rain overrides others
        if rain_chance > 30: return "c√≥ th·ªÉ m∆∞a"

        for state, keywords in self.WEATHER_CONDITIONS_KEYWORDS.items():
            if any(keyword in condition_text for keyword in keywords):
                return state # Return the first matching category

        return "quang ƒë√£ng" # Default if nothing significant matches


    def get_clothing_advice(self, weather_data: Dict[str, Any], target_date: str = None) -> str:
        """ƒê∆∞a ra l·ªùi khuy√™n v·ªÅ trang ph·ª•c."""
        weather_info = self._get_relevant_weather(weather_data, target_date)
        if not weather_info:
            return "Kh√¥ng ƒë·ªß th√¥ng tin th·ªùi ti·∫øt ƒë·ªÉ ƒë∆∞a ra l·ªùi khuy√™n trang ph·ª•c."

        temp_category = self._get_temp_category(weather_info)
        condition_text = weather_info.get("condition_text", "")
        rain_chance = weather_info.get("rain_chance", 0)
        dominant_condition = self._get_dominant_condition(condition_text, rain_chance)

        return self._generate_clothing_advice(temp_category, dominant_condition)


    def get_activity_advice(self, weather_data: Dict[str, Any], target_date: str = None) -> str:
        """ƒê∆∞a ra l·ªùi khuy√™n v·ªÅ ho·∫°t ƒë·ªông."""
        weather_info = self._get_relevant_weather(weather_data, target_date)
        if not weather_info:
            return "Kh√¥ng ƒë·ªß th√¥ng tin th·ªùi ti·∫øt ƒë·ªÉ ƒë∆∞a ra l·ªùi khuy√™n ho·∫°t ƒë·ªông."

        temp_category = self._get_temp_category(weather_info)
        condition_text = weather_info.get("condition_text", "")
        rain_chance = weather_info.get("rain_chance", 0)
        dominant_condition = self._get_dominant_condition(condition_text, rain_chance)

        return self._generate_activity_advice(temp_category, dominant_condition)

    def get_items_advice(self, weather_data: Dict[str, Any], target_date: str = None) -> str:
        """ƒê∆∞a ra l·ªùi khuy√™n v·ªÅ ƒë·ªì d√πng c·∫ßn mang theo."""
        weather_info = self._get_relevant_weather(weather_data, target_date)
        if not weather_info:
            return "Kh√¥ng ƒë·ªß th√¥ng tin th·ªùi ti·∫øt ƒë·ªÉ ƒë∆∞a ra l·ªùi khuy√™n v·∫≠t d·ª•ng."

        temp_category = self._get_temp_category(weather_info)
        condition_text = weather_info.get("condition_text", "")
        rain_chance = weather_info.get("rain_chance", 0)
        dominant_condition = self._get_dominant_condition(condition_text, rain_chance)
        uv_index = weather_info.get("uv")
        humidity = weather_info.get("humidity")

        return self._generate_items_advice(temp_category, dominant_condition, uv_index, humidity)


    def _generate_clothing_advice(self, temp_category: str, dominant_condition: str) -> str:
        """T·∫°o l·ªùi khuy√™n chi ti·∫øt v·ªÅ trang ph·ª•c."""
        advice = f"V·ªõi th·ªùi ti·∫øt ƒë∆∞·ª£c d·ª± b√°o l√† **{temp_category}** v√† **{dominant_condition}**, b·∫°n n√™n c√¢n nh·∫Øc:"
        items = []

        # Temperature based advice
        if temp_category == "r·∫•t l·∫°nh": items.extend(["√Åo kho√°c d√†y, √°o len", "M≈©, khƒÉn, gƒÉng tay", "Qu·∫ßn d√†y, gi√†y ·∫•m"])
        elif temp_category == "l·∫°nh": items.extend(["√Åo kho√°c v·ª´a", "√Åo d√†i tay", "Qu·∫ßn d√†i"])
        elif temp_category == "m√°t m·∫ª": items.extend(["√Åo kho√°c m·ªèng ho·∫∑c cardigan", "√Åo thun/s∆° mi d√†i tay", "Qu·∫ßn d√†i tho·∫£i m√°i"])
        elif temp_category == "·∫•m √°p": items.extend(["√Åo thun/s∆° mi ng·∫Øn tay", "Qu·∫ßn l·ª≠ng ho·∫∑c qu·∫ßn d√†i m·ªèng"])
        elif temp_category == "n√≥ng": items.extend(["√Åo thun/ba l·ªó m·ªèng, s√°ng m√†u", "Qu·∫ßn short/v√°y", "Ch·∫•t li·ªáu tho√°ng m√°t (cotton, linen)"])
        elif temp_category == "r·∫•t n√≥ng": items.extend(["Trang ph·ª•c m·ªèng, r·ªông, s√°ng m√†u nh·∫•t c√≥ th·ªÉ", "∆Øu ti√™n v·∫£i th·∫•m h√∫t m·ªì h√¥i", "Sandal ho·∫∑c d√©p tho√°ng kh√≠"])

        # Condition based advice
        if dominant_condition == "m∆∞a": items.extend(["**√î ho·∫∑c √°o m∆∞a**", "**Gi√†y ch·ªëng n∆∞·ªõc**"])
        elif dominant_condition == "c√≥ th·ªÉ m∆∞a": items.append("Mang theo √¥ d·ª± ph√≤ng")
        elif dominant_condition == "n·∫Øng": items.extend(["M≈©/n√≥n r·ªông v√†nh", "K√≠nh r√¢m", "√Åo ch·ªëng n·∫Øng n·∫øu c·∫ßn"])
        elif dominant_condition == "gi√≥": items.append("√Åo kho√°c coupe-vent (ch·∫Øn gi√≥)")
        elif dominant_condition == "gi√¥ng b√£o": items.append("H·∫°n ch·∫ø ra ngo√†i n·∫øu kh√¥ng c·∫ßn thi·∫øt!")

        if items:
            advice += "\n<ul>\n" + "".join([f"<li>{item}</li>" for item in items]) + "</ul>"
        else:
            advice += " M·∫∑c ƒë·ªì tho·∫£i m√°i l√† ƒë∆∞·ª£c."

        advice += "\n<p><i>H√£y ƒëi·ªÅu ch·ªânh theo c·∫£m nh·∫≠n th·ª±c t·∫ø c·ªßa b·∫°n nh√©!</i></p>"
        return advice


    def _generate_activity_advice(self, temp_category: str, dominant_condition: str) -> str:
        """T·∫°o l·ªùi khuy√™n chi ti·∫øt v·ªÅ ho·∫°t ƒë·ªông."""
        advice = f"Th·ªùi ti·∫øt **{temp_category}** v√† **{dominant_condition}** kh√° th√≠ch h·ª£p cho:"
        activities = []

        # Indoor options (always possible)
        indoor = ["Xem phim t·∫°i r·∫°p/nh√†", "ƒê·ªçc s√°ch/gh√© th∆∞ vi·ªán", "ThƒÉm b·∫£o t√†ng/tri·ªÉn l√£m", "ƒêi cafe c√πng b·∫°n b√®", "N·∫•u ƒÉn/h·ªçc m√≥n m·ªõi", "Ch∆°i board game"]
        # Outdoor options (weather dependent)
        outdoor_good = ["ƒêi d·∫°o c√¥ng vi√™n/b·ªù h·ªì", "D√£ ngo·∫°i", "ƒê·∫°p xe", "Ch∆°i th·ªÉ thao ngo√†i tr·ªùi", "Tham quan khu du l·ªãch"]
        outdoor_hot = ["ƒêi b∆°i", "ƒê·∫øn khu vui ch∆°i n∆∞·ªõc", "Ho·∫°t ƒë·ªông trong nh√† c√≥ ƒëi·ªÅu h√≤a", "ƒêi d·∫°o v√†o s√°ng s·ªõm/chi·ªÅu t·ªëi"]

        if dominant_condition in ["m∆∞a", "gi√¥ng b√£o", "tuy·∫øt"]:
            activities.extend(random.sample(indoor, min(len(indoor), 3)))
            advice += " c√°c ho·∫°t ƒë·ªông trong nh√†:"
        elif temp_category in ["n√≥ng", "r·∫•t n√≥ng"]:
            activities.extend(random.sample(outdoor_hot + indoor, min(len(outdoor_hot + indoor), 4)))
            advice += " c√°c ho·∫°t ƒë·ªông tr√°nh n√≥ng ho·∫∑c trong nh√†:"
        elif temp_category in ["m√°t m·∫ª", "·∫•m √°p"] and dominant_condition not in ["m∆∞a", "c√≥ th·ªÉ m∆∞a", "gi√¥ng b√£o"]:
             activities.extend(random.sample(outdoor_good, min(len(outdoor_good), 2)))
             activities.extend(random.sample(indoor, min(len(indoor), 1)))
             advice += " c·∫£ ho·∫°t ƒë·ªông ngo√†i tr·ªùi v√† trong nh√†:"
        else: # Cold or default weather
             activities.extend(random.sample(indoor, min(len(indoor), 2))) # Focus indoor for cold
             if temp_category not in ["r·∫•t l·∫°nh", "l·∫°nh"]:
                  activities.extend(random.sample(outdoor_good, min(len(outdoor_good), 1)))
             advice += " c√°c ho·∫°t ƒë·ªông trong nh√† ho·∫∑c ngo√†i tr·ªùi (n·∫øu b·∫°n th√≠ch):"


        if activities:
             advice += "\n<ul>\n" + "".join([f"<li>{act}</li>" for act in activities]) + "</ul>"
        else:
             advice += " nhi·ªÅu ho·∫°t ƒë·ªông kh√°c nhau t√πy s·ªü th√≠ch c·ªßa b·∫°n."

        return advice

    def _generate_items_advice(self, temp_category: str, dominant_condition: str, uv_index: Optional[float], humidity: Optional[float]) -> str:
        """T·∫°o l·ªùi khuy√™n chi ti·∫øt v·ªÅ ƒë·ªì d√πng."""
        advice = "Khi ra ngo√†i, b·∫°n c√≥ th·ªÉ c·∫ßn mang theo:"
        items = ["ƒêi·ªán tho·∫°i, v√≠ ti·ªÅn, ch√¨a kh√≥a"] # Basics

        if dominant_condition == "m∆∞a": items.extend(["**√î ho·∫∑c √°o m∆∞a**", "T√∫i ch·ªëng n∆∞·ªõc (n·∫øu c·∫ßn)"])
        elif dominant_condition == "c√≥ th·ªÉ m∆∞a": items.append("√î d·ª± ph√≤ng")

        if dominant_condition == "n·∫Øng": items.extend(["K√≠nh r√¢m", "M≈©/n√≥n"])
        if uv_index is not None and uv_index >= 3: items.append(f"**Kem ch·ªëng n·∫Øng (UV {uv_index:.1f})**")

        if temp_category == "r·∫•t l·∫°nh": items.extend(["GƒÉng tay", "KhƒÉn qu√†ng", "M≈© ·∫•m"])
        elif temp_category == "l·∫°nh": items.extend(["GƒÉng tay m·ªèng", "KhƒÉn qu√†ng nh·∫π"])

        if temp_category in ["n√≥ng", "r·∫•t n√≥ng"] or (humidity is not None and humidity > 75):
             items.append("Chai n∆∞·ªõc")
             items.append("KhƒÉn gi·∫•y/khƒÉn tay")

        if dominant_condition == "gi√¥ng b√£o": items.append("**Ch√∫ √Ω an to√†n, h·∫°n ch·∫ø ra ƒë∆∞·ªùng!**")


        if items:
             # Remove duplicates while preserving order (sort of)
            seen = set()
            unique_items = []
            for item in items:
                if item not in seen:
                    unique_items.append(item)
                    seen.add(item)

            advice += "\n<ul>\n" + "".join([f"<li>{item}</li>" for item in unique_items]) + "</ul>"
        else:
             advice += " c√°c v·∫≠t d·ª•ng c√° nh√¢n c·∫ßn thi·∫øt."

        return advice


# --- Tool Definitions (JSON Schema) ---
available_tools = [
    {
        "type": "function",
        "function": {
            "name": "add_family_member",
            "description": "Th√™m m·ªôt th√†nh vi√™n m·ªõi v√†o danh s√°ch gia ƒë√¨nh.",
            "parameters": {
                "type": "object",
                "properties": {
                    "name": {"type": "string", "description": "T√™n ƒë·∫ßy ƒë·ªß c·ªßa th√†nh vi√™n."},
                    "age": {"type": "string", "description": "Tu·ªïi c·ªßa th√†nh vi√™n (v√≠ d·ª•: '30', 'kho·∫£ng 10')."},
                    "preferences": {
                        "type": "object",
                        "description": "S·ªü th√≠ch c·ªßa th√†nh vi√™n (v√≠ d·ª•: {'food': 'Ph·ªü', 'hobby': 'ƒê·ªçc s√°ch'}).",
                        "properties": { # Allow dynamic properties or define common ones
                            "food": {"type": "string", "description": "M√≥n ƒÉn y√™u th√≠ch."},
                            "hobby": {"type": "string", "description": "S·ªü th√≠ch ch√≠nh."},
                            "color": {"type": "string", "description": "M√†u s·∫Øc y√™u th√≠ch."}
                        },
                         "additionalProperties": {"type": "string"} # Allow other preferences
                    }
                },
                "required": ["name"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "update_preference",
            "description": "C·∫≠p nh·∫≠t m·ªôt s·ªü th√≠ch c·ª• th·ªÉ cho m·ªôt th√†nh vi√™n gia ƒë√¨nh ƒë√£ bi·∫øt.",
            "parameters": {
                "type": "object",
                "properties": {
                    "member_id": {"type": "string", "description": "ID c·ªßa th√†nh vi√™n c·∫ßn c·∫≠p nh·∫≠t (l·∫•y t·ª´ th√¥ng tin gia ƒë√¨nh trong context)."},
                    "preference_key": {"type": "string", "description": "Lo·∫°i s·ªü th√≠ch c·∫ßn c·∫≠p nh·∫≠t (v√≠ d·ª•: 'food', 'hobby', 'color')."},
                    "preference_value": {"type": "string", "description": "Gi√° tr·ªã m·ªõi cho s·ªü th√≠ch ƒë√≥."}
                },
                "required": ["member_id", "preference_key", "preference_value"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "add_event",
            "description": "Th√™m m·ªôt s·ª± ki·ªán m·ªõi v√†o l·ªãch gia ƒë√¨nh. H·ªá th·ªëng s·∫Ω t·ª± ƒë·ªông t√≠nh to√°n ng√†y ch√≠nh x√°c t·ª´ m√¥ t·∫£.",
            "parameters": {
                "type": "object",
                "properties": {
                    "title": {"type": "string", "description": "Ti√™u ƒë·ªÅ c·ªßa s·ª± ki·ªán."},
                    "date_description": {"type": "string", "description": "M√¥ t·∫£ v·ªÅ ng√†y di·ªÖn ra s·ª± ki·ªán THEO L·ªúI NG∆Ø·ªúI D√ôNG (v√≠ d·ª•: 'ng√†y mai', 'th·ª© 6 tu·∫ßn sau', '25/12/2024', '20/7'). Kh√¥ng t·ª± t√≠nh to√°n ng√†y."},
                    "time": {"type": "string", "description": "Th·ªùi gian di·ªÖn ra s·ª± ki·ªán (HH:MM). M·∫∑c ƒë·ªãnh l√† 19:00 n·∫øu kh√¥ng ƒë∆∞·ª£c cung c·∫•p.", "default": "19:00"},
                    "description": {"type": "string", "description": "M√¥ t·∫£ chi ti·∫øt v·ªÅ s·ª± ki·ªán, bao g·ªìm c·∫£ th√¥ng tin l·∫∑p l·∫°i n·∫øu c√≥ (v√≠ d·ª•: 'H·ªçp gia ƒë√¨nh h√†ng th√°ng', 'h·ªçc ti·∫øng Anh m·ªói t·ªëi th·ª© 6')."},
                    "participants": {"type": "array", "items": {"type": "string"}, "description": "Danh s√°ch t√™n nh·ªØng ng∆∞·ªùi tham gia."}
                },
                "required": ["title", "date_description"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "update_event",
            "description": "C·∫≠p nh·∫≠t th√¥ng tin cho m·ªôt s·ª± ki·ªán ƒë√£ t·ªìn t·∫°i trong l·ªãch.",
            "parameters": {
                "type": "object",
                "properties": {
                    "event_id": {"type": "string", "description": "ID c·ªßa s·ª± ki·ªán c·∫ßn c·∫≠p nh·∫≠t (l·∫•y t·ª´ danh s√°ch s·ª± ki·ªán trong context)."},
                    "title": {"type": "string", "description": "Ti√™u ƒë·ªÅ m·ªõi cho s·ª± ki·ªán."},
                    "date_description": {"type": "string", "description": "M√¥ t·∫£ M·ªöI v·ªÅ ng√†y di·ªÖn ra s·ª± ki·ªán THEO L·ªúI NG∆Ø·ªúI D√ôNG (n·∫øu thay ƒë·ªïi)."},
                    "time": {"type": "string", "description": "Th·ªùi gian m·ªõi (HH:MM)."},
                    "description": {"type": "string", "description": "M√¥ t·∫£ chi ti·∫øt m·ªõi, bao g·ªìm th√¥ng tin l·∫∑p l·∫°i n·∫øu c√≥."},
                    "participants": {"type": "array", "items": {"type": "string"}, "description": "Danh s√°ch ng∆∞·ªùi tham gia m·ªõi."}
                },
                "required": ["event_id"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "delete_event",
            "description": "X√≥a m·ªôt s·ª± ki·ªán kh·ªèi l·ªãch gia ƒë√¨nh.",
            "parameters": {
                "type": "object",
                "properties": {
                    "event_id": {"type": "string", "description": "ID c·ªßa s·ª± ki·ªán c·∫ßn x√≥a (l·∫•y t·ª´ danh s√°ch s·ª± ki·ªán trong context)."}
                },
                "required": ["event_id"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "add_note",
            "description": "Th√™m m·ªôt ghi ch√∫ m·ªõi.",
            "parameters": {
                "type": "object",
                "properties": {
                    "title": {"type": "string", "description": "Ti√™u ƒë·ªÅ c·ªßa ghi ch√∫."},
                    "content": {"type": "string", "description": "N·ªôi dung chi ti·∫øt c·ªßa ghi ch√∫."},
                    "tags": {"type": "array", "items": {"type": "string"}, "description": "Danh s√°ch c√°c th·∫ª (tags) li√™n quan ƒë·∫øn ghi ch√∫."}
                },
                "required": ["title", "content"]
            }
        }
    }
]


# ------- Load/Save Data & Verification (Moved Data Loading below function defs) --------
def load_data(file_path):
    if os.path.exists(file_path):
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                # Ensure data is dict (or list for chat history?) -> dict for most
                if not isinstance(data, dict):
                     # Special case for chat history which might be list per member?
                     # No, structure is { member_id: [ {chat_entry}, ... ] } which is dict
                     logger.warning(f"D·ªØ li·ªáu trong {file_path} kh√¥ng ph·∫£i t·ª´ ƒëi·ªÉn. Kh·ªüi t·∫°o l·∫°i.")
                     return {}
                return data
        except json.JSONDecodeError as e:
            logger.error(f"L·ªói JSON khi ƒë·ªçc {file_path}: {e}. Tr·∫£ v·ªÅ d·ªØ li·ªáu tr·ªëng.")
            return {}
        except Exception as e:
            logger.error(f"L·ªói kh√¥ng x√°c ƒë·ªãnh khi ƒë·ªçc {file_path}: {e}", exc_info=True)
            return {}
    return {}

def save_data(file_path, data):
    try:
        os.makedirs(os.path.dirname(file_path) or '.', exist_ok=True)
        # Create a temporary file and write, then rename to make it more atomic
        temp_file_path = file_path + ".tmp"
        with open(temp_file_path, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False) # Use indent=2 for smaller files
        # Replace the original file with the temporary file
        shutil.move(temp_file_path, file_path)
        # logger.info(f"ƒê√£ l∆∞u d·ªØ li·ªáu v√†o {file_path}") # Reduce log noise?
        return True
    except Exception as e:
        logger.error(f"L·ªói khi l∆∞u d·ªØ li·ªáu v√†o {file_path}: {e}", exc_info=True)
        # Clean up temp file if it exists
        if os.path.exists(temp_file_path):
             try:
                 os.remove(temp_file_path)
             except OSError as rm_err:
                  logger.error(f"Kh√¥ng th·ªÉ x√≥a file t·∫°m {temp_file_path}: {rm_err}")
        return False

def verify_data_structure():
    """Ki·ªÉm tra v√† ƒë·∫£m b·∫£o c·∫•u tr√∫c d·ªØ li·ªáu ban ƒë·∫ßu."""
    global family_data, events_data, notes_data, chat_history
    needs_save = False

    if not isinstance(family_data, dict):
        logger.warning("family_data kh√¥ng ph·∫£i t·ª´ ƒëi·ªÉn. Kh·ªüi t·∫°o l·∫°i.")
        family_data = {}
        needs_save = True
    # Check internal structure if needed (e.g., ensure each member is a dict)

    if not isinstance(events_data, dict):
        logger.warning("events_data kh√¥ng ph·∫£i t·ª´ ƒëi·ªÉn. Kh·ªüi t·∫°o l·∫°i.")
        events_data = {}
        needs_save = True
    # Ensure events have IDs and basic structure

    if not isinstance(notes_data, dict):
        logger.warning("notes_data kh√¥ng ph·∫£i t·ª´ ƒëi·ªÉn. Kh·ªüi t·∫°o l·∫°i.")
        notes_data = {}
        needs_save = True

    if not isinstance(chat_history, dict):
        logger.warning("chat_history kh√¥ng ph·∫£i t·ª´ ƒëi·ªÉn. Kh·ªüi t·∫°o l·∫°i.")
        chat_history = {}
        needs_save = True
    # Ensure chat history format {member_id: list}

    if needs_save:
        logger.info("L∆∞u l·∫°i c·∫•u tr√∫c d·ªØ li·ªáu m·∫∑c ƒë·ªãnh do ph√°t hi·ªán l·ªói.")
        save_data(FAMILY_DATA_FILE, family_data)
        save_data(EVENTS_DATA_FILE, events_data)
        save_data(NOTES_DATA_FILE, notes_data)
        save_data(CHAT_HISTORY_FILE, chat_history)

# --- Data Management Functions ---
# (add_family_member, update_preference, add_event, update_event, delete_event, add_note)
# Define these functions *before* mapping them in tool_functions

def add_family_member(details):
    """Th√™m th√†nh vi√™n m·ªõi."""
    global family_data
    try:
        member_id = str(uuid.uuid4()) # Use UUID
        # Basic validation
        if not details.get("name"):
             logger.error("Kh√¥ng th·ªÉ th√™m th√†nh vi√™n: thi·∫øu t√™n.")
             return False
        family_data[member_id] = {
            "id": member_id,
            "name": details.get("name"),
            "age": details.get("age", ""),
            "preferences": details.get("preferences", {}),
            "added_on": datetime.datetime.now().isoformat()
        }
        if save_data(FAMILY_DATA_FILE, family_data):
             logger.info(f"ƒê√£ th√™m th√†nh vi√™n ID {member_id}: {details.get('name')}")
             return True
        else:
             logger.error(f"L∆∞u th·∫•t b·∫°i sau khi th√™m th√†nh vi√™n {member_id} v√†o b·ªô nh·ªõ.")
             if member_id in family_data: del family_data[member_id] # Rollback memory add
             return False
    except Exception as e:
         logger.error(f"L·ªói khi th√™m th√†nh vi√™n: {e}", exc_info=True)
         return False

def update_preference(details):
    """C·∫≠p nh·∫≠t s·ªü th√≠ch."""
    global family_data
    try:
        member_id = str(details.get("member_id"))
        preference_key = details.get("preference_key")
        preference_value = details.get("preference_value")

        if not member_id or not preference_key or preference_value is None:
             logger.error(f"Thi·∫øu th√¥ng tin ƒë·ªÉ c·∫≠p nh·∫≠t s·ªü th√≠ch: {details}")
             return False

        if member_id in family_data:
            if "preferences" not in family_data[member_id] or not isinstance(family_data[member_id]["preferences"], dict):
                family_data[member_id]["preferences"] = {} # Initialize if missing/invalid

            original_value = family_data[member_id]["preferences"].get(preference_key)
            family_data[member_id]["preferences"][preference_key] = preference_value
            family_data[member_id]["last_updated"] = datetime.datetime.now().isoformat()

            if save_data(FAMILY_DATA_FILE, family_data):
                logger.info(f"ƒê√£ c·∫≠p nh·∫≠t s·ªü th√≠ch '{preference_key}' cho th√†nh vi√™n {member_id}")
                return True
            else:
                 logger.error(f"L∆∞u th·∫•t b·∫°i sau khi c·∫≠p nh·∫≠t s·ªü th√≠ch cho {member_id}.")
                 # Rollback memory change
                 if original_value is not None:
                      family_data[member_id]["preferences"][preference_key] = original_value
                 else: # Value didn't exist before
                      if preference_key in family_data[member_id]["preferences"]:
                           del family_data[member_id]["preferences"][preference_key]
                 return False
        else:
            logger.warning(f"Kh√¥ng t√¨m th·∫•y th√†nh vi√™n ID={member_id} ƒë·ªÉ c·∫≠p nh·∫≠t s·ªü th√≠ch.")
            return False
    except Exception as e:
         logger.error(f"L·ªói khi c·∫≠p nh·∫≠t s·ªü th√≠ch: {e}", exc_info=True)
         return False


def add_event(details):
    """Th√™m m·ªôt s·ª± ki·ªán m·ªõi. Expects 'date' to be calculated YYYY-MM-DD."""
    global events_data
    try:
        event_id = str(uuid.uuid4()) # Use UUID for more robust IDs
        if not details.get('title') or 'date' not in details or details.get('date') is None:
            logger.error(f"Thi·∫øu title ho·∫∑c date ƒë√£ t√≠nh to√°n khi th√™m s·ª± ki·ªán: {details}")
            return False

        events_data[event_id] = {
            "id": event_id,
            "title": details.get("title"),
            "date": details.get("date"), # Use the calculated date
            "time": details.get("time", "19:00"),
            "description": details.get("description", ""),
            "participants": details.get("participants", []),
            "repeat_type": details.get("repeat_type", "ONCE"),
            "created_by": details.get("created_by"),
            "created_on": datetime.datetime.now().isoformat()
        }
        if save_data(EVENTS_DATA_FILE, events_data):
             logger.info(f"ƒê√£ th√™m s·ª± ki·ªán ID {event_id}: {details.get('title')}")
             return True
        else:
             logger.error(f"L∆∞u s·ª± ki·ªán ID {event_id} th·∫•t b·∫°i.")
             if event_id in events_data: del events_data[event_id] # Rollback memory add
             return False
    except Exception as e:
        logger.error(f"L·ªói nghi√™m tr·ªçng khi th√™m s·ª± ki·ªán: {e}", exc_info=True)
        return False

def update_event(details):
    """C·∫≠p nh·∫≠t s·ª± ki·ªán. Expects 'date' to be calculated YYYY-MM-DD if provided."""
    global events_data
    event_id_str = str(details.get("id"))
    original_event_copy = None

    try:
        if not event_id_str or event_id_str not in events_data:
            logger.warning(f"Kh√¥ng t√¨m th·∫•y s·ª± ki·ªán ID={event_id_str} ƒë·ªÉ c·∫≠p nh·∫≠t.")
            return False

        original_event_copy = events_data.get(event_id_str, {}).copy()
        if not original_event_copy:
             logger.error(f"Kh√¥ng th·ªÉ t·∫°o b·∫£n sao cho event ID {event_id_str}.")
             return False

        updated = False
        event_to_update = events_data[event_id_str]

        for key, value in details.items():
            if key == "id": continue
            current_value = event_to_update.get(key)
            if value != current_value:
                if key == 'date' and not value: # Don't update date to empty if calculation failed
                    logger.warning(f"B·ªè qua c·∫≠p nh·∫≠t date th√†nh gi√° tr·ªã r·ªóng cho event ID {event_id_str}")
                    continue
                event_to_update[key] = value
                updated = True
                logger.debug(f"Event {event_id_str}: Updated field '{key}'")

        if updated:
            event_to_update["last_updated"] = datetime.datetime.now().isoformat()
            logger.info(f"Attempting to save updated event ID={event_id_str}")
            if save_data(EVENTS_DATA_FILE, events_data):
                logger.info(f"ƒê√£ c·∫≠p nh·∫≠t v√† l∆∞u th√†nh c√¥ng s·ª± ki·ªán ID={event_id_str}")
                return True
            else:
                 logger.error(f"L∆∞u c·∫≠p nh·∫≠t s·ª± ki·ªán ID {event_id_str} th·∫•t b·∫°i.")
                 if event_id_str in events_data and original_event_copy:
                      events_data[event_id_str] = original_event_copy
                      logger.info(f"ƒê√£ rollback thay ƒë·ªïi trong b·ªô nh·ªõ cho event ID {event_id_str} do l∆∞u th·∫•t b·∫°i.")
                 return False
        else:
             logger.info(f"Kh√¥ng c√≥ thay ƒë·ªïi n√†o ƒë∆∞·ª£c √°p d·ª•ng cho s·ª± ki·ªán ID={event_id_str}")
             return True # No changes is success

    except Exception as e:
        logger.error(f"L·ªói nghi√™m tr·ªçng khi c·∫≠p nh·∫≠t s·ª± ki·ªán ID {details.get('id')}: {e}", exc_info=True)
        if event_id_str and event_id_str in events_data and original_event_copy:
             events_data[event_id_str] = original_event_copy
             logger.info(f"ƒê√£ rollback thay ƒë·ªïi trong b·ªô nh·ªõ cho event ID {event_id_str} do l·ªói x·ª≠ l√Ω.")
        return False

def delete_event(details):
    """X√≥a s·ª± ki·ªán d·ª±a tr√™n ID trong details dict."""
    global events_data
    event_id_to_delete = str(details.get("event_id")) # Get ID from args
    if not event_id_to_delete:
         logger.error("Thi·∫øu event_id ƒë·ªÉ x√≥a s·ª± ki·ªán.")
         return False
    try:
        if event_id_to_delete in events_data:
            deleted_event_copy = events_data.pop(event_id_to_delete) # Remove and get copy
            if save_data(EVENTS_DATA_FILE, events_data):
                 logger.info(f"ƒê√£ x√≥a s·ª± ki·ªán ID {event_id_to_delete}")
                 return True
            else:
                 logger.error(f"L∆∞u sau khi x√≥a s·ª± ki·ªán ID {event_id_to_delete} th·∫•t b·∫°i.")
                 # Rollback memory delete
                 events_data[event_id_to_delete] = deleted_event_copy
                 logger.info(f"ƒê√£ rollback x√≥a trong b·ªô nh·ªõ cho event ID {event_id_to_delete}.")
                 return False
        else:
            logger.warning(f"Kh√¥ng t√¨m th·∫•y s·ª± ki·ªán ID {event_id_to_delete} ƒë·ªÉ x√≥a.")
            return False # Indicate not found implicitly
    except Exception as e:
         logger.error(f"L·ªói khi x√≥a s·ª± ki·ªán ID {event_id_to_delete}: {e}", exc_info=True)
         return False


def add_note(details):
    """Th√™m ghi ch√∫ m·ªõi."""
    global notes_data
    try:
        note_id = str(uuid.uuid4())
        if not details.get("title") or not details.get("content"):
             logger.error(f"Thi·∫øu title ho·∫∑c content khi th√™m note: {details}")
             return False

        notes_data[note_id] = {
            "id": note_id,
            "title": details.get("title"),
            "content": details.get("content"),
            "tags": details.get("tags", []),
            "created_by": details.get("created_by"),
            "created_on": datetime.datetime.now().isoformat()
        }
        if save_data(NOTES_DATA_FILE, notes_data):
            logger.info(f"ƒê√£ th√™m ghi ch√∫ ID {note_id}: {details.get('title')}")
            return True
        else:
             logger.error(f"L∆∞u th·∫•t b·∫°i sau khi th√™m note {note_id} v√†o b·ªô nh·ªõ.")
             if note_id in notes_data: del notes_data[note_id] # Rollback memory add
             return False
    except Exception as e:
         logger.error(f"L·ªói khi th√™m note: {e}", exc_info=True)
         return False

# Map tool names to actual Python functions
tool_functions = {
    "add_family_member": add_family_member,
    "update_preference": update_preference,
    "add_event": add_event,
    "update_event": update_event,
    "delete_event": delete_event,
    "add_note": add_note,
}

# Load initial data after functions are defined
family_data = load_data(FAMILY_DATA_FILE)
events_data = load_data(EVENTS_DATA_FILE)
notes_data = load_data(NOTES_DATA_FILE)
chat_history = load_data(CHAT_HISTORY_FILE)
verify_data_structure() # Verify after loading

# Initialize Session Manager (Should be done after data loading if it depends on it)
session_manager = SessionManager(SESSIONS_DATA_FILE)

# Initialize Weather Service & Advisor
weather_service = WeatherService(openweather_api_key=os.getenv("OPENWEATHER_API_KEY"))
weather_advisor = WeatherAdvisor()

# ------- Date/Time/Cron Helper Functions --------
VIETNAMESE_WEEKDAY_MAP = weather_service.VIETNAMESE_WEEKDAY_MAP # Use the one from WeatherService
NEXT_WEEK_KEYWORDS = ["tu·∫ßn sau", "tu·∫ßn t·ªõi", "next week"]
RECURRING_KEYWORDS = [
    "h√†ng ng√†y", "m·ªói ng√†y", "h√†ng tu·∫ßn", "m·ªói tu·∫ßn", "h√†ng th√°ng", "m·ªói th√°ng",
    "h√†ng nƒÉm", "m·ªói nƒÉm", "ƒë·ªãnh k·ª≥", "l·∫∑p l·∫°i",
    "m·ªói s√°ng th·ª©", "m·ªói tr∆∞a th·ª©", "m·ªói chi·ªÅu th·ª©", "m·ªói t·ªëi th·ª©",
    "th·ª© 2 h√†ng tu·∫ßn", "m·ªói th·ª© 2", "m·ªói t2", "th·ª© 3 h√†ng tu·∫ßn", "m·ªói th·ª© 3", "m·ªói t3",
    "th·ª© 4 h√†ng tu·∫ßn", "m·ªói th·ª© 4", "m·ªói t4", "th·ª© 5 h√†ng tu·∫ßn", "m·ªói th·ª© 5", "m·ªói t5",
    "th·ª© 6 h√†ng tu·∫ßn", "m·ªói th·ª© 6", "m·ªói t6", "th·ª© 7 h√†ng tu·∫ßn", "m·ªói th·ª© 7", "m·ªói t7",
    "ch·ªß nh·∫≠t h√†ng tu·∫ßn", "m·ªói ch·ªß nh·∫≠t", "m·ªói cn",
    "daily", "every day", "weekly", "every week", "monthly", "every month",
    "yearly", "annually", "every year", "recurring", "repeating",
    "every monday", "every tuesday", "every wednesday", "every thursday",
    "every friday", "every saturday", "every sunday",
]

def get_date_from_relative_term(term):
    """Wrapper to use the method from WeatherService instance."""
    return weather_service.get_date_from_relative_term(term)


def date_time_to_cron(date_str, time_str="19:00"):
    """Chuy·ªÉn ng√†y gi·ªù c·ª• th·ªÉ sang Quartz cron expression."""
    try:
        # Ensure time_str is valid HH:MM
        if not time_str or ':' not in time_str: time_str = "19:00"
        hour, minute = map(int, time_str.split(":"))
        date_obj = datetime.datetime.strptime(date_str, "%Y-%m-%d")
        # Quartz: Seconds Minute Hour DayOfMonth Month DayOfWeek Year
        quartz_cron = f"0 {minute} {hour} {date_obj.day} {date_obj.month} ? {date_obj.year}"
        logger.info(f"Generated Quartz cron ONCE for {date_str} {time_str}: {quartz_cron}")
        return quartz_cron
    except (ValueError, TypeError) as e:
        logger.error(f"L·ªói t·∫°o cron expression ONCE cho date='{date_str}', time='{time_str}': {e}")
        return "" # Return empty string on error

def determine_repeat_type(description, title):
    """X√°c ƒë·ªãnh ki·ªÉu l·∫∑p l·∫°i d·ª±a tr√™n m√¥ t·∫£ v√† ti√™u ƒë·ªÅ."""
    combined_text = (str(description) + " " + str(title)).lower()
    for keyword in RECURRING_KEYWORDS:
        # Use word boundaries for keywords like 'daily' to avoid matching 'conditionally'
        if re.search(r'\b' + re.escape(keyword) + r'\b', combined_text):
            logger.info(f"Ph√°t hi·ªán t·ª´ kh√≥a l·∫∑p l·∫°i '{keyword}' -> RECURRING")
            return "RECURRING"
    logger.info(f"Kh√¥ng t√¨m th·∫•y t·ª´ kh√≥a l·∫∑p l·∫°i trong '{combined_text[:100]}...' -> ONCE")
    return "ONCE"

def generate_recurring_cron(description, title, time_str="19:00"):
    """T·∫°o Quartz cron expression cho s·ª± ki·ªán l·∫∑p l·∫°i."""
    try:
        if not time_str or ':' not in time_str: time_str = "19:00"
        hour, minute = map(int, time_str.split(":"))

        combined_text = (str(description) + " " + str(title)).lower()

        # 1. Daily
        if "h√†ng ng√†y" in combined_text or "m·ªói ng√†y" in combined_text or "daily" in combined_text:
            quartz_cron = f"0 {minute} {hour} ? * * *"
            logger.info(f"T·∫°o cron Quartz H√ÄNG NG√ÄY l√∫c {time_str}: {quartz_cron}")
            return quartz_cron

        # 2. Weekly on specific day
        # Quartz: 1=SUN, 2=MON, ..., 7=SAT
        quartz_day_map = {
            "ch·ªß nh·∫≠t": 1, "cn": 1, "sunday": 1, "th·ª© 2": 2, "t2": 2, "monday": 2,
            "th·ª© 3": 3, "t3": 3, "tuesday": 3, "th·ª© 4": 4, "t4": 4, "wednesday": 4,
            "th·ª© 5": 5, "t5": 5, "thursday": 5, "th·ª© 6": 6, "t6": 6, "friday": 6,
            "th·ª© 7": 7, "t7": 7, "saturday": 7
        }
        found_day_num = None
        found_day_text = ""
        for day_text, day_num in quartz_day_map.items():
            if re.search(r'\b' + re.escape(day_text) + r'\b', combined_text):
                found_day_num = day_num
                found_day_text = day_text
                break
        if found_day_num is not None:
            # Assume weekly if a specific day is mentioned with recurring keywords
             is_weekly = any(kw in combined_text for kw in ["h√†ng tu·∫ßn", "m·ªói tu·∫ßn", "weekly", "every"])
             if is_weekly or any(kw in combined_text for kw in RECURRING_KEYWORDS): # Check general recurring keywords too
                 quartz_cron = f"0 {minute} {hour} ? * {found_day_num} *"
                 logger.info(f"T·∫°o cron Quartz H√ÄNG TU·∫¶N v√†o Th·ª© {found_day_text} ({found_day_num}) l√∫c {time_str}: {quartz_cron}")
                 return quartz_cron
             else:
                  logger.warning(f"T√¨m th·∫•y '{found_day_text}' nh∆∞ng kh√¥ng r√µ l√† h√†ng tu·∫ßn. Kh√¥ng t·∫°o cron l·∫∑p l·∫°i.")


        # 3. Monthly (Example: day 15 or last day)
        monthly_match = re.search(r"(ng√†y\s+(\d{1,2})|ng√†y\s+cu·ªëi\s+c√πng)\s+(h√†ng\s+th√°ng|m·ªói\s+th√°ng)", combined_text)
        if monthly_match:
            day_specifier = monthly_match.group(1)
            day_of_month = "L" if "cu·ªëi c√πng" in day_specifier else ""
            if not day_of_month:
                day_num_match = re.search(r'\d{1,2}', day_specifier)
                if day_num_match: day_of_month = day_num_match.group(0)

            if day_of_month:
                quartz_cron = f"0 {minute} {hour} {day_of_month} * ? *"
                logger.info(f"T·∫°o cron Quartz H√ÄNG TH√ÅNG v√†o ng√†y {day_of_month} l√∫c {time_str}: {quartz_cron}")
                return quartz_cron

        # 4. Fallback for recurring (e.g., "h√†ng tu·∫ßn" without specific day) -> default daily
        logger.warning(f"Kh√¥ng th·ªÉ x√°c ƒë·ªãnh l·ªãch l·∫∑p l·∫°i c·ª• th·ªÉ t·ª´ '{combined_text[:100]}...'. Cron s·∫Ω r·ªóng.")
        return "" # Return empty if cannot determine specific recurring schedule

    except Exception as e:
        logger.error(f"L·ªói khi t·∫°o cron Quartz l·∫∑p l·∫°i: {e}", exc_info=True)
        return "" # Return empty string on error


# ------- Request & Response Models --------
class MessageContent(BaseModel):
    type: str
    text: Optional[str] = None
    html: Optional[str] = None
    image_url: Optional[Dict[str, str]] = None # Expects {"url": "data:..."}
    audio_data: Optional[str] = None # Base64 encoded

class Message(BaseModel):
    role: str
    content: Union[str, List[MessageContent], None] = None # Allow str for simple text, list for multimodal
    tool_calls: Optional[List[Dict[str, Any]]] = None # For assistant messages with tool calls
    tool_call_id: Optional[str] = None # For tool role messages

class ChatRequest(BaseModel):
    session_id: str
    member_id: Optional[str] = None
    message: MessageContent # The new message from user
    content_type: str = "text" # "text", "image", "audio"
    openai_api_key: Optional[str] = None
    tavily_api_key: Optional[str] = None
    messages: Optional[List[Message]] = None # Optional full history from client


class ChatResponse(BaseModel):
    session_id: str
    messages: List[Message] # Return only the *last* assistant message(s)
    audio_response: Optional[str] = None
    response_format: Optional[str] = "html"
    content_type: Optional[str] = "text" # Reflect back the input type
    event_data: Optional[Dict[str, Any]] = None # Include event data if generated

class MemberModel(BaseModel):
    name: str
    age: Optional[str] = None
    preferences: Optional[Dict[str, str]] = None

class EventModel(BaseModel):
    title: str
    date: str # Expects YYYY-MM-DD
    time: Optional[str] = "19:00"
    description: Optional[str] = None
    participants: Optional[List[str]] = None

class NoteModel(BaseModel):
    title: str
    content: str
    tags: Optional[List[str]] = None

class SearchRequest(BaseModel):
    query: str
    tavily_api_key: str
    openai_api_key: str
    is_news_query: Optional[bool] = None

class SuggestedQuestionsResponse(BaseModel):
    session_id: str
    member_id: Optional[str] = None
    suggested_questions: List[str]
    timestamp: str


# ------- API Endpoints -------------

# Helper function to execute a tool call
def execute_tool_call(tool_call: ChatCompletionMessageToolCall, current_member_id: Optional[str]) -> Tuple[Optional[Dict[str, Any]], str]:
    """
    Executes the appropriate Python function based on the tool call.
    Handles date calculation for event tools.
    Returns a tuple: (event_data_for_frontend, tool_result_content_for_llm)
    """
    function_name = tool_call.function.name
    try:
        # Handle potential empty arguments string
        arguments_str = tool_call.function.arguments
        if not arguments_str:
             arguments = {}
             logger.warning(f"Tool call {function_name} received empty arguments string.")
        else:
             arguments = json.loads(arguments_str)

        logger.info(f"Executing tool: {function_name} with args: {arguments}")

        # --- Special Handling for Event Dates ---
        final_date_str = None
        repeat_type = "ONCE"
        cron_expression = ""
        event_action_data = None

        if function_name in ["add_event", "update_event"]:
            date_description = arguments.get("date_description")
            time_str = arguments.get("time", "19:00")
            description = arguments.get("description", "")
            title = arguments.get("title", "")

            # For update, get potentially existing title/description if not provided in args
            if function_name == "update_event":
                event_id = arguments.get("event_id")
                if event_id and str(event_id) in events_data: # Ensure ID is string for lookup
                     event_id_str = str(event_id)
                     if not title: title = events_data[event_id_str].get("title", "")
                     if not description: description = events_data[event_id_str].get("description", "")
                     # Store string ID back into args for the function call
                     arguments["id"] = event_id_str
                else:
                    return None, f"L·ªói: Kh√¥ng t√¨m th·∫•y s·ª± ki·ªán ID '{event_id}' ƒë·ªÉ c·∫≠p nh·∫≠t."

            # Calculate the actual date using Python
            if date_description:
                final_date_str = get_date_from_relative_term(date_description)
                if final_date_str:
                    logger.info(f"Calculated date '{final_date_str}' from description '{date_description}'")
                    arguments["date"] = final_date_str # Use calculated date for the function call
                else:
                    logger.warning(f"Could not calculate date from '{date_description}'. Event date will be empty or unchanged.")
                    # Remove date field if calculation failed, otherwise it might try to save None/empty
                    if "date" in arguments: del arguments["date"]

            # Remove date_description as it's processed
            if "date_description" in arguments:
                del arguments["date_description"]

            # Determine recurrence and generate cron AFTER calculating the final date
            repeat_type = determine_repeat_type(description, title)
            arguments['repeat_type'] = repeat_type # Store for potential use in add/update logic
            is_recurring_event = (repeat_type == "RECURRING")

            # Use final_date_str (if calculated) or existing date (for update) for cron generation
            date_for_cron = final_date_str
            if function_name == "update_event" and not date_for_cron:
                event_id = arguments.get("id") # Use the potentially updated string ID
                if event_id and event_id in events_data:
                    date_for_cron = events_data[event_id].get('date')

            if is_recurring_event:
                cron_expression = generate_recurring_cron(description, title, time_str)
            elif date_for_cron: # ONCE with a valid date
                cron_expression = date_time_to_cron(date_for_cron, time_str)

            logger.info(f"Event type: {repeat_type}, Cron generated: '{cron_expression}'")

            # Prepare optional data for frontend if needed (e.g., for calendar updates)
            event_action_data = {
                "action": "add" if function_name == "add_event" else "update",
                "id": arguments.get("id") if function_name == "update_event" else None, # Use the string ID
                "title": title,
                "description": description,
                "cron_expression": cron_expression,
                "repeat_type": repeat_type,
                "original_date": final_date_str, # The specific date instance calculated/used
                "original_time": time_str,
                "participants": arguments.get("participants", [])
            }

        # --- Assign creator/updater ID ---
        if current_member_id:
            if function_name == "add_event" or function_name == "add_note":
                arguments["created_by"] = current_member_id
            elif function_name == "add_family_member":
                 pass # ID assigned by function
            elif function_name == "update_event":
                 arguments["updated_by"] = current_member_id # Add if needed

        # --- Execute the function ---
        if function_name in tool_functions:
            func_to_call = tool_functions[function_name]
            try:
                result = func_to_call(arguments) # Pass the modified arguments
                if result is False:
                     tool_result_content = f"Th·∫•t b·∫°i khi th·ª±c thi {function_name}. Chi ti·∫øt l·ªói ƒë√£ ƒë∆∞·ª£c ghi l·∫°i."
                     logger.error(f"Execution failed for tool {function_name} with args {arguments}")
                     event_action_data = None # Clear frontend data on failure
                else:
                     tool_result_content = f"ƒê√£ th·ª±c thi th√†nh c√¥ng {function_name}."
                     logger.info(f"Successfully executed tool {function_name}")
                     # Specific message/data for delete
                     if function_name == "delete_event":
                         deleted_event_id = arguments.get("event_id") # ID should be in args now
                         event_action_data = {"action": "delete", "id": deleted_event_id}
                         tool_result_content = f"ƒê√£ x√≥a th√†nh c√¥ng s·ª± ki·ªán ID {deleted_event_id}."

                return event_action_data, tool_result_content
            except Exception as func_exc:
                 logger.error(f"Error executing tool function {function_name}: {func_exc}", exc_info=True)
                 return None, f"L·ªói trong qu√° tr√¨nh th·ª±c thi {function_name}: {str(func_exc)}"
        else:
            logger.error(f"Unknown tool function: {function_name}")
            return None, f"L·ªói: Kh√¥ng t√¨m th·∫•y h√†m cho c√¥ng c·ª• {function_name}."

    except json.JSONDecodeError as json_err:
        logger.error(f"Error decoding arguments for {function_name}: {json_err}")
        logger.error(f"Invalid JSON string: {tool_call.function.arguments}")
        return None, f"L·ªói: D·ªØ li·ªáu cho c√¥ng c·ª• {function_name} kh√¥ng h·ª£p l·ªá (JSON sai ƒë·ªãnh d·∫°ng)."
    except Exception as e:
        logger.error(f"Unexpected error in execute_tool_call for {function_name}: {e}", exc_info=True)
        return None, f"L·ªói kh√¥ng x√°c ƒë·ªãnh khi chu·∫©n b·ªã th·ª±c thi {function_name}."

# --- Endpoint /chat ---
@app.post("/chat")
async def chat_endpoint(chat_request: ChatRequest):
    """
    Endpoint ch√≠nh cho tr√≤ chuy·ªán (s·ª≠ d·ª•ng Tool Calling).
    Includes event_data in the response.
    """
    openai_api_key = chat_request.openai_api_key or os.getenv("OPENAI_API_KEY", "")
    tavily_api_key = chat_request.tavily_api_key or os.getenv("TAVILY_API_KEY", "")
    if not openai_api_key or "sk-" not in openai_api_key:
        raise HTTPException(status_code=400, detail="OpenAI API key kh√¥ng h·ª£p l·ªá")

    session = session_manager.get_session(chat_request.session_id)
    current_member_id = chat_request.member_id or session.get("current_member")
    session["current_member"] = current_member_id

    # --- Message Handling ---
    # Load history from client if provided and session is empty
    if chat_request.messages is not None and not session.get("messages"):
         logger.info(f"Loading message history from client for session {chat_request.session_id}")
         # Validate client messages before accepting?
         session["messages"] = [msg.dict(exclude_none=True) for msg in chat_request.messages]

    # Process the new incoming message
    message_content_model = chat_request.message
    message_dict = message_content_model.dict(exclude_none=True)

    logger.info(f"Nh·∫≠n request v·ªõi content_type: {chat_request.content_type}")
    processed_content_list = [] # Store content in the format OpenAI expects (list of dicts)

    if chat_request.content_type == "audio" and message_dict.get("type") == "audio" and message_dict.get("audio_data"):
        processed_audio = process_audio(message_dict, openai_api_key)
        if processed_audio and processed_audio.get("text"):
             processed_content_list.append({"type": "text", "text": processed_audio["text"]})
             logger.info(f"ƒê√£ x·ª≠ l√Ω audio th√†nh text: {processed_audio['text'][:50]}...")
        else:
             logger.error("X·ª≠ l√Ω audio th·∫•t b·∫°i ho·∫∑c kh√¥ng tr·∫£ v·ªÅ text.")
             processed_content_list.append({"type": "text", "text": "[L·ªói x·ª≠ l√Ω audio]"})

    elif chat_request.content_type == "image" and message_dict.get("type") == "image_url":
        logger.info(f"ƒê√£ nh·∫≠n h√¨nh ·∫£nh: {message_dict.get('image_url', {}).get('url', '')[:60]}...")
        processed_content_list.append({"type": "image_url", "image_url": message_dict["image_url"]})
        # Add accompanying text if provided in the *same* message content object
        if message_dict.get("text"):
             processed_content_list.append({"type": "text", "text": message_dict["text"]})

    elif message_dict.get("type") == "html":
         clean_text = re.sub(r'<[^>]*>', ' ', message_dict["html"])
         clean_text = unescape(clean_text) # Handle HTML entities like &amp;
         clean_text = re.sub(r'\s+', ' ', clean_text).strip()
         processed_content_list.append({"type": "text", "text": clean_text})
         logger.info(f"ƒê√£ x·ª≠ l√Ω HTML th√†nh text: {clean_text[:50]}...")

    elif message_dict.get("type") == "text": # Default text type
        processed_content_list.append({"type": "text", "text": message_dict["text"]})

    else: # Handle unknown types?
         logger.warning(f"Lo·∫°i n·ªôi dung kh√¥ng x√°c ƒë·ªãnh trong request: {message_dict.get('type')}")
         processed_content_list.append({"type": "text", "text": "[N·ªôi dung kh√¥ng h·ªó tr·ª£]"})


    # Add the processed user message to the session history
    if processed_content_list:
         session["messages"].append({
             "role": "user",
             "content": processed_content_list
         })
    else:
         logger.error("Kh√¥ng th·ªÉ x·ª≠ l√Ω n·ªôi dung tin nh·∫Øn ng∆∞·ªùi d√πng.")
         # Maybe raise error or return immediately?


    # --- Tool Calling Flow ---
    final_event_data_to_return: Optional[Dict[str, Any]] = None

    try:
        client = OpenAI(api_key=openai_api_key)
        system_prompt_content = build_system_prompt(current_member_id)

        # Prepare messages for OpenAI API from session history
        openai_messages = [{"role": "system", "content": system_prompt_content}]
        for msg in session["messages"]:
             # Ensure content format is compatible with OpenAI API
             message_for_api = {
                 "role": msg["role"],
                 # Add tool calls/id if they exist from previous assistant/tool turns
                 **({ "tool_calls": msg["tool_calls"] } if msg.get("tool_calls") else {}),
                 **({ "tool_call_id": msg.get("tool_call_id") } if msg.get("tool_call_id") else {}),
             }
             # Handle content: should be string for text-only, list for multimodal/tool results
             msg_content = msg.get("content")
             if isinstance(msg_content, list):
                  # Assume list content is correctly formatted [{type:..., ...}]
                  message_for_api["content"] = msg_content
             elif isinstance(msg_content, str):
                  # Simple text content
                  message_for_api["content"] = msg_content
             elif msg.get("role") == "tool":
                  # Tool results have content as string
                  message_for_api["content"] = str(msg_content) if msg_content is not None else ""
             else:
                  # Handle unexpected content format
                  logger.warning(f"ƒê·ªãnh d·∫°ng content kh√¥ng mong ƒë·ª£i cho role {msg['role']}: {type(msg_content)}. S·ª≠ d·ª•ng chu·ªói r·ªóng.")
                  message_for_api["content"] = ""


             openai_messages.append(message_for_api)


        # --- Check Search Need ---
        search_result_for_prompt = await check_search_need(openai_messages, openai_api_key, tavily_api_key)
        if search_result_for_prompt:
             openai_messages[0] = {"role": "system", "content": system_prompt_content + search_result_for_prompt}


        logger.info("--- Calling OpenAI API (Potential First Pass) ---")
        logger.debug(f"Messages sent (last 3): {json.dumps(openai_messages[-3:], indent=2, ensure_ascii=False)}")

        first_response = client.chat.completions.create(
            model=openai_model,
            messages=openai_messages,
            tools=available_tools,
            tool_choice="auto",
            temperature=0.7,
            max_tokens=2048
        )

        response_message: ChatCompletionMessage = first_response.choices[0].message
        # Add assistant's response (potentially with tool calls) to history
        session["messages"].append(response_message.dict(exclude_none=True))


        # --- Handle Tool Calls ---
        tool_calls = response_message.tool_calls
        if tool_calls:
            logger.info(f"--- Tool Calls Detected: {len(tool_calls)} ---")
            messages_for_second_call = openai_messages + [response_message.dict(exclude_none=True)] # History for next call

            for tool_call in tool_calls:
                event_data_from_tool, tool_result_content = execute_tool_call(tool_call, current_member_id)

                if event_data_from_tool and final_event_data_to_return is None:
                    if event_data_from_tool.get("action") in ["add", "update", "delete"]:
                        final_event_data_to_return = event_data_from_tool
                        logger.info(f"Captured event_data for response: {final_event_data_to_return}")

                tool_result_message = {
                    "tool_call_id": tool_call.id,
                    "role": "tool",
                    "name": tool_call.function.name,
                    "content": tool_result_content,
                }
                messages_for_second_call.append(tool_result_message)
                session["messages"].append(tool_result_message) # Add tool result to session log

            logger.info("--- Calling OpenAI API (Second Pass - Summarizing Tool Results) ---")
            logger.debug(f"Messages for second call (last 4): {json.dumps(messages_for_second_call[-4:], indent=2, ensure_ascii=False)}")

            second_response = client.chat.completions.create(
                model=openai_model,
                messages=messages_for_second_call,
                temperature=0.7,
                max_tokens=1024
            )
            final_assistant_message = second_response.choices[0].message
            final_response_content = final_assistant_message.content

            session["messages"].append(final_assistant_message.dict(exclude_none=True))
            logger.info("Tool execution and summary completed.")

        else:
            logger.info("--- No Tool Calls Detected ---")
            final_response_content = response_message.content
            # Assistant message already added above

        # --- Final Processing & Response ---
        final_html_content = final_response_content if final_response_content else "T√¥i ƒë√£ th·ª±c hi·ªán xong y√™u c·∫ßu c·ªßa b·∫°n." # Fallback message

        audio_response_b64 = text_to_speech_google(final_html_content)

        # Save history & Update session AFTER the full interaction
        if current_member_id:
             summary = generate_chat_summary(session["messages"], openai_api_key)
             save_chat_history(current_member_id, session["messages"], summary, chat_request.session_id)

        session_manager.update_session(chat_request.session_id, {"messages": session["messages"]})

        # Get the *very last* message (should be assistant's final summary or direct response)
        last_message_dict = session["messages"][-1] if session["messages"] else {}
        last_assistant_msg_obj = None
        if last_message_dict.get("role") == "assistant":
             # Convert dict to Pydantic Message model for response structure
             try:
                  # Ensure content is in the right format (list for multimodal/text, str is okay too)
                  content = last_message_dict.get("content")
                  content_for_model = []
                  if isinstance(content, str):
                       content_for_model.append(MessageContent(type="html", html=content))
                  elif isinstance(content, list):
                       # Basic check if items look like MessageContent structure
                       if all(isinstance(item, dict) and 'type' in item for item in content):
                           content_for_model = [MessageContent(**item) for item in content]
                       else: # Attempt to force into text/html if structure is wrong
                            logger.warning("Assistant message content list has unexpected structure. Converting to text.")
                            content_text = " ".join(map(str, content))
                            content_for_model.append(MessageContent(type="html", html=content_text))

                  else: # Handle None or other types
                       content_for_model.append(MessageContent(type="html", html=""))


                  last_assistant_msg_obj = Message(
                      role="assistant",
                      content=content_for_model,
                      tool_calls=last_message_dict.get("tool_calls") # Include tool_calls if they were part of this message
                  )
             except Exception as model_err:
                  logger.error(f"Error creating response Message object: {model_err}", exc_info=True)


        if not last_assistant_msg_obj:
             fallback_content = MessageContent(type="html", html="ƒê√£ c√≥ l·ªói x·∫£y ra ho·∫∑c kh√¥ng c√≥ ph·∫£n h·ªìi.")
             last_assistant_msg_obj = Message(role="assistant", content=[fallback_content])

        return ChatResponse(
            session_id=chat_request.session_id,
            messages=[last_assistant_msg_obj], # Return only the last assistant message
            audio_response=audio_response_b64,
            response_format="html",
            content_type=chat_request.content_type,
            event_data=final_event_data_to_return
        )

    except Exception as e:
        logger.error(f"L·ªói nghi√™m tr·ªçng trong /chat endpoint: {str(e)}", exc_info=True)
        # Try saving session even on error
        session_manager.update_session(chat_request.session_id, {"messages": session.get("messages", [])})
        raise HTTPException(status_code=500, detail=f"L·ªói x·ª≠ l√Ω chat: {str(e)}")


# --- Endpoint /chat/stream ---
@app.post("/chat/stream")
async def chat_stream_endpoint(chat_request: ChatRequest):
    """
    Endpoint streaming cho tr√≤ chuy·ªán (s·ª≠ d·ª•ng Tool Calling).
    Includes event_data in the final completion message.
    """
    openai_api_key = chat_request.openai_api_key or os.getenv("OPENAI_API_KEY", "")
    tavily_api_key = chat_request.tavily_api_key or os.getenv("TAVILY_API_KEY", "")
    if not openai_api_key or "sk-" not in openai_api_key:
        raise HTTPException(status_code=400, detail="OpenAI API key kh√¥ng h·ª£p l·ªá")

    session = session_manager.get_session(chat_request.session_id)
    current_member_id = chat_request.member_id or session.get("current_member")
    session["current_member"] = current_member_id

    # --- Message Handling ---
    # Load history from client if provided and session is empty
    if chat_request.messages is not None and not session.get("messages"):
         logger.info(f"Stream: Loading message history from client for session {chat_request.session_id}")
         session["messages"] = [msg.dict(exclude_none=True) for msg in chat_request.messages]

    # Process the new incoming message
    message_content_model = chat_request.message
    message_dict = message_content_model.dict(exclude_none=True)
    logger.info(f"Stream: Nh·∫≠n request v·ªõi content_type: {chat_request.content_type}")
    processed_content_list = [] # Store content in the format OpenAI expects

    # --- Process incoming message based on content_type ---
    if chat_request.content_type == "audio" and message_dict.get("type") == "audio" and message_dict.get("audio_data"): # <<< S·ª¨A ƒê√ÇY
        processed_audio = process_audio(message_dict, openai_api_key)
        if processed_audio and processed_audio.get("text"):
             processed_content_list.append({"type": "text", "text": processed_audio["text"]})
             logger.info(f"Stream: ƒê√£ x·ª≠ l√Ω audio th√†nh text: {processed_audio['text'][:50]}...")
        else:
             logger.error("Stream: X·ª≠ l√Ω audio th·∫•t b·∫°i.")
             processed_content_list.append({"type": "text", "text": "[L·ªói x·ª≠ l√Ω audio]"})

    elif chat_request.content_type == "image" and message_dict.get("type") == "image_url": # <<< S·ª¨A ƒê√ÇY
        logger.info(f"Stream: ƒê√£ nh·∫≠n h√¨nh ·∫£nh: {message_dict.get('image_url', {}).get('url', '')[:60]}...")
        # Ensure image_url exists before appending
        if message_dict.get("image_url"):
            processed_content_list.append({"type": "image_url", "image_url": message_dict["image_url"]})
        else:
            logger.error("Stream: Content type l√† image nh∆∞ng thi·∫øu image_url.")
            processed_content_list.append({"type": "text", "text": "[L·ªói x·ª≠ l√Ω ·∫£nh: thi·∫øu URL]"})

        # Add accompanying text if provided
        if message_dict.get("text"):
             processed_content_list.append({"type": "text", "text": message_dict["text"]})

    elif message_dict.get("type") == "html": # <<< S·ª¨A ƒê√ÇY
         if message_dict.get("html"):
            clean_text = re.sub(r'<[^>]*>', ' ', message_dict["html"])
            clean_text = unescape(clean_text) # Handle HTML entities
            clean_text = re.sub(r'\s+', ' ', clean_text).strip()
            processed_content_list.append({"type": "text", "text": clean_text})
            logger.info(f"Stream: ƒê√£ x·ª≠ l√Ω HTML th√†nh text: {clean_text[:50]}...")
         else:
             logger.warning("Stream: Lo·∫°i n·ªôi dung l√† html nh∆∞ng thi·∫øu tr∆∞·ªùng 'html'.")
             processed_content_list.append({"type": "text", "text": "[L·ªói x·ª≠ l√Ω HTML: thi·∫øu n·ªôi dung]"})


    elif message_dict.get("type") == "text": # <<< S·ª¨A ƒê√ÇY (Th√™m ki·ªÉm tra text t·ªìn t·∫°i)
        text_content = message_dict.get("text")
        if text_content:
            processed_content_list.append({"type": "text", "text": text_content})
        else:
             logger.warning("Stream: Lo·∫°i n·ªôi dung l√† text nh∆∞ng thi·∫øu tr∆∞·ªùng 'text'.")
             # Decide if an empty text message is valid or an error
             # processed_content_list.append({"type": "text", "text": ""}) # Allow empty text?

    else: # <<< S·ª¨A ƒê√ÇY
         logger.warning(f"Stream: Lo·∫°i n·ªôi dung kh√¥ng x√°c ƒë·ªãnh ho·∫∑c thi·∫øu d·ªØ li·ªáu: {message_dict.get('type')}")
         processed_content_list.append({"type": "text", "text": "[N·ªôi dung kh√¥ng h·ªó tr·ª£ ho·∫∑c b·ªã l·ªói]"})
    # --- End of message processing ---

    # Add the processed user message to the session history only if content was processed
    if processed_content_list:
         session["messages"].append({
             "role": "user",
             "content": processed_content_list
         })
    else:
         # If message processing failed completely, maybe don't proceed?
         logger.error("Stream: Kh√¥ng th·ªÉ x·ª≠ l√Ω n·ªôi dung tin nh·∫Øn ƒë·∫øn. Kh√¥ng th√™m v√†o l·ªãch s·ª≠.")
         # Consider how to handle this - maybe return an error response immediately?
         # For now, we let the generator handle it, but it might start with bad history.

    # --- Streaming Generator (B·∫Øt ƒë·∫ßu t·ª´ ƒë√¢y) ---
    async def response_stream_generator():
        # ... (Ph·∫ßn c√≤n l·∫°i c·ªßa h√†m generator gi·ªØ nguy√™n nh∆∞ phi√™n b·∫£n tr∆∞·ªõc) ...
        # Initialize necessary variables within the generator's scope
        final_event_data_to_return: Optional[Dict[str, Any]] = None # <<< Initialize event_data
        client = OpenAI(api_key=openai_api_key)
        system_prompt_content = build_system_prompt(current_member_id) # Build fresh prompt content

        # --- Prepare initial messages list from session ---
        # (This assumes session["messages"] was correctly updated just before calling this generator)
        openai_messages = [{"role": "system", "content": system_prompt_content}]
        for msg in session["messages"]:
             # Adapt message format for API call (same logic as non-stream)
             message_for_api = {
                 "role": msg["role"],
                 **({ "tool_calls": msg["tool_calls"] } if msg.get("tool_calls") else {}),
                 **({ "tool_call_id": msg.get("tool_call_id") } if msg.get("tool_call_id") else {}),
             }
             msg_content = msg.get("content")
             if isinstance(msg_content, list): message_for_api["content"] = msg_content
             elif isinstance(msg_content, str): message_for_api["content"] = msg_content
             elif msg.get("role") == "tool": message_for_api["content"] = str(msg_content) if msg_content is not None else ""
             else: message_for_api["content"] = "" # Default empty content if format is wrong
             openai_messages.append(message_for_api)

        # --- Check Search Need ---
        try:
             search_result_for_prompt = await check_search_need(openai_messages, openai_api_key, tavily_api_key)
             if search_result_for_prompt:
                  # Update system prompt in the list
                  openai_messages[0] = {"role": "system", "content": system_prompt_content + search_result_for_prompt}
        except Exception as search_err:
             logger.error(f"Error during search need check: {search_err}", exc_info=True)
             # Proceed without search results, maybe log or yield an error?

        # Variables for streaming state management
        accumulated_tool_calls = []
        accumulated_assistant_content = ""
        assistant_message_dict_for_session = {"role": "assistant", "content": None, "tool_calls": None}
        tool_call_chunks = {} # {index: {"id": ..., "type": ..., "function": {"name":..., "arguments": ...}}}

        # --- Main Streaming Logic ---
        try:
            logger.info("--- Calling OpenAI API (Streaming - Potential First Pass) ---")
            stream = client.chat.completions.create(
                model=openai_model,
                messages=openai_messages,
                tools=available_tools,
                tool_choice="auto",
                temperature=0.7,
                max_tokens=2048,
                stream=True
            )

            async for chunk in stream:
                delta = chunk.choices[0].delta if chunk.choices else None # Handle empty choices list
                if not delta: continue # Skip empty deltas

                finish_reason = chunk.choices[0].finish_reason

                # Accumulate content and yield chunks
                if delta.content:
                    accumulated_assistant_content += delta.content
                    yield json.dumps({"chunk": delta.content, "type": "html", "content_type": chat_request.content_type}) + "\n"
                    await asyncio.sleep(0) # Yield control to event loop

                # Accumulate tool call parts
                if delta.tool_calls:
                    for tc_chunk in delta.tool_calls:
                        index = tc_chunk.index
                        if index not in tool_call_chunks:
                            tool_call_chunks[index] = {"function": {"arguments": ""}}
                        # Safely update fields
                        if tc_chunk.id: tool_call_chunks[index]["id"] = tc_chunk.id
                        if tc_chunk.type: tool_call_chunks[index]["type"] = tc_chunk.type
                        if tc_chunk.function:
                             if tc_chunk.function.name: tool_call_chunks[index]["function"]["name"] = tc_chunk.function.name
                             if tc_chunk.function.arguments: tool_call_chunks[index]["function"]["arguments"] += tc_chunk.function.arguments

                # Process when the first call finishes
                if finish_reason:
                    if finish_reason == "tool_calls":
                        logger.info("--- Stream detected tool_calls ---")
                        # Reconstruct full tool calls
                        for index in sorted(tool_call_chunks.keys()):
                             chunk_data = tool_call_chunks[index]
                             if chunk_data.get("id") and chunk_data.get("function", {}).get("name"):
                                  try:
                                       reconstructed_tc = ChatCompletionMessageToolCall(
                                           id=chunk_data["id"],
                                           type='function',
                                           function=chunk_data["function"]
                                       )
                                       accumulated_tool_calls.append(reconstructed_tc)
                                  except Exception as recon_err:
                                       logger.error(f"Error reconstructing tool call at index {index}: {recon_err} - Data: {chunk_data}")
                             else:
                                  logger.error(f"Incomplete data for tool call reconstruction at index {index}: {chunk_data}")

                        if accumulated_tool_calls:
                             assistant_message_dict_for_session["tool_calls"] = [tc.dict() for tc in accumulated_tool_calls]
                             assistant_message_dict_for_session["content"] = accumulated_assistant_content or None
                             logger.info(f"Reconstructed {len(accumulated_tool_calls)} tool calls.")
                        else:
                             logger.error("Tool calls detected by finish_reason, but failed reconstruction.")
                             assistant_message_dict_for_session["content"] = accumulated_assistant_content

                    elif finish_reason == "stop":
                        logger.info("--- Stream finished without tool_calls ---")
                        assistant_message_dict_for_session["content"] = accumulated_assistant_content

                    else:
                         logger.warning(f"Stream finished with reason: {finish_reason}")
                         assistant_message_dict_for_session["content"] = accumulated_assistant_content

                    break # Exit the first stream processing loop

            # --- Execute Tools and Second Stream (if needed) ---
            if accumulated_tool_calls:
                logger.info(f"--- Executing {len(accumulated_tool_calls)} Tool Calls (Non-Streamed) ---")
                session["messages"].append(assistant_message_dict_for_session)
                messages_for_second_call = openai_messages + [assistant_message_dict_for_session]

                for tool_call in accumulated_tool_calls:
                    yield json.dumps({"tool_start": tool_call.function.name}) + "\n"; await asyncio.sleep(0.05)
                    event_data_from_tool, tool_result_content = execute_tool_call(tool_call, current_member_id)

                    if event_data_from_tool and final_event_data_to_return is None:
                         if event_data_from_tool.get("action") in ["add", "update", "delete"]:
                              final_event_data_to_return = event_data_from_tool
                              logger.info(f"Captured event_data for stream response: {final_event_data_to_return}")

                    yield json.dumps({"tool_end": tool_call.function.name, "result_preview": tool_result_content[:50]+"..."}) + "\n"; await asyncio.sleep(0.05)

                    tool_result_message = {
                        "tool_call_id": tool_call.id, "role": "tool",
                        "name": tool_call.function.name, "content": tool_result_content,
                    }
                    messages_for_second_call.append(tool_result_message)
                    session["messages"].append(tool_result_message)

                logger.info("--- Calling OpenAI API (Streaming - Second Pass - Summary) ---")
                logger.debug(f"Messages for second stream call (last 4): {json.dumps(messages_for_second_call[-4:], indent=2, ensure_ascii=False)}")
                summary_stream = client.chat.completions.create(
                    model=openai_model, messages=messages_for_second_call,
                    temperature=0.7, max_tokens=1024, stream=True
                )

                final_summary_content = ""
                async for summary_chunk in summary_stream:
                     delta_summary = summary_chunk.choices[0].delta.content if summary_chunk.choices else None
                     if delta_summary:
                          final_summary_content += delta_summary
                          yield json.dumps({"chunk": delta_summary, "type": "html", "content_type": chat_request.content_type}) + "\n"
                          await asyncio.sleep(0)

                session["messages"].append({"role": "assistant", "content": final_summary_content})
                final_response_for_tts = final_summary_content if final_summary_content else "ƒê√£ x·ª≠ l√Ω xong."

            else:
                session["messages"].append(assistant_message_dict_for_session)
                final_response_for_tts = accumulated_assistant_content if accumulated_assistant_content else "V√¢ng."

            # --- Post-Streaming Processing ---
            logger.info("Generating final audio response...")
            audio_response_b64 = text_to_speech_google(final_response_for_tts)

            if current_member_id:
                 summary = generate_chat_summary(session["messages"], openai_api_key)
                 save_chat_history(current_member_id, session["messages"], summary, chat_request.session_id)

            session_manager.update_session(chat_request.session_id, {"messages": session["messages"]})

            complete_response = {
                "complete": True,
                "audio_response": audio_response_b64,
                "content_type": chat_request.content_type,
                "event_data": final_event_data_to_return
            }
            yield json.dumps(complete_response) + "\n"
            logger.info("--- Streaming finished successfully ---")

        except Exception as e:
            logger.error(f"L·ªói nghi√™m tr·ªçng trong qu√° tr√¨nh stream: {str(e)}", exc_info=True)
            error_msg = f"Xin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra trong qu√° tr√¨nh x·ª≠ l√Ω: {str(e)}"
            try:
                yield json.dumps({"error": error_msg, "content_type": chat_request.content_type}) + "\n"
            except Exception as yield_err:
                 logger.error(f"L·ªói khi g·ª≠i th√¥ng b√°o l·ªói stream cu·ªëi c√πng: {yield_err}")
        finally:
            logger.info("ƒê·∫£m b·∫£o l∆∞u session sau khi stream k·∫øt th√∫c ho·∫∑c g·∫∑p l·ªói.")
            session_manager.update_session(chat_request.session_id, {"messages": session.get("messages", [])})

    # Return the StreamingResponse object, wrapping the generator
    return StreamingResponse(
        response_stream_generator(),
        media_type="application/x-ndjson" # Use newline-delimited JSON for streaming
    )


# ------- Other Helper Functions --------

# --- Audio Processing ---
def process_audio(message_dict, api_key):
    """Chuy·ªÉn ƒë·ªïi audio base64 sang text d√πng Whisper."""
    try:
        if not message_dict.get("audio_data"):
            logger.error("process_audio: Thi·∫øu audio_data.")
            return None
        audio_data = base64.b64decode(message_dict["audio_data"])

        # Save to a temporary file (consider file extension if known, else use .wav or .mp3)
        file_extension = ".wav" # Default, adjust if possible
        temp_audio_path = os.path.join(TEMP_DIR, f"{uuid.uuid4()}{file_extension}")

        with open(temp_audio_path, "wb") as f:
            f.write(audio_data)

        client = OpenAI(api_key=api_key)
        with open(temp_audio_path, "rb") as audio_file:
            transcript = client.audio.transcriptions.create(
                model="whisper-1",
                file=audio_file,
            )

        os.remove(temp_audio_path) # Clean up temp file

        return {"type": "text", "text": transcript.text}

    except base64.binascii.Error as b64_err:
        logger.error(f"L·ªói gi·∫£i m√£ Base64 audio: {b64_err}")
        return None
    except Exception as e:
        logger.error(f"L·ªói khi x·ª≠ l√Ω audio: {e}", exc_info=True)
        # Clean up temp file if it exists and error occurred after creation
        if 'temp_audio_path' in locals() and os.path.exists(temp_audio_path):
             try: os.remove(temp_audio_path)
             except OSError: pass
        return None

# --- System Prompt Builder ---
def build_system_prompt(current_member_id=None):
    """X√¢y d·ª±ng system prompt cho tr·ª£ l√Ω gia ƒë√¨nh (s·ª≠ d·ª•ng Tool Calling)."""
    # Start with the base persona and instructions
    system_prompt_parts = [
        "B·∫°n l√† tr·ª£ l√Ω gia ƒë√¨nh th√¥ng minh, ƒëa nƒÉng v√† th√¢n thi·ªán t√™n l√† HGDS. Nhi·ªám v·ª• c·ªßa b·∫°n l√† gi√∫p qu·∫£n l√Ω th√¥ng tin gia ƒë√¨nh, s·ª± ki·ªán, ghi ch√∫, tr·∫£ l·ªùi c√¢u h·ªèi, t√¨m ki·∫øm th√¥ng tin, ph√¢n t√≠ch h√¨nh ·∫£nh v√† ƒë∆∞a ra l·ªùi khuy√™n h·ªØu √≠ch.",
        "Giao ti·∫øp t·ª± nhi√™n, l·ªãch s·ª± v√† theo phong c√°ch tr√≤ chuy·ªán b·∫±ng ti·∫øng Vi·ªát.",
        "S·ª≠ d·ª•ng ƒë·ªãnh d·∫°ng HTML ƒë∆°n gi·∫£n cho ph·∫£n h·ªìi vƒÉn b·∫£n (th·∫ª p, b, i, ul, li, h3, h4, br).",
        f"H√¥m nay l√† {datetime.datetime.now().strftime('%A, %d/%m/%Y')}.", # Vietnamese day name
        "\n**C√°c C√¥ng C·ª• C√≥ S·∫µn:**",
        "B·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng c√°c c√¥ng c·ª• sau khi c·∫ßn thi·∫øt ƒë·ªÉ th·ª±c hi·ªán y√™u c·∫ßu c·ªßa ng∆∞·ªùi d√πng:",
        "- `add_family_member`: ƒê·ªÉ th√™m th√†nh vi√™n m·ªõi.",
        "- `update_preference`: ƒê·ªÉ c·∫≠p nh·∫≠t s·ªü th√≠ch cho th√†nh vi√™n ƒë√£ bi·∫øt.",
        "- `add_event`: ƒê·ªÉ th√™m s·ª± ki·ªán m·ªõi. H√£y cung c·∫•p m√¥ t·∫£ ng√†y theo l·ªùi ng∆∞·ªùi d√πng (v√≠ d·ª•: 'ng√†y mai', 'th·ª© 6 tu·∫ßn sau') v√†o `date_description`, h·ªá th·ªëng s·∫Ω t√≠nh ng√†y ch√≠nh x√°c. Bao g·ªìm m√¥ t·∫£ l·∫∑p l·∫°i (v√≠ d·ª• 'h√†ng tu·∫ßn') trong `description` n·∫øu c√≥.",
        "- `update_event`: ƒê·ªÉ s·ª≠a s·ª± ki·ªán. Cung c·∫•p `event_id` v√† c√°c tr∆∞·ªùng c·∫ßn thay ƒë·ªïi. T∆∞∆°ng t·ª± `add_event` v·ªÅ c√°ch x·ª≠ l√Ω ng√†y (`date_description`) v√† l·∫∑p l·∫°i (`description`).",
        "- `delete_event`: ƒê·ªÉ x√≥a s·ª± ki·ªán.",
        "- `add_note`: ƒê·ªÉ t·∫°o ghi ch√∫ m·ªõi.",
        "\n**QUY T·∫ÆC QUAN TR·ªåNG:**",
        "1.  **Ch·ªß ƒë·ªông s·ª≠ d·ª•ng c√¥ng c·ª•:** Khi ng∆∞·ªùi d√πng y√™u c·∫ßu r√µ r√†ng (th√™m, s·ª≠a, x√≥a, t·∫°o...), h√£y s·ª≠ d·ª•ng c√¥ng c·ª• t∆∞∆°ng ·ª©ng.",
        "2.  **X·ª≠ l√Ω ng√†y/gi·ªù:** KH√îNG t·ª± t√≠nh to√°n ng√†y YYYY-MM-DD. H√£y g·ª≠i m√¥ t·∫£ ng√†y c·ªßa ng∆∞·ªùi d√πng (v√≠ d·ª• 'ng√†y mai', '20/7', 'th·ª© 3 t·ªõi') trong tr∆∞·ªùng `date_description` c·ªßa c√¥ng c·ª• `add_event` ho·∫∑c `update_event`. N·∫øu s·ª± ki·ªán l·∫∑p l·∫°i, h√£y n√™u r√µ trong tr∆∞·ªùng `description` (v√≠ d·ª• 'h·ªçc ti·∫øng Anh th·ª© 6 h√†ng tu·∫ßn').",
        "3.  **T√¨m ki·∫øm & Th·ªùi ti·∫øt:** S·ª≠ d·ª•ng th√¥ng tin t√¨m ki·∫øm ho·∫∑c th·ªùi ti·∫øt ƒë∆∞·ª£c cung c·∫•p trong context (ƒë√°nh d·∫•u b·∫±ng --- TH√îNG TIN ---) ƒë·ªÉ tr·∫£ l·ªùi c√°c c√¢u h·ªèi li√™n quan. ƒê·ª´ng g·ªçi c√¥ng c·ª• n·∫øu th√¥ng tin ƒë√£ c√≥ s·∫µn.",
        "4.  **Ph√¢n t√≠ch h√¨nh ·∫£nh:** Khi nh·∫≠n ƒë∆∞·ª£c h√¨nh ·∫£nh, h√£y m√¥ t·∫£ n√≥ v√† li√™n k·∫øt v·ªõi th√¥ng tin gia ƒë√¨nh n·∫øu ph√π h·ª£p.",
        "5.  **X√°c nh·∫≠n:** Sau khi s·ª≠ d·ª•ng c√¥ng c·ª• th√†nh c√¥ng (nh·∫≠n ƒë∆∞·ª£c k·∫øt qu·∫£ t·ª´ 'tool role'), h√£y th√¥ng b√°o ng·∫Øn g·ªçn cho ng∆∞·ªùi d√πng bi·∫øt h√†nh ƒë·ªông ƒë√£ ƒë∆∞·ª£c th·ª±c hi·ªán d·ª±a tr√™n k·∫øt qu·∫£ ƒë√≥. N·∫øu tool th·∫•t b·∫°i, h√£y th√¥ng b√°o l·ªói m·ªôt c√°ch l·ªãch s·ª±."
        "6. **ƒê·ªô d√†i ph·∫£n h·ªìi:** Gi·ªØ ph·∫£n h·ªìi cu·ªëi c√πng cho ng∆∞·ªùi d√πng t∆∞∆°ng ƒë·ªëi ng·∫Øn g·ªçn v√† t·∫≠p trung v√†o y√™u c·∫ßu ch√≠nh, tr·ª´ khi ƒë∆∞·ª£c y√™u c·∫ßu chi ti·∫øt."
    ]

    # Add current user context
    member_context = ""
    if current_member_id and current_member_id in family_data:
        current_member = family_data[current_member_id]
        member_context = f"""
        \n**Th√¥ng Tin Ng∆∞·ªùi D√πng Hi·ªán T·∫°i:**
        - ID: {current_member_id}
        - T√™n: {current_member.get('name')}
        - Tu·ªïi: {current_member.get('age', 'Ch∆∞a bi·∫øt')}
        - S·ªü th√≠ch: {json.dumps(current_member.get('preferences', {}), ensure_ascii=False)}
        (H√£y c√° nh√¢n h√≥a t∆∞∆°ng t√°c v√† ghi nh·∫≠n h√†nh ƒë·ªông d∆∞·ªõi t√™n ng∆∞·ªùi d√πng n√†y. S·ª≠ d·ª•ng ID '{current_member_id}' khi c·∫ßn `member_id`.)
        """
        system_prompt_parts.append(member_context)
    else:
         system_prompt_parts.append("\n(Hi·ªán t·∫°i ƒëang t∆∞∆°ng t√°c v·ªõi kh√°ch.)")


    # Add data context (Limit size if data grows large)
    # Maybe summarize data instead of dumping everything?
    data_summary = {
         "Th√†nh vi√™n": list(family_data.keys()),
         "S·ªë s·ª± ki·ªán": len(events_data),
         "S·ªë ghi ch√∫": len(notes_data)
    }
    # Example: Show details only for a few recent events?
    recent_events_summary = {}
    try:
         # Sort events by created_on (assuming ISO format) descending
         sorted_event_ids = sorted(
             events_data.keys(),
             key=lambda eid: events_data[eid].get("created_on", ""),
             reverse=True
         )
         for eid in sorted_event_ids[:3]: # Show last 3 added
              event = events_data[eid]
              recent_events_summary[eid] = f"{event.get('title')} ({event.get('date')})"
    except Exception as sort_err:
         logger.error(f"Error summarizing recent events: {sort_err}")
         recent_events_summary = {"error": "Kh√¥ng th·ªÉ t√≥m t·∫Øt"}


    data_context = f"""
    \n**D·ªØ Li·ªáu Hi·ªán T·∫°i (T√≥m t·∫Øt):**
    *   Th√†nh vi√™n (IDs): {json.dumps(list(family_data.keys()), ensure_ascii=False)}
    *   S·ª± ki·ªán g·∫ßn ƒë√¢y (IDs & Titles): {json.dumps(recent_events_summary, ensure_ascii=False)} (T·ªïng c·ªông: {len(events_data)})
    *   Ghi ch√∫ (T·ªïng c·ªông): {len(notes_data)}
    (S·ª≠ d·ª•ng ID s·ª± ki·ªán t·ª´ t√≥m t·∫Øt n√†y khi c·∫ßn `event_id` cho vi·ªác c·∫≠p nh·∫≠t ho·∫∑c x√≥a.)
    """
    # Alternative: Full data dump (use with caution for large data)
    # data_context = f"""
    # \n**D·ªØ Li·ªáu Hi·ªán T·∫°i:**
    # *   Th√†nh vi√™n gia ƒë√¨nh: {json.dumps(family_data, ensure_ascii=False, indent=2, default=str)}
    # *   S·ª± ki·ªán: {json.dumps(events_data, ensure_ascii=False, indent=2, default=str)}
    # *   Ghi ch√∫: {json.dumps(notes_data, ensure_ascii=False, indent=2, default=str)}
    # """
    system_prompt_parts.append(data_context)

    return "\n".join(system_prompt_parts)


# --- Search & Summarize Helpers ---
async def check_search_need(messages: List[Dict], openai_api_key: str, tavily_api_key: str) -> str:
    """Ki·ªÉm tra nhu c·∫ßu t√¨m ki·∫øm, th·ªùi ti·∫øt, t∆∞ v·∫•n t·ª´ tin nh·∫Øn cu·ªëi c·ªßa ng∆∞·ªùi d√πng."""
    if not tavily_api_key: return "" # Need Tavily for web search part

    last_user_message_content = None
    # Find the last message from the user
    for message in reversed(messages):
        if message["role"] == "user":
            last_user_message_content = message["content"]
            break

    if not last_user_message_content: return ""

    # Extract text content from the last user message
    last_user_text = ""
    if isinstance(last_user_message_content, str):
        last_user_text = last_user_message_content
    elif isinstance(last_user_message_content, list):
        # Find the first text part
        for item in last_user_message_content:
             if isinstance(item, dict) and item.get("type") == "text":
                  last_user_text = item.get("text", "")
                  break # Use the first text part found

    if not last_user_text: return "" # No text found to analyze

    logger.info(f"Checking search/weather/advice need for: '{last_user_text[:100]}...'")

    # 1. Check for Advice Query
    is_advice_query, advice_type, time_term = weather_advisor.detect_advice_query(last_user_text)
    if is_advice_query:
        logger.info(f"Ph√°t hi·ªán truy v·∫•n t∆∞ v·∫•n: lo·∫°i={advice_type}, th·ªùi gian={time_term}")
        target_date = get_date_from_relative_term(time_term) if time_term else None

        # Extract location for advice context
        location = "H√† N·ªôi" # Default
        loc_pattern = r'(·ªü|t·∫°i)\s+([^?.,!\n]+)'
        loc_match = re.search(loc_pattern, last_user_text.lower())
        if loc_match: location = loc_match.group(2).strip().title()

        try:
            weather_data = await weather_service.get_weather(location, forecast_days=7, target_date=target_date) # Get enough forecast data
            if not weather_data or weather_data.get("error"):
                 logger.warning(f"Kh√¥ng th·ªÉ l·∫•y d·ªØ li·ªáu th·ªùi ti·∫øt cho t∆∞ v·∫•n t·∫°i {location}.")
                 # Fall through to general search if weather fails
            else:
                 advice_result = ""
                 if advice_type == "clothing": advice_result = weather_advisor.get_clothing_advice(weather_data, target_date)
                 elif advice_type == "activity": advice_result = weather_advisor.get_activity_advice(weather_data, target_date)
                 elif advice_type == "items": advice_result = weather_advisor.get_items_advice(weather_data, target_date)

                 # Get weather summary for context, marked as non-displayable
                 weather_context_summary = weather_service.format_weather_message(weather_data, location, 1, target_date, advice_only=True)

                 advice_prompt_addition = f"""
                 \n\n--- TH√îNG TIN T∆Ø V·∫§N TH·ªúI TI·∫æT (D√ôNG ƒê·ªÇ THAM KH·∫¢O) ---
                 Ng∆∞·ªùi d√πng h·ªèi: "{last_user_text}" (Lo·∫°i: {advice_type}, Th·ªùi gian: {time_term}, V·ªã tr√≠: {location})
                 T√≥m t·∫Øt th·ªùi ti·∫øt li√™n quan: {weather_context_summary}
                 L·ªùi khuy√™n ƒë∆∞·ª£c t·∫°o:
                 {advice_result}
                 --- K·∫æT TH√öC T∆Ø V·∫§N ---
                 H√£y tr·∫£ l·ªùi ng∆∞·ªùi d√πng b·∫±ng c√°ch di·ªÖn ƒë·∫°t l·∫°i ph·∫ßn "L·ªùi khuy√™n ƒë∆∞·ª£c t·∫°o" m·ªôt c√°ch t·ª± nhi√™n. KH√îNG ƒë∆∞a ra c√°c ch·ªâ s·ªë th·ªùi ti·∫øt c·ª• th·ªÉ tr·ª´ khi l·ªùi khuy√™n c√≥ ƒë·ªÅ c·∫≠p.
                 """
                 return advice_prompt_addition
        except Exception as advice_err:
            logger.error(f"L·ªói khi x·ª≠ l√Ω truy v·∫•n t∆∞ v·∫•n: {advice_err}", exc_info=True)
            # Fall through to general search on error

    # 2. Check for Weather Query (if not advice)
    is_weather_query, location, days, time_term_weather = weather_service.detect_weather_query(last_user_text)
    if is_weather_query and location:
        logger.info(f"Ph√°t hi·ªán truy v·∫•n th·ªùi ti·∫øt: v·ªã tr√≠={location}, c·ª•m t·ª´='{time_term_weather}'")
        target_date = get_date_from_relative_term(time_term_weather) if time_term_weather else None
        try:
            weather_data = await weather_service.get_weather(location, days=7, target_date=target_date) # Get enough data
            if not weather_data or weather_data.get("error"):
                 logger.warning(f"Kh√¥ng th·ªÉ l·∫•y d·ªØ li·ªáu th·ªùi ti·∫øt cho {location} t·ª´ API.")
                 # Fall through to search as backup
            else:
                 weather_html = weather_service.format_weather_message(weather_data, location, days, target_date) # Format requested days
                 weather_prompt_addition = f"""
                 \n\n--- TH√îNG TIN TH·ªúI TI·∫æT (D√ôNG ƒê·ªÇ TR·∫¢ L·ªúI) ---
                 Ng∆∞·ªùi d√πng h·ªèi: "{last_user_text}"
                 D·ª± b√°o chi ti·∫øt:
                 {weather_html}
                 --- K·∫æT TH√öC TH√îNG TIN TH·ªúI TI·∫æT ---
                 H√£y tr√¨nh b√†y th√¥ng tin th·ªùi ti·∫øt chi ti·∫øt ·ªü tr√™n cho ng∆∞·ªùi d√πng m·ªôt c√°ch r√µ r√†ng, th√¢n thi·ªán.
                 """
                 return weather_prompt_addition
        except Exception as weather_err:
             logger.error(f"L·ªói khi l·∫•y/ƒë·ªãnh d·∫°ng th√¥ng tin th·ªùi ti·∫øt: {weather_err}", exc_info=True)
             # Fall through to general search on error

    # 3. Check for General Search Intent (if not weather/advice)
    if tavily_api_key: # Only proceed if Tavily key exists
         need_search, search_query, is_news_query = await detect_search_intent(last_user_text, openai_api_key) # Changed to await
         if need_search:
             logger.info(f"Ph√°t hi·ªán nhu c·∫ßu t√¨m ki·∫øm: query='{search_query}', is_news={is_news_query}")
             domains_to_include = VIETNAMESE_NEWS_DOMAINS if is_news_query else None
             try:
                search_summary = await search_and_summarize( # Changed to await
                    tavily_api_key, search_query, openai_api_key, include_domains=domains_to_include
                )
                search_prompt_addition = f"""
                \n\n--- TH√îNG TIN T√åM KI·∫æM (D√ôNG ƒê·ªÇ TR·∫¢ L·ªúI) ---
                Ng∆∞·ªùi d√πng h·ªèi: "{last_user_text}"
                K·∫øt qu·∫£ t√¨m ki·∫øm v√† t√≥m t·∫Øt cho truy v·∫•n '{search_query}':
                {search_summary}
                --- K·∫æT TH√öC TH√îNG TIN T√åM KI·∫æM ---
                H√£y s·ª≠ d·ª•ng k·∫øt qu·∫£ t√≥m t·∫Øt n√†y ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch t·ª± nhi√™n, tr√≠ch d·∫´n ngu·ªìn n·∫øu c√≥.
                """
                return search_prompt_addition
             except Exception as search_err:
                  logger.error(f"L·ªói khi t√¨m ki·∫øm/t√≥m t·∫Øt cho '{search_query}': {search_err}", exc_info=True)
                  return "\n\n--- L·ªñI T√åM KI·∫æM: Kh√¥ng th·ªÉ l·∫•y th√¥ng tin. H√£y b√°o l·∫°i cho ng∆∞·ªùi d√πng. ---"

    # No specific need detected
    logger.info("Kh√¥ng ph√°t hi·ªán nhu c·∫ßu t√¨m ki·∫øm/th·ªùi ti·∫øt/t∆∞ v·∫•n ƒë·∫∑c bi·ªát.")
    return ""

# Make Tavily functions async if they use async libs, or wrap sync calls
# Assuming Tavily client might be sync, wrap with asyncio.to_thread
# For simplicity, keeping them sync for now, but mark as needing async upgrade

async def tavily_extract(api_key, urls, include_images=False, extract_depth="basic"):
    """Tr√≠ch xu·∫•t n·ªôi dung t·ª´ URL (Wrap sync call)."""
    # ... (keep internal sync logic) ...
    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    data = {"urls": urls, "include_images": include_images, "extract_depth": extract_depth}
    try:
        response = await asyncio.to_thread(
             requests.post, "https://api.tavily.com/extract", headers=headers, json=data, timeout=15
        )
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        logger.error(f"L·ªói Tavily Extract API ({e.__class__.__name__}): {e}")
        return None
    except Exception as e:
         logger.error(f"L·ªói kh√¥ng x√°c ƒë·ªãnh trong tavily_extract: {e}", exc_info=True)
         return None


async def tavily_search(api_key, query, search_depth="advanced", max_results=5, include_domains=None, exclude_domains=None):
    """T√¨m ki·∫øm Tavily (Wrap sync call)."""
    # ... (keep internal sync logic) ...
    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    data = {"query": query, "search_depth": search_depth, "max_results": max_results}
    if include_domains: data["include_domains"] = include_domains
    if exclude_domains: data["exclude_domains"] = exclude_domains
    try:
        response = await asyncio.to_thread(
            requests.post, "https://api.tavily.com/search", headers=headers, json=data, timeout=10
        )
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        logger.error(f"L·ªói Tavily Search API ({e.__class__.__name__}): {e}")
        return None
    except Exception as e:
         logger.error(f"L·ªói kh√¥ng x√°c ƒë·ªãnh trong tavily_search: {e}", exc_info=True)
         return None


async def search_and_summarize(tavily_api_key, query, openai_api_key, include_domains=None):
    """T√¨m ki·∫øm v√† t·ªïng h·ª£p (ƒë√£ l√† async do g·ªçi c√°c h√†m async con)."""
    if not tavily_api_key or not openai_api_key or not query:
        return "Thi·∫øu th√¥ng tin API key ho·∫∑c c√¢u truy v·∫•n."

    try:
        logger.info(f"B·∫Øt ƒë·∫ßu t√¨m ki·∫øm Tavily cho: '{query}'" + (f" (Domains: {include_domains})" if include_domains else ""))
        search_results = await tavily_search(
            tavily_api_key, query, include_domains=include_domains, max_results=5 # Limit results
        )

        if not search_results or not search_results.get("results"):
            logger.warning(f"Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ Tavily cho '{query}'")
            return f"Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ n√†o cho '{query}'" + (f" trong c√°c trang tin t·ª©c ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh." if include_domains else ".")

        # Extract content from top results more efficiently
        urls_to_extract = [result["url"] for result in search_results["results"][:3]] # Extract top 3
        if not urls_to_extract:
            logger.warning(f"Kh√¥ng c√≥ URL n√†o ƒë·ªÉ tr√≠ch xu·∫•t t·ª´ k·∫øt qu·∫£ Tavily cho '{query}'.")
            return f"ƒê√£ t√¨m th·∫•y m·ªôt s·ªë ti√™u ƒë·ªÅ li√™n quan ƒë·∫øn '{query}' nh∆∞ng kh√¥ng th·ªÉ tr√≠ch xu·∫•t n·ªôi dung."

        logger.info(f"Tr√≠ch xu·∫•t n·ªôi dung t·ª´ URLs: {urls_to_extract}")
        # Use tavily_extract which now supports list of URLs
        extract_result = await tavily_extract(tavily_api_key, urls_to_extract) # Pass list directly

        extracted_contents = []
        if extract_result and extract_result.get("results"):
             for res in extract_result["results"]:
                  content = res.get("raw_content", "")
                  if content: # Only add if content exists
                       # Limit length per source
                       max_len_per_source = 4000
                       content = content[:max_len_per_source] + "..." if len(content) > max_len_per_source else content
                       extracted_contents.append({"url": res.get("url"), "content": content})
                  else:
                       logger.warning(f"N·ªôi dung tr·ªëng r·ªóng t·ª´ URL: {res.get('url')}")
        else:
             logger.warning(f"Tr√≠ch xu·∫•t n·ªôi dung th·∫•t b·∫°i t·ª´ Tavily cho URLs: {urls_to_extract}")
             # Fallback: use titles/snippets from search results?
             basic_info = ""
             for res in search_results.get("results", [])[:3]:
                 basic_info += f"- Ti√™u ƒë·ªÅ: {res.get('title', '')}\n URL: {res.get('url')}\n\n"
             if basic_info:
                  return f"Kh√¥ng th·ªÉ tr√≠ch xu·∫•t chi ti·∫øt n·ªôi dung, nh∆∞ng ƒë√¢y l√† m·ªôt s·ªë k·∫øt qu·∫£ t√¨m th·∫•y:\n{basic_info}"
             else:
                  return f"Kh√¥ng th·ªÉ tr√≠ch xu·∫•t n·ªôi dung t·ª´ c√°c k·∫øt qu·∫£ t√¨m ki·∫øm cho '{query}'."


        if not extracted_contents:
             logger.warning(f"Kh√¥ng c√≥ n·ªôi dung n√†o ƒë∆∞·ª£c tr√≠ch xu·∫•t th√†nh c√¥ng cho '{query}'.")
             return f"Kh√¥ng th·ªÉ tr√≠ch xu·∫•t n·ªôi dung chi ti·∫øt cho '{query}'."

        # Summarize using OpenAI
        logger.info(f"T·ªïng h·ª£p {len(extracted_contents)} ngu·ªìn tr√≠ch xu·∫•t cho '{query}'.")
        client = OpenAI(api_key=openai_api_key)

        # Prepare content string for prompt, limiting total length
        content_for_prompt = ""
        total_len = 0
        max_total_len = 15000 # Limit total context length for summarization
        for item in extracted_contents:
             source_text = f"\n--- Ngu·ªìn: {item['url']} ---\n{item['content']}\n--- H·∫øt ngu·ªìn ---\n"
             if total_len + len(source_text) > max_total_len:
                  logger.warning(f"ƒê√£ ƒë·∫°t gi·ªõi h·∫°n ƒë·ªô d√†i context khi t·ªïng h·ª£p, b·ªè qua c√°c ngu·ªìn sau.")
                  break
             content_for_prompt += source_text
             total_len += len(source_text)


        prompt = f"""
        D∆∞·ªõi ƒë√¢y l√† n·ªôi dung tr√≠ch xu·∫•t t·ª´ c√°c trang web li√™n quan ƒë·∫øn c√¢u h·ªèi: "{query}"

        {content_for_prompt}

        Nhi·ªám v·ª• c·ªßa b·∫°n:
        1.  **T·ªïng h·ª£p th√¥ng tin ch√≠nh:** Ph√¢n t√≠ch v√† t·ªïng h·ª£p c√°c th√¥ng tin quan tr·ªçng nh·∫•t t·ª´ c√°c ngu·ªìn tr√™n ƒë·ªÉ tr·∫£ l·ªùi cho c√¢u h·ªèi "{query}".
        2.  **T·∫≠p trung v√†o ng√†y c·ª• th·ªÉ (n·∫øu c√≥):** N·∫øu c√¢u h·ªèi ƒë·ªÅ c·∫≠p ng√†y c·ª• th·ªÉ, ∆∞u ti√™n th√¥ng tin ng√†y ƒë√≥.
        3.  **Tr√¨nh b√†y r√µ r√†ng:** Vi·∫øt m·ªôt b·∫£n t√≥m t·∫Øt m·∫°ch l·∫°c, c√≥ c·∫•u tr√∫c b·∫±ng ti·∫øng Vi·ªát.
        4.  **X·ª≠ l√Ω m√¢u thu·∫´n:** N·∫øu c√≥ th√¥ng tin tr√°i ng∆∞·ª£c, h√£y n√™u r√µ.
        5.  **N√™u ngu·ªìn:** C·ªë g·∫Øng tr√≠ch d·∫´n ngu·ªìn (URL) cho c√°c th√¥ng tin quan tr·ªçng n·∫øu c√≥ th·ªÉ, v√≠ d·ª•: "(Ngu·ªìn: [URL])".
        6.  **Ph·∫°m vi:** Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ c√°c ngu·ªìn ƒë∆∞·ª£c cung c·∫•p. Kh√¥ng th√™m ki·∫øn th·ª©c ngo√†i.
        7.  **ƒê·ªãnh d·∫°ng:** S·ª≠ d·ª•ng HTML ƒë∆°n gi·∫£n (p, b, ul, li).

        H√£y b·∫Øt ƒë·∫ßu b·∫£n t√≥m t·∫Øt c·ªßa b·∫°n.
        """

        try:
            response = await asyncio.to_thread( # Run sync completion in thread
                 client.chat.completions.create,
                 model=openai_model, # Use a model good at summarization
                 messages=[
                     {"role": "system", "content": "B·∫°n l√† m·ªôt tr·ª£ l√Ω t·ªïng h·ª£p th√¥ng tin chuy√™n nghi·ªáp. Nhi·ªám v·ª• c·ªßa b·∫°n l√† t·ªïng h·ª£p n·ªôi dung t·ª´ c√°c ngu·ªìn ƒë∆∞·ª£c cung c·∫•p ƒë·ªÉ t·∫°o ra m·ªôt b·∫£n t√≥m t·∫Øt ch√≠nh x√°c, t·∫≠p trung v√†o y√™u c·∫ßu c·ªßa ng∆∞·ªùi d√πng v√† tr√≠ch d·∫´n ngu·ªìn n·∫øu c√≥ th·ªÉ."},
                     {"role": "user", "content": prompt}
                 ],
                 temperature=0.3,
                 max_tokens=1500 # Allow longer summary if needed
            )
            summarized_info = response.choices[0].message.content

            # Optionally add a footer with all sources if not included in summary
            # sources_footer = "\n\n<p><b>Ngu·ªìn tham kh·∫£o:</b></p><ul>" + "".join([f"<li><a href='{c['url']}' target='_blank'>{c['url']}</a></li>" for c in extracted_contents]) + "</ul>"
            # if not any(c['url'] in summarized_info for c in extracted_contents):
            #      final_response = f"{summarized_info}{sources_footer}"
            # else:
            #      final_response = summarized_info

            return summarized_info.strip()

        except Exception as summary_err:
             logger.error(f"L·ªói khi g·ªçi OpenAI ƒë·ªÉ t·ªïng h·ª£p: {summary_err}", exc_info=True)
             # Fallback: return combined raw content? Or just error?
             # return "L·ªói khi t·ªïng h·ª£p th√¥ng tin. N·ªôi dung g·ªëc:\n" + content_for_prompt
             return "Xin l·ªói, t√¥i g·∫∑p l·ªói khi ƒëang t√≥m t·∫Øt th√¥ng tin t√¨m ki·∫øm."


    except Exception as e:
        logger.error(f"L·ªói trong qu√° tr√¨nh t√¨m ki·∫øm v√† t·ªïng h·ª£p cho '{query}': {e}", exc_info=True)
        return f"C√≥ l·ªói x·∫£y ra trong qu√° tr√¨nh t√¨m ki·∫øm v√† t·ªïng h·ª£p th√¥ng tin: {str(e)}"


async def detect_search_intent(query, api_key):
    """Ph√°t hi·ªán √Ω ƒë·ªãnh t√¨m ki·∫øm (async wrapper)."""
    if not api_key or not query: return False, query, False

    try:
        client = OpenAI(api_key=api_key)
        current_date_str = datetime.datetime.now().strftime("%Y-%m-%d")
        # Prompt remains the same, instructing JSON output
        system_prompt = f"""
B·∫°n l√† m·ªôt h·ªá th·ªëng ph√¢n lo·∫°i v√† tinh ch·ªânh c√¢u h·ªèi th√¥ng minh. Nhi·ªám v·ª• c·ªßa b·∫°n l√†:
1. X√°c ƒë·ªãnh xem c√¢u h·ªèi c√≥ c·∫ßn t√¨m ki·∫øm th√¥ng tin th·ª±c t·∫ø, tin t·ª©c m·ªõi ho·∫∑c d·ªØ li·ªáu c·∫≠p nh·∫≠t kh√¥ng (`need_search`). C√¢u h·ªèi v·ªÅ ki·∫øn th·ª©c chung, ƒë·ªãnh nghƒ©a ƒë∆°n gi·∫£n th∆∞·ªùng kh√¥ng c·∫ßn t√¨m ki·∫øm.
2. N·∫øu c·∫ßn t√¨m ki·∫øm, h√£y tinh ch·ªânh c√¢u h·ªèi th√†nh m·ªôt truy v·∫•n t√¨m ki·∫øm t·ªëi ∆∞u (`search_query`), bao g·ªìm y·∫øu t·ªë th·ªùi gian n·∫øu c√≥ (h√¥m nay, 26/03...).
3. X√°c ƒë·ªãnh xem c√¢u h·ªèi c√≥ ch·ªß y·∫øu v·ªÅ tin t·ª©c, th·ªùi s·ª±, th·ªÉ thao, s·ª± ki·ªán hi·ªán t·∫°i kh√¥ng (`is_news_query`). C√¢u h·ªèi v·ªÅ th·ªùi ti·∫øt c≈©ng coi l√† tin t·ª©c. C√¢u h·ªèi v·ªÅ gi√° c·∫£, s·∫£n ph·∫©m, h∆∞·ªõng d·∫´n KH√îNG ph·∫£i l√† tin t·ª©c.

H√¥m nay l√† ng√†y: {current_date_str}.

V√≠ d·ª•:
- User: "tin t·ª©c covid h√¥m nay" -> {{ "need_search": true, "search_query": "tin t·ª©c covid m·ªõi nh·∫•t ng√†y {current_date_str}", "is_news_query": true }}
- User: "k·∫øt qu·∫£ tr·∫≠n MU t·ªëi qua" -> {{ "need_search": true, "search_query": "k·∫øt qu·∫£ Manchester United t·ªëi qua", "is_news_query": true }}
- User: "c√≥ phim g√¨ hay tu·∫ßn n√†y?" -> {{ "need_search": true, "search_query": "phim chi·∫øu r·∫°p hay tu·∫ßn n√†y", "is_news_query": false }}
- User: "gi√° v√†ng SJC" -> {{ "need_search": true, "search_query": "gi√° v√†ng SJC m·ªõi nh·∫•t", "is_news_query": false }}
- User: "th·ªß ƒë√¥ n∆∞·ªõc Ph√°p l√† g√¨?" -> {{ "need_search": false, "search_query": "th·ªß ƒë√¥ n∆∞·ªõc Ph√°p l√† g√¨?", "is_news_query": false }}
- User: "th·ªùi ti·∫øt H√† N·ªôi ng√†y mai" -> {{ "need_search": true, "search_query": "d·ª± b√°o th·ªùi ti·∫øt H√† N·ªôi ng√†y mai", "is_news_query": true }}
- User: "c√°ch l√†m b√°nh cu·ªën" -> {{ "need_search": true, "search_query": "c√°ch l√†m b√°nh cu·ªën ngon", "is_news_query": false }}
- User: "sin(pi/2) b·∫±ng m·∫•y?" -> {{ "need_search": false, "search_query": "sin(pi/2)", "is_news_query": false }}


Tr·∫£ l·ªùi D∆Ø·ªöI D·∫†NG JSON H·ª¢P L·ªÜ v·ªõi 3 tr∆∞·ªùng: need_search (boolean), search_query (string), is_news_query (boolean).
"""
        # Run synchronous API call in a thread
        response = await asyncio.to_thread(
             client.chat.completions.create,
             model=openai_model, # Or a faster model if available
             messages=[
                 {"role": "system", "content": system_prompt},
                 {"role": "user", "content": f"C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: \"{query}\""}
             ],
             temperature=0.1,
             max_tokens=150, # Shorter response needed
             response_format={"type": "json_object"}
        )

        result_str = response.choices[0].message.content
        logger.info(f"K·∫øt qu·∫£ detect_search_intent (raw): {result_str}")

        try:
            result = json.loads(result_str)
            need_search = result.get("need_search", False)
            search_query = query
            is_news_query = False

            if need_search:
                search_query = result.get("search_query", query)
                if not search_query: search_query = query # Fallback if empty
                is_news_query = result.get("is_news_query", False)

            logger.info(f"Ph√¢n t√≠ch truy v·∫•n '{query}': need_search={need_search}, search_query='{search_query}', is_news_query={is_news_query}")
            return need_search, search_query, is_news_query

        except (json.JSONDecodeError, TypeError) as e:
            logger.error(f"L·ªói gi·∫£i m√£ JSON t·ª´ detect_search_intent: {e}. Raw: {result_str}")
            # Fallback: assume search needed for safety? Or not? Let's assume not.
            return False, query, False
    except Exception as e:
        logger.error(f"L·ªói khi g·ªçi OpenAI trong detect_search_intent: {e}", exc_info=True)
        return False, query, False # Default to no search on error

# --- Suggested Questions ---
def generate_dynamic_suggested_questions(api_key, member_id=None, max_questions=5):
    """T·∫°o c√¢u h·ªèi g·ª£i √Ω ƒë·ªông (s·ª≠ d·ª•ng m·∫´u c√¢u)."""
    # This function can remain largely the same as the template-based one
    # if calling OpenAI here is too slow or costly.
    # Re-pasting the template logic here for completeness.
    logger.info("S·ª≠ d·ª•ng ph∆∞∆°ng ph√°p m·∫´u c√¢u ƒë·ªÉ t·∫°o c√¢u h·ªèi g·ª£i √Ω")

    random_seed = int(hashlib.md5(f"{datetime.datetime.now().strftime('%Y-%m-%d_%H')}_{member_id or 'guest'}".encode()).hexdigest(), 16)
    random.seed(random_seed)

    question_templates = {
        "news": [ "Tin t·ª©c {topic} m·ªõi nh·∫•t?", "C√≥ g√¨ m·ªõi v·ªÅ {topic} h√¥m nay?", "ƒêi·ªÉm tin {topic} s√°ng nay?", "C·∫≠p nh·∫≠t t√¨nh h√¨nh {topic}?" ],
        "weather": [ "Th·ªùi ti·∫øt {location} ng√†y mai th·∫ø n√†o?", "D·ª± b√°o th·ªùi ti·∫øt {location} cu·ªëi tu·∫ßn?", "{location} h√¥m nay c√≥ m∆∞a kh√¥ng?" ],
        "events": [ "S·ª± ki·ªán n·ªïi b·∫≠t tu·∫ßn n√†y?", "L·ªãch chi·∫øu phim {cinema}?", "Tr·∫≠n ƒë·∫•u {team} t·ªëi nay m·∫•y gi·ªù?" ],
        "food": [ "C√¥ng th·ª©c n·∫•u m√≥n {dish}?", "Qu√°n {dish} ngon ·ªü {district}?", "C√°ch l√†m {dessert} ƒë∆°n gi·∫£n?" ],
        "hobbies": [ "S√°ch hay v·ªÅ ch·ªß ƒë·ªÅ {genre}?", "M·∫πo ch·ª•p ·∫£nh ƒë·∫πp b·∫±ng ƒëi·ªán tho·∫°i?", "B√†i t·∫≠p yoga gi·∫£m cƒÉng th·∫≥ng?" ],
        "general": [ "K·ªÉ m·ªôt c√¢u chuy·ªán c∆∞·ªùi?", "ƒê·ªë vui v·ªÅ {category}?", "H√¥m nay c√≥ ng√†y g√¨ ƒë·∫∑c bi·ªát?" ]
    }

    # Get member preferences if available
    prefs = {}
    if member_id and member_id in family_data:
         prefs = family_data[member_id].get("preferences", {})

    # Variables for templates
    replacements = {
        "topic": ["th·∫ø gi·ªõi", "kinh t·∫ø", "th·ªÉ thao", "gi·∫£i tr√≠", "c√¥ng ngh·ªá", "gi√°o d·ª•c", "y t·∫ø", prefs.get("hobby", "khoa h·ªçc")],
        "location": [prefs.get("location", "H√† N·ªôi"), "TP HCM", "ƒê√† N·∫µng", "n∆°i b·∫°n ·ªü"],
        "cinema": ["CGV", "Lotte", "BHD", "Galaxy"],
        "team": [prefs.get("team", "Vi·ªát Nam"), "Man City", "Real Madrid", "Arsenal"],
        "dish": [prefs.get("food", "ph·ªü"), "b√∫n ch·∫£", "c∆°m t·∫•m", "pizza", "sushi"],
        "district": ["qu·∫≠n 1", "Ho√†n Ki·∫øm", "H·∫£i Ch√¢u", "g·∫ßn ƒë√¢y"],
        "dessert": ["ch√®", "b√°nh flan", "rau c√¢u"],
        "genre": [prefs.get("book_genre", "trinh th√°m"), "l·ªãch s·ª≠", "khoa h·ªçc vi·ªÖn t∆∞·ªüng", "t√¢m l√Ω"],
        "category": ["ƒë·ªông v·∫≠t", "l·ªãch s·ª≠", "khoa h·ªçc", "phim ·∫£nh"]
    }

    all_questions = []
    categories = list(question_templates.keys())
    random.shuffle(categories)

    for category in categories:
        template = random.choice(question_templates[category])
        question = template
        # Simple replacement - find first placeholder and replace
        placeholder_match = re.search(r'\{(\w+)\}', question)
        while placeholder_match:
             key = placeholder_match.group(1)
             if key in replacements:
                  replacement = random.choice(replacements[key])
                  question = question.replace(placeholder_match.group(0), replacement, 1)
             else:
                  question = question.replace(placeholder_match.group(0), "...", 1) # Fallback
             placeholder_match = re.search(r'\{(\w+)\}', question) # Find next
        all_questions.append(question)

    # Select unique questions up to max_questions
    final_suggestions = []
    seen_suggestions = set()
    for q in all_questions:
         if len(final_suggestions) >= max_questions: break
         if q not in seen_suggestions:
              final_suggestions.append(q)
              seen_suggestions.add(q)

    # Ensure minimum number of questions if generation fails
    while len(final_suggestions) < max_questions and len(final_suggestions) < len(all_questions):
         q = random.choice(all_questions)
         if q not in seen_suggestions:
              final_suggestions.append(q)
              seen_suggestions.add(q)


    logger.info(f"ƒê√£ t·∫°o {len(final_suggestions)} c√¢u h·ªèi g·ª£i √Ω b·∫±ng m·∫´u.")
    return final_suggestions


# --- Chat History ---
def generate_chat_summary(messages, api_key):
    """T·∫°o t√≥m t·∫Øt t·ª´ l·ªãch s·ª≠ tr√≤ chuy·ªán (async wrapper)."""
    if not api_key or not messages or len(messages) < 2:
        return "Ch∆∞a ƒë·ªß n·ªôi dung ƒë·ªÉ t√≥m t·∫Øt."

    # Prepare conversation text, focusing on user and final assistant messages
    conversation_text = ""
    for msg in messages[-10:]: # Limit context for summary
        role = msg.get("role")
        content = msg.get("content")
        text_content = ""
        if isinstance(content, str):
            text_content = content
        elif isinstance(content, list):
             for item in content:
                  if isinstance(item, dict) and item.get("type") == "text":
                       text_content += item.get("text", "") + " "
        elif role == "tool":
             # Include tool result briefly
             text_content = f"[Tool {msg.get('name')} result: {str(content)[:50]}...]"

        if role and text_content:
             conversation_text += f"{role.capitalize()}: {text_content.strip()}\n"


    if not conversation_text: return "Kh√¥ng c√≥ n·ªôi dung text ƒë·ªÉ t√≥m t·∫Øt."

    try:
        client = OpenAI(api_key=api_key)
        # Run sync completion in thread
        response = asyncio.run(asyncio.to_thread( # Need await here? No, run sync
             client.chat.completions.create,
             model=openai_model, # Use a fast model?
             messages=[
                 {"role": "system", "content": "T√≥m t·∫Øt cu·ªôc tr√≤ chuy·ªán sau th√†nh 1 c√¢u ng·∫Øn g·ªçn b·∫±ng ti·∫øng Vi·ªát, n√™u b·∫≠t y√™u c·∫ßu ch√≠nh ho·∫∑c k·∫øt qu·∫£ cu·ªëi c√πng."},
                 {"role": "user", "content": conversation_text}
             ],
             temperature=0.2,
             max_tokens=100
        ))
        return response.choices[0].message.content.strip()
    except Exception as e:
        logger.error(f"L·ªói khi t·∫°o t√≥m t·∫Øt chat: {e}", exc_info=True)
        return "[L·ªói t√≥m t·∫Øt]"


def save_chat_history(member_id, messages, summary=None, session_id=None):
    """L∆∞u l·ªãch s·ª≠ chat cho member_id."""
    global chat_history
    if not member_id: return # Cannot save without member ID

    # Ensure the member_id key exists and is a list
    if member_id not in chat_history or not isinstance(chat_history[member_id], list):
        chat_history[member_id] = []

    history_entry = {
        "timestamp": datetime.datetime.now().isoformat(),
        "messages": messages, # Store the full message list for this session turn
        "summary": summary or "",
        "session_id": session_id
    }

    chat_history[member_id].insert(0, history_entry)

    # Limit history size per member
    max_history_per_member = 20
    if len(chat_history[member_id]) > max_history_per_member:
        chat_history[member_id] = chat_history[member_id][:max_history_per_member]

    if not save_data(CHAT_HISTORY_FILE, chat_history):
        logger.error(f"L∆∞u l·ªãch s·ª≠ chat cho member {member_id} th·∫•t b·∫°i.")


# --- Text to Speech ---
def text_to_speech_google(text, lang='vi', slow=False, max_length=5000):
    """Chuy·ªÉn text th√†nh audio base64 d√πng gTTS."""
    try:
        # Clean HTML and limit length
        clean_text = re.sub(r'<[^>]*>', ' ', text)
        clean_text = unescape(clean_text)
        clean_text = re.sub(r'\s+', ' ', clean_text).strip()

        if not clean_text:
             logger.warning("TTS: VƒÉn b·∫£n r·ªóng sau khi l√†m s·∫°ch.")
             return None

        if len(clean_text) > max_length:
            logger.warning(f"TTS: VƒÉn b·∫£n qu√° d√†i ({len(clean_text)}), c·∫Øt ng·∫Øn c√≤n {max_length}.")
            # Find last sentence end before max_length for cleaner cut
            cut_pos = clean_text.rfind('.', 0, max_length)
            if cut_pos == -1: cut_pos = clean_text.rfind('?', 0, max_length)
            if cut_pos == -1: cut_pos = clean_text.rfind('!', 0, max_length)
            if cut_pos == -1 or cut_pos < max_length // 2: cut_pos = max_length # Fallback to hard cut
            clean_text = clean_text[:cut_pos+1]


        audio_buffer = BytesIO()
        tts = gTTS(text=clean_text, lang=lang, slow=slow)
        tts.write_to_fp(audio_buffer)
        audio_buffer.seek(0)
        audio_data = audio_buffer.read()
        audio_base64 = base64.b64encode(audio_data).decode('utf-8')
        return audio_base64

    except Exception as e:
        logger.error(f"L·ªói khi s·ª≠ d·ª•ng Google TTS: {e}", exc_info=True)
        return None

# --- Image Processing ---
def get_image_base64(image_raw: Image.Image) -> Optional[str]:
    """Chuy·ªÉn ƒë·ªëi t∆∞·ª£ng PIL Image sang base64 data URL."""
    try:
        buffered = BytesIO()
        # Determine format, default to JPEG if unknown or problematic
        img_format = image_raw.format if image_raw.format else "JPEG"
        if img_format not in ["JPEG", "PNG", "GIF", "WEBP"]:
             logger.warning(f"ƒê·ªãnh d·∫°ng ·∫£nh kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£ tr·ª±c ti·∫øp '{img_format}', chuy·ªÉn ƒë·ªïi sang JPEG.")
             img_format = "JPEG"
             # Ensure image is in RGB mode for JPEG saving
             if image_raw.mode != "RGB":
                 image_raw = image_raw.convert("RGB")

        image_raw.save(buffered, format=img_format)
        img_byte = buffered.getvalue()
        mime_type = f"image/{img_format.lower()}"
        base64_str = base64.b64encode(img_byte).decode('utf-8')
        return f"data:{mime_type};base64,{base64_str}"
    except Exception as e:
         logger.error(f"L·ªói chuy·ªÉn ƒë·ªïi ·∫£nh sang base64: {e}", exc_info=True)
         return None


# --- Event Filtering ---
def filter_events_by_member(member_id=None):
    """L·ªçc s·ª± ki·ªán theo th√†nh vi√™n (ng∆∞·ªùi t·∫°o ho·∫∑c tham gia)."""
    if not member_id: return events_data

    filtered = {}
    member_name = family_data.get(member_id, {}).get("name") if member_id in family_data else None

    for event_id, event in events_data.items():
        is_creator = event.get("created_by") == member_id
        is_participant = member_name and (member_name in event.get("participants", []))

        if is_creator or is_participant:
            filtered[event_id] = event
    return filtered


# ------- Remaining API Endpoints --------

# --- GET / ---
@app.get("/")
async def root():
    return {
        "name": "Tr·ª£ l√Ω Gia ƒë√¨nh API (Tool Calling)", "version": "1.1.0",
        "description": "API cho ·ª©ng d·ª•ng Tr·ª£ l√Ω Gia ƒë√¨nh th√¥ng minh",
        "endpoints": ["/chat", "/chat/stream", "/suggested_questions", "/family_members", "/events", "/notes", "/search", "/session", "/weather/{location}", "/analyze_image", "/transcribe_audio", "/tts", "/chat_history/{member_id}"]
    }

# --- Family Members ---
@app.get("/family_members")
async def get_family_members():
    return family_data

@app.post("/family_members")
async def add_family_member_endpoint(member: MemberModel):
    """Th√™m th√†nh vi√™n (qua endpoint tr·ª±c ti·∫øp)."""
    # This uses direct model binding, not tool calling
    details = member.dict()
    if add_family_member(details): # Call the same internal function
         # Find the newly added member's ID (assuming add_family_member sets it)
         # This is a bit indirect, maybe add_family_member should return the ID?
         new_member_id = None
         for mid, mdata in family_data.items():
              if mdata.get("name") == member.name and mdata.get("age") == member.age: # Simple match
                   new_member_id = mid
                   break
         if new_member_id:
             return {"id": new_member_id, "member": family_data[new_member_id]}
         else:
             # Should not happen if add succeeded
             raise HTTPException(status_code=500, detail="Th√™m th√†nh c√¥ng nh∆∞ng kh√¥ng t√¨m th·∫•y ID.")
    else:
        raise HTTPException(status_code=500, detail="Kh√¥ng th·ªÉ th√™m th√†nh vi√™n.")


# --- Events ---
@app.get("/events")
async def get_events(member_id: Optional[str] = None):
    if member_id:
        return filter_events_by_member(member_id)
    return events_data

@app.post("/events")
async def add_event_endpoint(event: EventModel, member_id: Optional[str] = None):
    """Th√™m s·ª± ki·ªán (qua endpoint tr·ª±c ti·∫øp)."""
    # This uses direct model binding, not tool calling
    details = event.dict()
    details["created_by"] = member_id # Assign creator if provided via query param
    details["repeat_type"] = determine_repeat_type(details.get("description"), details.get("title")) # Determine type

    # Note: This endpoint expects YYYY-MM-DD date directly
    if add_event(details):
         # Find the newly added event ID
         new_event_id = None
         for eid, edata in events_data.items():
              if (edata.get("title") == event.title and
                  edata.get("date") == event.date and
                  edata.get("created_by") == member_id):
                   new_event_id = eid
                   break
         if new_event_id:
              return {"id": new_event_id, "event": events_data[new_event_id]}
         else:
              raise HTTPException(status_code=500, detail="Th√™m s·ª± ki·ªán th√†nh c√¥ng nh∆∞ng kh√¥ng t√¨m th·∫•y ID.")

    else:
        raise HTTPException(status_code=500, detail="Kh√¥ng th·ªÉ th√™m s·ª± ki·ªán.")


# --- Notes ---
@app.get("/notes")
async def get_notes(member_id: Optional[str] = None):
    if member_id:
        return {note_id: note for note_id, note in notes_data.items()
                if note.get("created_by") == member_id}
    return notes_data

@app.post("/notes")
async def add_note_endpoint(note: NoteModel, member_id: Optional[str] = None):
    """Th√™m ghi ch√∫ (qua endpoint tr·ª±c ti·∫øp)."""
    details = note.dict()
    details["created_by"] = member_id
    if add_note(details):
         # Find the newly added note ID
         new_note_id = None
         for nid, ndata in notes_data.items():
              if (ndata.get("title") == note.title and
                  ndata.get("content") == note.content and
                  ndata.get("created_by") == member_id):
                   new_note_id = nid
                   break
         if new_note_id:
              return {"id": new_note_id, "note": notes_data[new_note_id]}
         else:
              raise HTTPException(status_code=500, detail="Th√™m note th√†nh c√¥ng nh∆∞ng kh√¥ng t√¨m th·∫•y ID.")
    else:
        raise HTTPException(status_code=500, detail="Kh√¥ng th·ªÉ th√™m ghi ch√∫.")


# --- Search ---
@app.post("/search")
async def search_endpoint(search_request: SearchRequest):
    """T√¨m ki·∫øm th√¥ng tin th·ªùi gian th·ª±c."""
    if not search_request.tavily_api_key or not search_request.openai_api_key:
        raise HTTPException(status_code=400, detail="Thi·∫øu API key cho t√¨m ki·∫øm.")

    domains_to_include = VIETNAMESE_NEWS_DOMAINS if search_request.is_news_query else None
    try:
        result = await search_and_summarize(
            search_request.tavily_api_key,
            search_request.query,
            search_request.openai_api_key,
            include_domains=domains_to_include
        )
        return {"query": search_request.query, "result": result}
    except Exception as e:
         logger.error(f"L·ªói trong search_endpoint: {e}", exc_info=True)
         raise HTTPException(status_code=500, detail=f"L·ªói t√¨m ki·∫øm: {str(e)}")


# --- Session Management ---
@app.post("/session")
async def create_session():
    session_id = str(uuid.uuid4())
    session_manager.get_session(session_id) # Creates if not exists
    return {"session_id": session_id}

@app.delete("/session/{session_id}")
async def delete_session(session_id: str):
    if session_manager.delete_session(session_id):
        return {"status": "success", "message": f"ƒê√£ x√≥a session {session_id}"}
    raise HTTPException(status_code=404, detail="Session kh√¥ng t·ªìn t·∫°i")

@app.get("/sessions")
async def list_sessions():
    sessions_info = {}
    for session_id, session_data in session_manager.sessions.items():
        sessions_info[session_id] = {
            "created_at": session_data.get("created_at"),
            "last_updated": session_data.get("last_updated"),
            "member_id": session_data.get("current_member"),
            "message_count": len(session_data.get("messages", [])),
        }
    # Sort by last_updated descending?
    sorted_sessions = sorted(sessions_info.items(), key=lambda item: item[1].get('last_updated', ''), reverse=True)
    return dict(sorted_sessions)


@app.delete("/cleanup_sessions")
async def cleanup_old_sessions_endpoint(days: int = 30): # Renamed endpoint
    try:
         session_manager.cleanup_old_sessions(days_threshold=days)
         return {"status": "success", "message": f"ƒê√£ b·∫Øt ƒë·∫ßu d·ªçn d·∫πp sessions kh√¥ng ho·∫°t ƒë·ªông tr√™n {days} ng√†y"}
    except Exception as e:
         logger.error(f"L·ªói khi d·ªçn d·∫πp session: {e}", exc_info=True)
         raise HTTPException(status_code=500, detail=f"L·ªói d·ªçn d·∫πp session: {str(e)}")


# --- Suggested Questions ---
@app.get("/suggested_questions")
async def get_suggested_questions(
    session_id: str,
    member_id: Optional[str] = None,
    openai_api_key: Optional[str] = None # Keep option to pass key if needed
):
    """L·∫•y c√¢u h·ªèi g·ª£i √Ω."""
    api_key = openai_api_key or os.getenv("OPENAI_API_KEY", "")
    session = session_manager.get_session(session_id)
    current_member_id = member_id or session.get("current_member")

    # Using template-based generation for now
    suggested_questions = generate_dynamic_suggested_questions(api_key, current_member_id, max_questions=5)
    current_timestamp = datetime.datetime.now().isoformat()

    # Cache the suggestions in session
    session["suggested_question"] = suggested_questions
    session["question_timestamp"] = current_timestamp
    session_manager.update_session(session_id, session) # Save updated session

    return SuggestedQuestionsResponse(
        session_id=session_id,
        member_id=current_member_id,
        suggested_questions=suggested_questions,
        timestamp=current_timestamp
    )

@app.get("/cached_suggested_questions")
async def get_cached_suggested_questions(session_id: str):
    """L·∫•y c√¢u h·ªèi g·ª£i √Ω ƒë√£ cache trong session."""
    session = session_manager.get_session(session_id)
    suggested = session.get("suggested_question", [])
    timestamp = session.get("question_timestamp", datetime.datetime.now().isoformat())
    return SuggestedQuestionsResponse(
        session_id=session_id,
        member_id=session.get("current_member"),
        suggested_questions=suggested,
        timestamp=timestamp
    )

# --- Chat History ---
@app.get("/chat_history/{member_id}")
async def get_member_chat_history(member_id: str): # Renamed path param
    """L·∫•y l·ªãch s·ª≠ chat c·ªßa m·ªôt th√†nh vi√™n."""
    if member_id in chat_history:
        # Return limited number of history entries?
        return chat_history[member_id][:10] # Return last 10 conversations
    return []

@app.get("/chat_history/session/{session_id}")
async def get_session_chat_history(session_id: str):
    """L·∫•y l·ªãch s·ª≠ chat theo session_id (c√≥ th·ªÉ ch·∫≠m)."""
    session_chats = []
    for member_id, histories in chat_history.items():
        for history in histories:
            if history.get("session_id") == session_id:
                history_with_member = history.copy()
                history_with_member["member_id"] = member_id
                if member_id in family_data:
                    history_with_member["member_name"] = family_data[member_id].get("name", "")
                session_chats.append(history_with_member)
    session_chats.sort(key=lambda x: x.get("timestamp", ""), reverse=True)
    return session_chats


# --- Multimedia Endpoints ---
@app.post("/analyze_image")
async def analyze_image_endpoint(
    file: UploadFile = File(...),
    openai_api_key: str = Form(...),
    member_id: Optional[str] = Form(None),
    prompt: Optional[str] = Form("M√¥ t·∫£ chi ti·∫øt h√¨nh ·∫£nh n√†y b·∫±ng ti·∫øng Vi·ªát."),
    content_type: str = Form("image") # Keep for consistency?
):
    """Ph√¢n t√≠ch h√¨nh ·∫£nh s·ª≠ d·ª•ng OpenAI Vision."""
    if not file.content_type.startswith("image/"):
        raise HTTPException(status_code=400, detail="File t·∫£i l√™n kh√¥ng ph·∫£i l√† h√¨nh ·∫£nh.")
    if not openai_api_key or "sk-" not in openai_api_key:
         raise HTTPException(status_code=400, detail="OpenAI API key kh√¥ng h·ª£p l·ªá.")

    try:
        image_content = await file.read()
        # Use BytesIO directly without saving to temp file if possible
        img = Image.open(BytesIO(image_content))
        img_base64_url = get_image_base64(img)

        if not img_base64_url:
             raise HTTPException(status_code=500, detail="Kh√¥ng th·ªÉ x·ª≠ l√Ω ·∫£nh th√†nh base64.")

        client = OpenAI(api_key=openai_api_key)
        # Run sync completion in thread
        response = await asyncio.to_thread(
             client.chat.completions.create,
             model="gpt-4o-mini", # Or gpt-4-vision-preview / gpt-4o
             messages=[
                 {"role": "system", "content": "B·∫°n l√† chuy√™n gia ph√¢n t√≠ch h√¨nh ·∫£nh. M√¥ t·∫£ chi ti·∫øt, n·∫øu l√† m√≥n ƒÉn, n√™u t√™n v√† g·ª£i √Ω c√¥ng th·ª©c/nguy√™n li·ªáu. N·∫øu l√† ho·∫°t ƒë·ªông, m√¥ t·∫£ ho·∫°t ƒë·ªông ƒë√≥."},
                 {"role": "user", "content": [
                     {"type": "text", "text": prompt},
                     {"type": "image_url", "image_url": {"url": img_base64_url}}
                 ]}
             ],
             max_tokens=1000
        )

        analysis_text = response.choices[0].message.content
        audio_response = text_to_speech_google(analysis_text)

        return {
            "analysis": analysis_text,
            "member_id": member_id,
            "content_type": content_type, # Return the provided content type
            "audio_response": audio_response
        }

    except Exception as e:
        logger.error(f"L·ªói khi ph√¢n t√≠ch h√¨nh ·∫£nh: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"L·ªói khi ph√¢n t√≠ch h√¨nh ·∫£nh: {str(e)}")

@app.post("/transcribe_audio")
async def transcribe_audio_endpoint(
    file: UploadFile = File(...),
    openai_api_key: str = Form(...)
):
    """Chuy·ªÉn ƒë·ªïi audio th√†nh text."""
    if not file.content_type.startswith("audio/"):
         # Allow common non-standard types too? e.g., application/octet-stream
         logger.warning(f"Content-Type file audio kh√¥ng chu·∫©n: {file.content_type}. V·∫´n th·ª≠ x·ª≠ l√Ω.")
         # raise HTTPException(status_code=400, detail="File t·∫£i l√™n kh√¥ng ph·∫£i l√† audio.")
    if not openai_api_key or "sk-" not in openai_api_key:
         raise HTTPException(status_code=400, detail="OpenAI API key kh√¥ng h·ª£p l·ªá.")

    # Save to temp file as Whisper works best with files
    temp_audio_path = os.path.join(TEMP_DIR, f"{uuid.uuid4()}_{file.filename}")
    try:
        # Read content async
        audio_content = await file.read()
        with open(temp_audio_path, "wb") as f:
            f.write(audio_content)

        client = OpenAI(api_key=openai_api_key)
        # Run sync transcription in thread
        with open(temp_audio_path, "rb") as audio_file_obj:
            transcript = await asyncio.to_thread(
                client.audio.transcriptions.create,
                model="whisper-1",
                file=audio_file_obj
            )

        return {"text": transcript.text}

    except Exception as e:
        logger.error(f"L·ªói khi x·ª≠ l√Ω file audio: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"L·ªói khi x·ª≠ l√Ω file audio: {str(e)}")
    finally:
        # Ensure temp file cleanup
        if os.path.exists(temp_audio_path):
            try: os.remove(temp_audio_path)
            except OSError: pass


@app.post("/tts")
async def text_to_speech_endpoint(
    text: str = Form(...),
    lang: str = Form(default="vi"),
    slow: bool = Form(default=False)
):
    """Chuy·ªÉn ƒë·ªïi text th√†nh audio base64 d√πng gTTS."""
    try:
        if not text:
            raise HTTPException(status_code=400, detail="Thi·∫øu n·ªôi dung vƒÉn b·∫£n.")

        audio_base64 = text_to_speech_google(text, lang, slow)
        if audio_base64:
            return {
                "audio_data": audio_base64,
                "format": "mp3", # gTTS typically outputs mp3
                "lang": lang,
                "provider": "Google TTS"
            }
        else:
            raise HTTPException(status_code=500, detail="Kh√¥ng th·ªÉ t·∫°o file √¢m thanh.")
    except Exception as e:
        logger.error(f"L·ªói trong text_to_speech_endpoint: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"L·ªói x·ª≠ l√Ω TTS: {str(e)}")

# --- Endpoint Th·ªùi ti·∫øt Ri√™ng bi·ªát ---
@app.get("/weather/{location}")
async def get_weather_endpoint(
    location: str,
    days: int = 1,
    target_date: Optional[str] = None, # Add target_date parameter
    openweather_api_key: Optional[str] = None
):
    """Endpoint ri√™ng bi·ªát ƒë·ªÉ l·∫•y th√¥ng tin th·ªùi ti·∫øt."""
    api_key = openweather_api_key or os.getenv("OPENWEATHER_API_KEY")
    if not api_key:
         raise HTTPException(status_code=400, detail="Thi·∫øu API key cho OpenWeatherMap.")

    # Use the global weather_service instance or create temporary if needed?
    # Using global instance is better for caching
    if not weather_service.openweather_api_key:
         # Update the key of the global instance if provided via query
         weather_service.openweather_api_key = api_key
         logger.info("C·∫≠p nh·∫≠t API key cho WeatherService t·ª´ request.")


    try:
        weather_data = await weather_service.get_weather(location, days, target_date=target_date)
        if weather_data.get("error"):
             raise HTTPException(status_code=404, detail=weather_data.get("message", "Kh√¥ng th·ªÉ l·∫•y d·ªØ li·ªáu th·ªùi ti·∫øt."))

        # Format message based on parameters
        weather_html = weather_service.format_weather_message(weather_data, location, days, target_date)

        return {
            "location": location_info.get("name", location) if (location_info := weather_data.get("location")) else location,
            "days_requested": days,
            "target_date": target_date,
            "formatted_html": weather_html,
            "raw_data": weather_data, # Optionally return raw data
            "status": "success"
        }
    except HTTPException as http_exc:
         raise http_exc # Re-raise exceptions from weather_service
    except Exception as e:
        logger.error(f"L·ªói kh√¥ng x√°c ƒë·ªãnh trong get_weather_endpoint cho {location}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"L·ªói m√°y ch·ªß khi l·∫•y th·ªùi ti·∫øt: {str(e)}")


# ----- Server Startup/Shutdown Hooks -----
@app.on_event("startup")
async def startup_event():
    """C√°c t√°c v·ª• c·∫ßn th·ª±c hi·ªán khi kh·ªüi ƒë·ªông server."""
    logger.info("Kh·ªüi ƒë·ªông Family Assistant API server (Tool Calling version)")
    # Data is loaded after function definitions now
    logger.info("ƒê√£ t·∫£i d·ªØ li·ªáu v√† s·∫µn s√†ng ho·∫°t ƒë·ªông.")

@app.on_event("shutdown")
async def shutdown_event():
    """C√°c t√°c v·ª• c·∫ßn th·ª±c hi·ªán khi ƒë√≥ng server."""
    logger.info("ƒê√≥ng Family Assistant API server...")
    # Ensure all data is saved
    save_data(FAMILY_DATA_FILE, family_data)
    save_data(EVENTS_DATA_FILE, events_data)
    save_data(NOTES_DATA_FILE, notes_data)
    save_data(CHAT_HISTORY_FILE, chat_history)
    session_manager._save_sessions() # Save sessions explicitly on shutdown
    logger.info("ƒê√£ l∆∞u d·ªØ li·ªáu. Server t·∫Øt.")


# ----- Main Execution Block -----
if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Tr·ª£ l√Ω Gia ƒë√¨nh API (Tool Calling)")
    parser.add_argument("--host", type=str, default="0.0.0.0", help="Host IP")
    parser.add_argument("--port", type=int, default=8000, help="Port")
    parser.add_argument("--reload", action="store_true", help="Auto reload server on code changes")
    args = parser.parse_args()

    log_level = "debug" if args.reload else "info" # More logs during dev

    logger.info(f"Kh·ªüi ƒë·ªông Tr·ª£ l√Ω Gia ƒë√¨nh API (Tool Calling) tr√™n http://{args.host}:{args.port}")

    # Use uvicorn.run directly for better control potentially
    uvicorn.run(
        "app:app", # Reference the FastAPI app instance
        host=args.host,
        port=args.port,
        reload=args.reload,
        log_level=log_level.lower() # Set log level for uvicorn
    )
